name: Deploy Base Kubernetes-Infrastructure

on:
  push:
    branches: [ main ]
    paths:
      - 'infra/addons/**'
      - 'infra/rbac/**'
      - 'infra/charts/**'
      - 'infra/templates/**'
      - '.github/workflows/build-push.yml'
  workflow_dispatch:

jobs:
  mirror:
    runs-on: ubuntu-latest
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python venv
        shell: bash
        run: |
          set -euo pipefail
          python3 -m venv .venv
          source .venv/bin/activate
          pip install --upgrade pip PyYAML
          echo "VENV_PYTHON=$(pwd)/.venv/bin/python" >> "$GITHUB_ENV"

      - name: Login to ACR
        uses: docker/login-action@v3
        with:
          registry: ${{ secrets.ACR_REGISTRY }}
          username: ${{ secrets.ACR_USERNAME }}
          password: ${{ secrets.ACR_PASSWORD }}

      - name: Setup Helm
        shell: bash
        run: |
          set -euo pipefail
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          helm version
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update prometheus-community

      - name: Generate mirror artifacts
        id: map
        run: |
          set -euo pipefail
          "$VENV_PYTHON" scripts/generate_addon_artifacts.py
          echo "map-file=image-map.txt" >> "$GITHUB_OUTPUT"
          echo "values-file=infra/addons/values/observability.acr.yaml" >> "$GITHUB_OUTPUT"
          
          # VALIDATION: Check if observability.acr.yaml has all images correctly rewritten to ACR
          echo "üîç Validating observability.acr.yaml - checking if all images are rewritten to ACR..."
          if [ -f "infra/addons/values/observability.acr.yaml" ]; then
            # Check if quay.io, docker.io or registry.k8s.io are still in the file (should not occur)
            PUBLIC_REGISTRIES=$(grep -E "(quay\.io|docker\.io|registry\.k8s\.io)" infra/addons/values/observability.acr.yaml 2>/dev/null || echo "")
            if [ -n "$PUBLIC_REGISTRIES" ]; then
              echo "‚ö†Ô∏è WARNING: Found public registries in observability.acr.yaml:"
              echo "$PUBLIC_REGISTRIES"
              echo "‚ö†Ô∏è These should be normalized (registry prefix removed) and registry field set to ACR"
            else
              echo "‚úÖ No public registries found in observability.acr.yaml (all images should use ACR registry)"
            fi
            
            # Check if all registry fields are set to ACR
            MISSING_REGISTRY=$(grep -A 2 "repository:" infra/addons/values/observability.acr.yaml | grep -B 1 "repository:" | grep -v "registry:" | grep "repository:" | head -1 || echo "")
            if [ -n "$MISSING_REGISTRY" ]; then
              echo "‚ö†Ô∏è WARNING: Some image entries may be missing registry field"
            else
              echo "‚úÖ All image entries have registry field set"
            fi
            
            # Show some example images from the file
            echo "üìã Sample images from observability.acr.yaml:"
            grep -A 3 "repository:" infra/addons/values/observability.acr.yaml | head -20 || true
          else
            echo "‚ö†Ô∏è WARNING: observability.acr.yaml not found!"
          fi
          
          # CRITICAL: Extract ALL images that Helm Chart will actually use (including defaults)
          echo "üîç Extracting all images from Helm Chart (including defaults)..."
          
            # Template the chart with our values to see all actually used images
          if [ -f "infra/addons/values/observability.acr.yaml" ]; then
            VALUES_FILE="infra/addons/values/observability.acr.yaml"
          elif [ -f "infra/addons/values/observability.yaml" ]; then
            VALUES_FILE="infra/addons/values/observability.yaml"
          else
            VALUES_FILE=""
          fi
          
          if [ -n "$VALUES_FILE" ]; then
            # Template das Chart (ohne es zu installieren)
            HELM_TEMPLATE_OUTPUT=$(helm template kube-prometheus-stack prometheus-community/kube-prometheus-stack \
              -n monitoring \
              -f "$VALUES_FILE" 2>/dev/null || echo "")
            
            if [ -n "$HELM_TEMPLATE_OUTPUT" ]; then
              # Extrahiere alle Image-Referenzen aus dem gerenderten YAML
              HELM_IMAGES=$(echo "$HELM_TEMPLATE_OUTPUT" | grep -E "^\s+image:" | sed 's/.*image:[[:space:]]*//' | sed 's/"//g' | sed "s/'//g" | sort -u || echo "")
              
              if [ -n "$HELM_IMAGES" ]; then
                echo "üì¶ Found images from Helm template:"
                echo "$HELM_IMAGES" | head -20
                
                # Add missing images to image-map.txt
                MISSING_COUNT=0
                while IFS= read -r image; do
                  [[ -z "$image" ]] && continue
                  
                  # Parse image (kann registry/repo:tag oder repo:tag sein)
                  if [[ "$image" == *"/"* ]]; then
                    # Hat Registry: registry/repo:tag
                    REGISTRY=$(echo "$image" | cut -d'/' -f1)
                    REPO_AND_TAG=$(echo "$image" | cut -d'/' -f2-)
                  else
                    # Keine Registry: repo:tag
                    REGISTRY=""
                    REPO_AND_TAG="$image"
                  fi
                  
                  if [[ "$REPO_AND_TAG" == *":"* ]]; then
                    REPO=$(echo "$REPO_AND_TAG" | cut -d':' -f1)
                    TAG=$(echo "$REPO_AND_TAG" | cut -d':' -f2-)
                  else
                    REPO="$REPO_AND_TAG"
                    TAG="latest"
                  fi
                  
                  # Normalisiere Registry
                  if [[ "$REGISTRY" == "docker.io" ]] || [[ "$REGISTRY" == "quay.io" ]] || [[ "$REGISTRY" == "registry.k8s.io" ]]; then
                    SOURCE_REPO="${REGISTRY}/${REPO}"
                    TARGET_REPO="$REPO"
                  elif [ -z "$REGISTRY" ]; then
                    SOURCE_REPO="docker.io/${REPO}"
                    TARGET_REPO="$REPO"
                  else
                    # Bereits ACR oder andere Registry
                    continue
                  fi
                  
                  # Check if image is already in image-map.txt (flexible match)
                  if ! grep -qE "^${SOURCE_REPO}\|.*\|${TAG}$|^.*\|${TARGET_REPO}\|${TAG}$" image-map.txt 2>/dev/null; then
                    echo "${SOURCE_REPO}|${TARGET_REPO}|${TAG}" >> image-map.txt
                    echo "  ‚ûï Added missing image: ${SOURCE_REPO}:${TAG} -> ${ACR_REGISTRY}/${TARGET_REPO}:${TAG}"
                    MISSING_COUNT=$((MISSING_COUNT + 1))
                  fi
                done <<< "$HELM_IMAGES"
                
                if [ $MISSING_COUNT -gt 0 ]; then
                  echo "‚úÖ Added $MISSING_COUNT missing images from Helm Chart defaults to image-map.txt"
                  echo "üìã Updated image-map.txt now contains:"
                  wc -l image-map.txt || true
                else
                  echo "‚úÖ All images from Helm Chart are already in image-map.txt"
                fi
              else
                echo "‚ö†Ô∏è No images found in Helm template output"
              fi
            else
              echo "‚ö†Ô∏è Helm template failed or returned empty output"
            fi
          else
            echo "‚ö†Ô∏è Values file not found, skipping Helm template extraction"
          fi

      - name: Mirror container images
        run: |
          set -euo pipefail
          while IFS='|' read -r SOURCE_REPO TARGET_REPO TAG; do
            [[ -z "${SOURCE_REPO:-}" ]] && continue
            SOURCE_IMAGE="${SOURCE_REPO}:${TAG}"
            TARGET_IMAGE="${ACR_REGISTRY}/${TARGET_REPO}:${TAG}"
            echo "‚Üí Mirroring $SOURCE_IMAGE to $TARGET_IMAGE"
            docker pull "$SOURCE_IMAGE"
            docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
            docker push "$TARGET_IMAGE"
          done < image-map.txt

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: addon-mirror-metadata
          path: |
            image-map.txt
            infra/addons/values/observability.acr.yaml

  local-path:
    name: Deploy Local Path Provisioner
    needs: mirror
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Prepare namespace and pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns local-path-storage >/dev/null 2>&1 || kubectl create namespace local-path-storage
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n local-path-storage \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Check if Local Path Provisioner already exists
        id: check_provisioner
        shell: bash
        run: |
          set -euo pipefail
          # Check for deployment in local-path-storage namespace
          if kubectl get deployment local-path-provisioner -n local-path-storage >/dev/null 2>&1; then
            echo "exists=true" >> "$GITHUB_OUTPUT"
            echo "namespace=local-path-storage" >> "$GITHUB_OUTPUT"
            echo "‚úÖ Local Path Provisioner already exists in local-path-storage namespace"
            exit 0
          fi
          # Check for deployment in kube-system namespace (k3s default)
          if kubectl get deployment local-path-provisioner -n kube-system >/dev/null 2>&1; then
            echo "exists=true" >> "$GITHUB_OUTPUT"
            echo "namespace=kube-system" >> "$GITHUB_OUTPUT"
            echo "‚úÖ Local Path Provisioner already exists in kube-system namespace (k3s default)"
            exit 0
          fi
          # Check for StorageClass
          if kubectl get storageclass local-path >/dev/null 2>&1; then
            echo "storageclass_exists=true" >> "$GITHUB_OUTPUT"
            echo "‚úÖ StorageClass 'local-path' already exists"
          else
            echo "storageclass_exists=false" >> "$GITHUB_OUTPUT"
          fi
          echo "exists=false" >> "$GITHUB_OUTPUT"

      - name: Deploy Local Path Provisioner
        if: steps.check_provisioner.outputs.exists != 'true'
        shell: bash
        run: |
          set -euo pipefail
          curl -sL https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml | \
            sed "s|rancher/local-path-provisioner:v0.0.26|${ACR_REGISTRY}/rancher/local-path-provisioner:v0.0.26|g" | \
            kubectl apply -f -
          sleep 3
          kubectl patch serviceaccount local-path-provisioner-service-account -n local-path-storage \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          kubectl -n local-path-storage rollout status deployment/local-path-provisioner --timeout=5m

      - name: Configure Local Path Provisioner RBAC
        shell: bash
        run: |
          set -euo pipefail
          # Configure RBAC for local-path-provisioner (supports both k3s and manual deployments)
          # Check which namespaces have the provisioner
          HAS_KUBE_SYSTEM=false
          HAS_LOCAL_PATH_STORAGE=false
          
          if kubectl get deployment local-path-provisioner -n kube-system >/dev/null 2>&1; then
            HAS_KUBE_SYSTEM=true
          fi
          
          if kubectl get deployment local-path-provisioner -n local-path-storage >/dev/null 2>&1; then
            HAS_LOCAL_PATH_STORAGE=true
          fi
          
          if [ "$HAS_KUBE_SYSTEM" = false ] && [ "$HAS_LOCAL_PATH_STORAGE" = false ]; then
            echo "‚ÑπÔ∏è Local Path Provisioner not found, skipping RBAC configuration"
            exit 0
          fi
          
          echo "üîê Configuring RBAC for Local Path Provisioner..."
          
          # Apply ClusterRole (shared for both namespaces)
          cat <<EOF | kubectl apply -f -
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          metadata:
            name: local-path-provisioner-role
          rules:
          # Helper-Pods erstellen und verwalten
          - apiGroups: [""]
            resources: ["pods"]
            verbs: ["create", "delete", "get", "list", "watch"]
          # Pod-Logs lesen (f√ºr Debugging)
          - apiGroups: [""]
            resources: ["pods/log"]
            verbs: ["get", "list"]
          # PersistentVolumes verwalten (inkl. patch f√ºr Updates)
          - apiGroups: [""]
            resources: ["persistentvolumes"]
            verbs: ["get", "list", "watch", "create", "delete", "patch"]
          # PersistentVolumeClaims √ºberwachen
          - apiGroups: [""]
            resources: ["persistentvolumeclaims"]
            verbs: ["get", "list", "watch", "update"]
          # StorageClasses lesen
          - apiGroups: ["storage.k8s.io"]
            resources: ["storageclasses"]
            verbs: ["get", "list", "watch"]
          # Nodes lesen (um zu wissen, wo Volumes erstellt werden)
          - apiGroups: [""]
            resources: ["nodes"]
            verbs: ["get", "list"]
          # Events f√ºr Logging
          - apiGroups: [""]
            resources: ["events"]
            verbs: ["create", "patch"]
          # Endpoints (f√ºr Service-Discovery)
          - apiGroups: [""]
            resources: ["endpoints"]
            verbs: ["get", "list", "watch", "create", "update", "patch"]
          # ConfigMaps lesen (f√ºr local-path-config) - KRITISCH!
          - apiGroups: [""]
            resources: ["configmaps"]
            verbs: ["get", "list", "watch"]
          EOF
          
          # Create ClusterRoleBinding for kube-system if provisioner exists there
          if [ "$HAS_KUBE_SYSTEM" = true ]; then
            cat <<EOF | kubectl apply -f -
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: local-path-provisioner-binding-kube-system
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: local-path-provisioner-role
          subjects:
          - kind: ServiceAccount
            name: local-path-provisioner-service-account
            namespace: kube-system
          EOF
            echo "‚úÖ RBAC binding created for kube-system namespace"
          fi
          
          # Create ClusterRoleBinding for local-path-storage if provisioner exists there
          if [ "$HAS_LOCAL_PATH_STORAGE" = true ]; then
            cat <<EOF | kubectl apply -f -
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: local-path-provisioner-binding-local-path-storage
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: local-path-provisioner-role
          subjects:
          - kind: ServiceAccount
            name: local-path-provisioner-service-account
            namespace: local-path-storage
          EOF
            echo "‚úÖ RBAC binding created for local-path-storage namespace"
          fi
          
          echo "‚úÖ RBAC configured for Local Path Provisioner"

      - name: Ensure StorageClass exists
        shell: bash
        run: |
          set -euo pipefail
          # StorageClass is now managed by Helm Chart (cluster-storage)
          # This step ensures it exists and is set as default if Helm Chart didn't create it
          if ! kubectl get storageclass local-path >/dev/null 2>&1; then
            echo "‚ö†Ô∏è StorageClass local-path not found - should be created by Helm Chart"
            echo "   Checking Helm release status..."
            helm list -n "$PROVISIONER_NAMESPACE" | grep cluster-storage || echo "   Helm release not found"
          else
            echo "‚úÖ StorageClass local-path exists (managed by Helm Chart)"
          fi

      - name: Configure ServiceAccount ImagePullSecret
        shell: bash
        run: |
          set -euo pipefail
          # Configure ImagePullSecret for local-path-provisioner ServiceAccount (both namespaces)
          
          # Function to create ACR pull secret and patch ServiceAccount
          configure_serviceaccount() {
            local NAMESPACE=$1
            
            echo "üîê Configuring ImagePullSecret for Local Path Provisioner in namespace $NAMESPACE..."
            
            # Create ACR pull secret if not exists
            kubectl create secret docker-registry acr-pull \
              --docker-server="${ACR_REGISTRY}" \
              --docker-username="${ACR_USERNAME}" \
              --docker-password="${ACR_PASSWORD}" \
              -n "$NAMESPACE" \
              --dry-run=client -o yaml | kubectl apply -f -
            
            # Check if ServiceAccount already has the secret
            if kubectl get serviceaccount local-path-provisioner-service-account -n "$NAMESPACE" -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null | grep -q acr-pull; then
              echo "‚úÖ ServiceAccount already has acr-pull secret"
            else
              # Patch ServiceAccount (try JSON patch first, fallback to merge)
              kubectl patch serviceaccount local-path-provisioner-service-account -n "$NAMESPACE" \
                --type json \
                -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null || \
              kubectl patch serviceaccount local-path-provisioner-service-account -n "$NAMESPACE" \
                -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
              
              echo "‚úÖ ServiceAccount patched with ACR pull secret"
            fi
          }
          
          # Configure for kube-system (k3s)
          if kubectl get deployment local-path-provisioner -n kube-system >/dev/null 2>&1; then
            configure_serviceaccount "kube-system"
          fi
          
          # Configure for local-path-storage (manual deployment)
          if kubectl get deployment local-path-provisioner -n local-path-storage >/dev/null 2>&1; then
            configure_serviceaccount "local-path-storage"
          fi
          
          if ! kubectl get deployment local-path-provisioner -n kube-system >/dev/null 2>&1 && \
             ! kubectl get deployment local-path-provisioner -n local-path-storage >/dev/null 2>&1; then
            echo "‚ÑπÔ∏è Local Path Provisioner not found, skipping ImagePullSecret configuration"
          fi

      - name: Patch Provisioner Deployment Image to ACR
        shell: bash
        run: |
          set -euo pipefail
          # Force update provisioner to use ACR image v0.0.31 (the version we mirror)
          
          patch_deployment_image() {
            local NAMESPACE=$1
            
            echo "üîÑ Patching provisioner deployment in $NAMESPACE to use ACR image..."
            
            # Check if deployment exists
            if ! kubectl get deployment local-path-provisioner -n "$NAMESPACE" >/dev/null 2>&1; then
              echo "‚ö†Ô∏è Deployment not found in $NAMESPACE, skipping"
              return
            fi
            
            # Get current image
            CURRENT_IMAGE=$(kubectl get deployment local-path-provisioner -n "$NAMESPACE" -o jsonpath='{.spec.template.spec.containers[0].image}' 2>/dev/null || echo "")
            
            if [ -z "$CURRENT_IMAGE" ]; then
              echo "‚ö†Ô∏è Could not determine current image, skipping"
              return
            fi
            
            echo "Current image: $CURRENT_IMAGE"
            
            # Always use v0.0.31 from ACR (the version we mirror)
            ACR_IMAGE="${ACR_REGISTRY}/rancher/local-path-provisioner:v0.0.31"
            
            # Check if already using the correct ACR image
            if [ "$CURRENT_IMAGE" = "$ACR_IMAGE" ]; then
              echo "‚úÖ Deployment already uses correct ACR image: $ACR_IMAGE"
            else
              echo "üîÑ Updating to ACR image: $ACR_IMAGE"
              
              # Patch deployment
              kubectl set image deployment/local-path-provisioner \
                local-path-provisioner="$ACR_IMAGE" \
                -n "$NAMESPACE"
              
              echo "‚úÖ Deployment patched to use ACR image"
            fi
            
            # Ensure ServiceAccount has ACR pull secret
            echo "üîê Ensuring ServiceAccount has ACR pull secret..."
            SA_NAME="local-path-provisioner-service-account"
            if ! kubectl get serviceaccount "$SA_NAME" -n "$NAMESPACE" >/dev/null 2>&1; then
              echo "‚ö†Ô∏è ServiceAccount $SA_NAME not found, trying default..."
              SA_NAME="default"
            fi
            
            # Check if secret already exists
            if kubectl get serviceaccount "$SA_NAME" -n "$NAMESPACE" -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null | grep -q "acr-pull"; then
              echo "‚úÖ ServiceAccount already has acr-pull secret"
            else
              # Try to add the secret
              kubectl patch serviceaccount "$SA_NAME" -n "$NAMESPACE" \
                -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null || \
              kubectl patch serviceaccount "$SA_NAME" -n "$NAMESPACE" \
                --type=json -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null || {
                echo "‚ö†Ô∏è Could not patch ServiceAccount, but continuing..."
              }
              echo "‚úÖ ServiceAccount patched with ACR pull secret"
            fi
            
            # Wait for rollout
            echo "‚è≥ Waiting for deployment rollout..."
            kubectl rollout status deployment/local-path-provisioner -n "$NAMESPACE" --timeout=5m || {
              echo "‚ö†Ô∏è Rollout timeout, checking status..."
              kubectl get pods -n "$NAMESPACE" -l app=local-path-provisioner || true
              kubectl describe pod -n "$NAMESPACE" -l app=local-path-provisioner | tail -20 || true
            }
          }
          
          # Patch for kube-system (k3s)
          if kubectl get deployment local-path-provisioner -n kube-system >/dev/null 2>&1; then
            patch_deployment_image "kube-system"
          fi
          
          # Patch for local-path-storage (manual deployment)
          if kubectl get deployment local-path-provisioner -n local-path-storage >/dev/null 2>&1; then
            patch_deployment_image "local-path-storage"
          fi
          
          if ! kubectl get deployment local-path-provisioner -n kube-system >/dev/null 2>&1 && \
             ! kubectl get deployment local-path-provisioner -n local-path-storage >/dev/null 2>&1; then
            echo "‚ÑπÔ∏è Local Path Provisioner not found, skipping deployment image patch"
          fi

      - name: Deploy Cluster Storage Configuration
        shell: bash
        run: |
          set -euo pipefail
          # Deploy centralized storage configuration using Helm Chart
          # This replaces manual ConfigMap management with a standardized approach
          
          echo "üì¶ Deploying cluster storage configuration via Helm..."
          
          # Determine which namespace has the provisioner and configure accordingly
          PROVISIONER_NAMESPACE=""
          STORAGE_PATH=""
          
          if kubectl get deployment local-path-provisioner -n kube-system >/dev/null 2>&1; then
            PROVISIONER_NAMESPACE="kube-system"
            STORAGE_PATH="/var/lib/rancher/k3s/storage"
            echo "‚úÖ Found provisioner in kube-system (k3s default)"
          elif kubectl get deployment local-path-provisioner -n local-path-storage >/dev/null 2>&1; then
            PROVISIONER_NAMESPACE="local-path-storage"
            STORAGE_PATH="/opt/local-path-provisioner"
            echo "‚úÖ Found provisioner in local-path-storage (manual deployment)"
          else
            echo "‚ö†Ô∏è Local Path Provisioner not found, skipping storage configuration"
            exit 0
          fi
          
          # Check if ConfigMap exists and is not managed by Helm
          if kubectl get configmap local-path-config -n "$PROVISIONER_NAMESPACE" >/dev/null 2>&1; then
            MANAGED_BY=$(kubectl get configmap local-path-config -n "$PROVISIONER_NAMESPACE" -o jsonpath='{.metadata.labels.app\.kubernetes\.io/managed-by}' 2>/dev/null || echo "")
            if [ "$MANAGED_BY" != "Helm" ]; then
              echo "‚ö†Ô∏è ConfigMap exists but is not managed by Helm, adopting it..."
              echo "   Deleting existing ConfigMap to allow Helm to recreate it..."
              kubectl delete configmap local-path-config -n "$PROVISIONER_NAMESPACE" --ignore-not-found=true
              echo "‚úÖ Existing ConfigMap deleted"
              sleep 2  # Brief wait for deletion to complete
            else
              echo "‚úÖ ConfigMap is already managed by Helm"
            fi
          fi
          
          # Check if StorageClass exists and is not managed by Helm
          if kubectl get storageclass local-path >/dev/null 2>&1; then
            MANAGED_BY=$(kubectl get storageclass local-path -o jsonpath='{.metadata.labels.app\.kubernetes\.io/managed-by}' 2>/dev/null || echo "")
            if [ "$MANAGED_BY" != "Helm" ]; then
              echo "‚ö†Ô∏è StorageClass exists but is not managed by Helm, adopting it..."
              echo "   Deleting existing StorageClass to allow Helm to recreate it..."
              kubectl delete storageclass local-path --ignore-not-found=true
              echo "‚úÖ Existing StorageClass deleted"
              sleep 2  # Brief wait for deletion to complete
            else
              echo "‚úÖ StorageClass is already managed by Helm"
            fi
          fi
          
          # Deploy cluster-storage Helm chart
          helm upgrade --install cluster-storage ./infra/charts/cluster-storage \
            --namespace "$PROVISIONER_NAMESPACE" \
            --create-namespace \
            --set localPathProvisioner.namespace="$PROVISIONER_NAMESPACE" \
            --set localPathProvisioner.storagePath="$STORAGE_PATH" \
            --set localPathProvisioner.configMap.helperPod.image="${ACR_REGISTRY}/busybox:1.36" \
            --set rbac.clusterRole.enabled=true \
            --set storageClass.enabled=true \
            --set storageClass.isDefault=true \
            --wait --timeout=2m
          
          echo "‚úÖ Cluster storage configuration deployed successfully"

      - name: Deploy Helper Pod Fix Controller
        shell: bash
        run: |
          set -euo pipefail
          echo "üîß Deploying Helper Pod Fix Controller..."
          
          # Determine provisioner namespace
          PROVISIONER_NAMESPACE="kube-system"
          if ! kubectl get deployment local-path-provisioner -n kube-system >/dev/null 2>&1; then
            if kubectl get deployment local-path-provisioner -n local-path-storage >/dev/null 2>&1; then
              PROVISIONER_NAMESPACE="local-path-storage"
            fi
          fi
          
          # Deploy controller (without --wait, we'll wait manually for better control)
          echo "üì¶ Installing Helper Pod Fix Controller..."
          helm upgrade --install helper-pod-fix-controller ./infra/charts/helper-pod-fix-controller \
            --namespace "$PROVISIONER_NAMESPACE" \
            --create-namespace \
            --set controller.namespace="$PROVISIONER_NAMESPACE" \
            --set controller.image.repository="${ACR_REGISTRY}/python" \
            --set controller.image.tag="3.11-slim" \
            --set rbac.enabled=true
          
          # Ensure ServiceAccount has ACR pull secret
          echo "üîê Ensuring ServiceAccount has ACR pull secret..."
          kubectl patch serviceaccount helper-pod-fix-controller -n "$PROVISIONER_NAMESPACE" \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null || \
          kubectl patch serviceaccount helper-pod-fix-controller -n "$PROVISIONER_NAMESPACE" \
            --type=json -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null || true
          
          # Verify ServiceAccount is patched
          SA_IPS=$(kubectl get serviceaccount helper-pod-fix-controller -n "$PROVISIONER_NAMESPACE" -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
          if [[ "$SA_IPS" == *"acr-pull"* ]]; then
            echo "‚úÖ ServiceAccount patched with ACR pull secret"
          else
            echo "‚ö†Ô∏è Warning: ServiceAccount may not have ACR pull secret"
          fi
          
          echo "‚úÖ Helm release installed, waiting for pod to be ready..."
          
          # Wait for Init-Container and Pod to be ready (with better logging)
          echo "‚è≥ Waiting for controller pod to be ready (init container installing dependencies, this may take 3-5 minutes)..."
          for i in {1..120}; do  # 10 minutes max
            POD_NAME=$(kubectl get pods -n "$PROVISIONER_NAMESPACE" -l app.kubernetes.io/name=helper-pod-fix-controller -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
            
            if [ -n "$POD_NAME" ]; then
              # Check Init-Container Status
              INIT_STATUS=$(kubectl get pod "$POD_NAME" -n "$PROVISIONER_NAMESPACE" -o jsonpath='{.status.initContainerStatuses[0].state.terminated.reason}' 2>/dev/null || echo "")
              POD_STATUS=$(kubectl get pod "$POD_NAME" -n "$PROVISIONER_NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")
              
              if [ "$INIT_STATUS" = "Completed" ] && [ "$POD_STATUS" = "Running" ]; then
                READY=$(kubectl get pod "$POD_NAME" -n "$PROVISIONER_NAMESPACE" -o jsonpath='{.status.containerStatuses[0].ready}' 2>/dev/null || echo "false")
                if [ "$READY" = "true" ]; then
                  echo "‚úÖ Controller pod is ready!"
                  break
                fi
              fi
              
              # Show progress every 30 seconds
              if [ $((i % 6)) -eq 0 ]; then
                echo "‚è≥ Still waiting... (Init: ${INIT_STATUS:-Running}, Pod: $POD_STATUS, Ready: ${READY:-false})"
                kubectl get pod "$POD_NAME" -n "$PROVISIONER_NAMESPACE" 2>/dev/null || true
              fi
            else
              # Pod not created yet
              if [ $((i % 6)) -eq 0 ]; then
                echo "‚è≥ Waiting for pod to be created... (attempt $i/120)"
              fi
            fi
            
            if [ $i -eq 120 ]; then
              echo "‚ö†Ô∏è Controller pod not ready after 10 minutes, checking status..."
              kubectl get pods -n "$PROVISIONER_NAMESPACE" -l app.kubernetes.io/name=helper-pod-fix-controller || true
              kubectl describe pod -n "$PROVISIONER_NAMESPACE" -l app.kubernetes.io/name=helper-pod-fix-controller | tail -50 || true
              echo "‚ö†Ô∏è Continuing anyway, controller may start later..."
              break
            fi
            
            sleep 5
          done
          
          echo "‚úÖ Helper Pod Fix Controller deployment completed"

      - name: Restart Provisioner and Validate
        shell: bash
        run: |
          set -euo pipefail
          # Restart provisioner after Helm Chart deployment and validate (KRITISCH!)
          # The provisioner only reads ConfigMap on startup, so restart is required
          
          # Determine which namespace has the provisioner
          PROVISIONER_NAMESPACE=""
          if kubectl get deployment local-path-provisioner -n kube-system >/dev/null 2>&1; then
            PROVISIONER_NAMESPACE="kube-system"
          elif kubectl get deployment local-path-provisioner -n local-path-storage >/dev/null 2>&1; then
            PROVISIONER_NAMESPACE="local-path-storage"
          else
            echo "‚ö†Ô∏è Local Path Provisioner not found, skipping restart"
            exit 0
          fi
          
          # Function to restart provisioner and validate
          restart_provisioner() {
            local NAMESPACE=$1
            
            echo "üîÑ Restarting provisioner in $NAMESPACE to apply ConfigMap changes..."
            echo "üí° The provisioner reads ConfigMap only on startup, so we need to force delete the pod"
            
            # Step 1: Get current provisioner pod
            CURRENT_POD=$(kubectl get pods -n "$NAMESPACE" -l app=local-path-provisioner -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
            if [ -z "$CURRENT_POD" ]; then
              echo "‚ö†Ô∏è No provisioner pod found in $NAMESPACE, skipping restart"
              return
            fi
            
            CURRENT_STATUS=$(kubectl get pod "$CURRENT_POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
            echo "Current pod: $CURRENT_POD (Status: $CURRENT_STATUS)"
            
            # Step 2: Cleanup failed helper pods before restart
            echo "üßπ Cleaning up failed helper pods..."
            kubectl get pods -n "$NAMESPACE" 2>/dev/null | grep helper-pod | awk '{print $1}' | \
              xargs -r kubectl delete pod -n "$NAMESPACE" --grace-period=0 --force 2>/dev/null || true
            
            # Step 3: Force delete provisioner pod (more reliable than rollout restart)
            # This ensures the pod reads the ConfigMap fresh on startup
            echo "üóëÔ∏è Force deleting provisioner pod to force ConfigMap reload..."
            kubectl delete pod "$CURRENT_POD" -n "$NAMESPACE" --grace-period=0 --force || true
            
            # Step 4: Wait for new pod to be created and ready
            echo "‚è≥ Waiting for new provisioner pod to be created and ready..."
            sleep 5  # Brief wait for pod deletion
            
            # Wait for new pod to exist
            for i in {1..12}; do
              NEW_POD=$(kubectl get pods -n "$NAMESPACE" -l app=local-path-provisioner -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
              if [ -n "$NEW_POD" ] && [ "$NEW_POD" != "$CURRENT_POD" ]; then
                echo "‚úÖ New pod created: $NEW_POD"
                break
              fi
              echo "  Waiting for new pod... (attempt $i/12)"
              sleep 5
            done
            
            if [ -z "$NEW_POD" ] || [ "$NEW_POD" = "$CURRENT_POD" ]; then
              echo "‚ö†Ô∏è New pod not created, checking deployment status..."
              kubectl get deployment local-path-provisioner -n "$NAMESPACE" || true
              return
            fi
            
            # Wait for pod to be ready
            echo "‚è≥ Waiting for pod $NEW_POD to be ready..."
            set +e  # Temporarily allow errors
            if kubectl wait --for=condition=ready pod "$NEW_POD" -n "$NAMESPACE" --timeout=5m; then
              echo "‚úÖ Provisioner pod is ready: $NEW_POD"
              set -e
            else
              echo "‚ö†Ô∏è Provisioner pod not ready after 5 minutes, checking status..."
              set -e
              
              # Show pod status
              echo "üì¶ Current pod status:"
              kubectl get pod "$NEW_POD" -n "$NAMESPACE" || true
              
              # Show pod events
              echo "üìã Recent events:"
              kubectl get events -n "$NAMESPACE" --field-selector involvedObject.name="$NEW_POD" --sort-by='.lastTimestamp' | tail -10 || true
              
              # Show pod logs
              echo "üìã Provisioner logs:"
              kubectl logs -n "$NAMESPACE" "$NEW_POD" --tail=50 2>/dev/null || true
              
              # Check for ImagePull errors
              if kubectl describe pod "$NEW_POD" -n "$NAMESPACE" | grep -qi "ImagePull\|ErrImagePull\|ImagePullBackOff"; then
                echo "‚ö†Ô∏è ImagePull error detected. The provisioner image might not be available on ACR."
                echo "üí° Note: The provisioner image should be available on ACR from the mirror job."
              fi
              
              # Check if pod is at least running (even if not ready)
              POD_PHASE=$(kubectl get pod "$NEW_POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "")
              if [ "$POD_PHASE" = "Running" ]; then
                echo "‚ö†Ô∏è Pod is running but not ready - ConfigMap should be loaded"
              else
                echo "‚ö†Ô∏è Pod is not running - ConfigMap might not be loaded yet"
              fi
            fi
            
            # Wait briefly for provisioner to fully start
            sleep 5
            
            # CRITICAL: Verify ConfigMap is correctly loaded by provisioner
            echo "üîç Verifying ConfigMap is correctly loaded by provisioner..."
            PROVISIONER_POD=$(kubectl get pods -n "$NAMESPACE" -l app=local-path-provisioner -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
            if [ -n "$PROVISIONER_POD" ]; then
              echo "üìã Checking ConfigMap content that provisioner sees..."
              # Read ConfigMap from provisioner pod's mounted volume
              CONFIGMAP_CONTENT=$(kubectl exec -n "$NAMESPACE" "$PROVISIONER_POD" -- cat /etc/config/config.json 2>/dev/null || echo "")
              if [ -n "$CONFIGMAP_CONTENT" ]; then
                echo "ConfigMap content:"
                echo "$CONFIGMAP_CONTENT" | jq '.' 2>/dev/null || echo "$CONFIGMAP_CONTENT"
                
                # Check if helperPod image is set correctly
                HELPER_IMAGE_IN_CONFIG=$(echo "$CONFIGMAP_CONTENT" | jq -r '.helperPod.image // empty' 2>/dev/null || echo "")
                if [ -n "$HELPER_IMAGE_IN_CONFIG" ]; then
                  echo "Helper Pod Image in ConfigMap: $HELPER_IMAGE_IN_CONFIG"
                  if echo "$HELPER_IMAGE_IN_CONFIG" | grep -q "${ACR_REGISTRY}"; then
                    echo "‚úÖ ConfigMap contains ACR image - provisioner should use it!"
                  else
                    echo "‚ö†Ô∏è ConfigMap contains: $HELPER_IMAGE_IN_CONFIG (expected: ${ACR_REGISTRY}/busybox:1.36)"
                    echo "   This indicates the ConfigMap update might not have been applied correctly"
                  fi
                else
                  echo "‚ö†Ô∏è No helperPod.image found in ConfigMap - using default?"
                fi
              else
                echo "‚ö†Ô∏è Could not read ConfigMap from provisioner pod"
              fi
            fi
            
            # CRITICAL: Delete all existing helper pods so new ones are created with updated ConfigMap
            echo "üßπ Deleting all existing helper pods to force recreation with new ConfigMap..."
            HELPER_PODS=$(kubectl get pods -n "$NAMESPACE" -o name 2>/dev/null | grep helper-pod || echo "")
            if [ -n "$HELPER_PODS" ]; then
              echo "Found helper pods to delete:"
              echo "$HELPER_PODS"
              # Before deleting, check their images
              for POD_NAME in $(echo "$HELPER_PODS" | sed 's|pod/||'); do
                OLD_IMAGE=$(kubectl get pod "$POD_NAME" -n "$NAMESPACE" -o jsonpath='{.spec.containers[0].image}' 2>/dev/null || echo "")
                OLD_STATUS=$(kubectl get pod "$POD_NAME" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "")
                echo "  Pod $POD_NAME: Image=$OLD_IMAGE, Status=$OLD_STATUS"
              done
              echo "$HELPER_PODS" | xargs -r kubectl delete -n "$NAMESPACE" --grace-period=0 --force 2>/dev/null || true
              echo "‚úÖ Old helper pods deleted"
              sleep 3  # Wait briefly for cleanup
            else
              echo "‚ÑπÔ∏è No existing helper pods found"
            fi
            
            # Validate: Check provisioner logs for errors
            echo "‚úÖ Validating provisioner configuration in $NAMESPACE..."
            PROVISIONER_LOGS=$(kubectl logs -n "$NAMESPACE" -l app=local-path-provisioner --tail=30 2>/dev/null || echo "")
            if echo "$PROVISIONER_LOGS" | grep -qiE "error|failed|fatal"; then
              echo "‚ö†Ô∏è Provisioner logs show errors:"
              kubectl logs -n "$NAMESPACE" -l app=local-path-provisioner --tail=50 || true
            else
              echo "‚úì Provisioner logs look good"
            fi
            
            # Validate: Check if Helper-Pods use ACR image
            echo "üß™ Validating helper pod image configuration..."
            # Cleanup any existing test PVC first
            kubectl delete pvc test-provisioner-validation -n default --ignore-not-found=true 2>/dev/null || true
            sleep 2
            
            # Create temporary test PVC to trigger helper pod creation
            kubectl apply -f - <<EOF 2>/dev/null || echo "‚ö†Ô∏è Test PVC creation skipped (might already exist)"
          apiVersion: v1
          kind: PersistentVolumeClaim
          metadata:
            name: test-provisioner-validation
            namespace: default
          spec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 100Mi
            storageClassName: local-path
          EOF
            
            # Wait for helper pod to be created and monitor it
            echo "‚è≥ Waiting for helper pod to be created..."
            HELPER_POD=""
            for i in {1..20}; do
              sleep 1
              HELPER_POD=$(kubectl get pods -n "$NAMESPACE" -o name 2>/dev/null | grep helper-pod | head -1 | sed 's|pod/||' || echo "")
              if [ -n "$HELPER_POD" ]; then
                echo "‚úÖ Helper pod found: $HELPER_POD"
                break
              fi
              if [ $i -eq 10 ]; then
                echo "  Still waiting for helper pod... (attempt $i/20)"
              fi
            done
            
            if [ -n "$HELPER_POD" ]; then
              # Get pod details immediately (it might disappear quickly)
              HELPER_IMAGE=$(kubectl get pod "$HELPER_POD" -n "$NAMESPACE" -o jsonpath='{.spec.containers[0].image}' 2>/dev/null || echo "")
              HELPER_STATUS=$(kubectl get pod "$HELPER_POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "")
              HELPER_REASON=$(kubectl get pod "$HELPER_POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[0].state.waiting.reason}' 2>/dev/null || echo "")
              
              echo "Helper Pod: $HELPER_POD"
              echo "  Image: $HELPER_IMAGE"
              echo "  Status: $HELPER_STATUS"
              if [ -n "$HELPER_REASON" ]; then
                echo "  Reason: $HELPER_REASON"
              fi
              
              if echo "$HELPER_IMAGE" | grep -q "${ACR_REGISTRY}"; then
                echo "‚úÖ Helper Pod uses ACR image - ConfigMap update successful!"
              else
                echo "‚ö†Ô∏è Helper Pod uses: $HELPER_IMAGE (expected: ${ACR_REGISTRY}/busybox:1.36)"
                echo "   This indicates the ConfigMap might not be correctly loaded by the provisioner"
                
                # Check if it's an ImagePull error
                if echo "$HELPER_REASON" | grep -qiE "ImagePull|ErrImagePull|ImagePullBackOff"; then
                  echo "   ‚ö†Ô∏è ImagePull error detected - helper pod cannot pull image"
                  echo "   üìã Pod events:"
                  kubectl get events -n "$NAMESPACE" --field-selector involvedObject.name="$HELPER_POD" --sort-by='.lastTimestamp' | tail -5 || true
                fi
              fi
              
              # Show pod description for debugging
              echo "üìã Pod details:"
              kubectl describe pod "$HELPER_POD" -n "$NAMESPACE" 2>/dev/null | tail -20 || true
            else
              echo "‚ÑπÔ∏è No helper pod found after 20 seconds"
              echo "   Checking PVC status..."
              kubectl get pvc test-provisioner-validation -n default 2>/dev/null || true
              echo "   Checking provisioner logs for errors..."
              kubectl logs -n "$NAMESPACE" -l app=local-path-provisioner --tail=20 --since=30s 2>/dev/null || true
            fi
            
            # Cleanup Test-PVC
            kubectl delete pvc test-provisioner-validation -n default --ignore-not-found=true 2>/dev/null || true
          }
          
          # Restart provisioner in the detected namespace
          restart_provisioner "$PROVISIONER_NAMESPACE"

      - name: Ensure StorageClass is default
        shell: bash
        run: |
          set -euo pipefail
          # StorageClass is managed by Helm Chart, but ensure it's set as default
          if kubectl get storageclass local-path >/dev/null 2>&1; then
            CURRENT_DEFAULT=$(kubectl get storageclass local-path -o jsonpath='{.metadata.annotations.storageclass\.kubernetes\.io/is-default-class}' 2>/dev/null || echo "")
            if [ "$CURRENT_DEFAULT" != "true" ]; then
              echo "üìù Setting StorageClass local-path as default..."
              kubectl patch storageclass local-path \
                -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' \
                || echo "‚ö†Ô∏è Failed to patch StorageClass"
            else
              echo "‚úÖ StorageClass local-path is already set as default"
            fi
          else
            echo "‚ö†Ô∏è StorageClass 'local-path' not found - should be created by Helm Chart"
          fi

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl get pods -n local-path-storage
          kubectl get storageclass

  metrics:
    name: Deploy Metrics Server
    needs: local-path
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Ensure pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n kube-system \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl patch serviceaccount default -n kube-system \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true

      - name: Deploy Metrics Server
        shell: bash
        run: |
          set -euo pipefail
          echo "üì¶ Deploying Metrics Server from latest GitHub release..."
          curl -sL https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml | \
            sed "s|registry.k8s.io/metrics-server/metrics-server|${ACR_REGISTRY}/metrics-server/metrics-server|g" | \
            kubectl apply -f -
          
          # Patch metrics-server ServiceAccount with pull secret (must be done after deployment creates the SA)
          echo "üîê Patching metrics-server ServiceAccount with ACR pull secret..."
          sleep 2
          kubectl patch serviceaccount metrics-server -n kube-system \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          
          # Restart deployment to pick up the new pull secret
          kubectl rollout restart deployment/metrics-server -n kube-system || true
          
          echo "‚è≥ Waiting for Metrics Server to be ready..."
          kubectl -n kube-system rollout status deployment/metrics-server --timeout=5m

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n kube-system get deploy metrics-server
          kubectl top nodes || echo "Metrics may take a few moments to appear."

  ingress:
    name: Deploy Ingress Controller
    needs: metrics
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Remove Traefik (k3s default) to avoid conflicts
        shell: bash
        run: |
          set -euo pipefail
          echo "üóëÔ∏è Removing Traefik components to avoid port conflicts with NGINX..."
          
          # Delete Traefik Service (LoadBalancer)
          kubectl delete -n kube-system svc traefik --ignore-not-found=true || true
          
          # Delete Traefik Deployment
          kubectl delete -n kube-system deployment traefik --ignore-not-found=true || true
          
          # Delete Traefik DaemonSet (svclb)
          kubectl delete -n kube-system daemonset svclb-traefik --ignore-not-found=true || true
          
          # Delete Traefik Helm Install Jobs (CRD and main install) - delete all matching jobs
          kubectl delete -n kube-system jobs -l app=helm,name=traefik --ignore-not-found=true || true
          kubectl delete -n kube-system job helm-install-traefik-crd --ignore-not-found=true || true
          kubectl delete -n kube-system job helm-install-traefik --ignore-not-found=true || true
          
          # Wait a bit for resources to be cleaned up
          sleep 5
          
          echo "‚úÖ Traefik components and Helm install jobs removed (if they existed)"
          echo "‚ÑπÔ∏è  Note: Traefik jobs will be deleted on each workflow run. To permanently disable Traefik,"
          echo "   manually add '--disable traefik' to k3s startup flags or configure /etc/rancher/k3s/config.yaml"

      - name: Ensure namespace and pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns ingress-nginx >/dev/null 2>&1 || kubectl create namespace ingress-nginx
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n ingress-nginx \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl patch serviceaccount default -n ingress-nginx \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true

      - name: Install helm dependencies
        shell: bash
        run: |
          set -euo pipefail
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo update

      - name: Deploy ingress controller
        shell: bash
        run: |
          set -euo pipefail
          ACR_IMAGE="${ACR_REGISTRY}/ingress-nginx/controller:v1.9.4"
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            -n ingress-nginx \
            -f infra/addons/values/ingress-nginx.yaml \
            --set controller.image.image="$ACR_IMAGE" \
            --timeout 5m
          kubectl patch serviceaccount ingress-nginx -n ingress-nginx \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          kubectl rollout status deployment/ingress-nginx-controller -n ingress-nginx --timeout=5m

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl get pods -n ingress-nginx

  monitoring:
    name: Deploy Monitoring Stack
    needs: ingress
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download mirror artifacts
        uses: actions/download-artifact@v4
        with:
          name: addon-mirror-metadata

      - name: Restore generated values
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p infra/addons/values
          # Artifacts preserve directory structure, so check both locations
          if [ -f "infra/addons/values/observability.acr.yaml" ]; then
            echo "‚úÖ observability.acr.yaml found in infra/addons/values/"
          elif [ -f "observability.acr.yaml" ]; then
            cp observability.acr.yaml infra/addons/values/observability.acr.yaml
            echo "‚úÖ observability.acr.yaml copied to infra/addons/values/"
          else
            echo "‚ö†Ô∏è observability.acr.yaml not found, will be generated if needed"
          fi
          
          # Validation: Check if observability.acr.yaml rewrites all images to ACR
          if [ -f "infra/addons/values/observability.acr.yaml" ]; then
            echo "üîç Validating observability.acr.yaml..."
            # Check if quay.io or docker.io references are still present (should not be)
            PUBLIC_REFS=$(grep -E "(quay\.io|docker\.io)" infra/addons/values/observability.acr.yaml | grep -v "^#" | grep -v "registry:" || echo "")
            if [ -n "$PUBLIC_REFS" ]; then
              echo "‚ö†Ô∏è Warning: Found public registry references in observability.acr.yaml:"
              echo "$PUBLIC_REFS"
              echo "‚ö†Ô∏è These should be rewritten to use ACR registry"
            else
              echo "‚úÖ All image references appear to use ACR registry"
            fi
            
            # Show some image references for verification
            echo "üìã Sample image references from observability.acr.yaml:"
            grep -A 2 "repository:" infra/addons/values/observability.acr.yaml | head -10 || true
          fi

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Prepare namespace and pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns monitoring >/dev/null 2>&1 || kubectl create namespace monitoring
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n monitoring \
            --dry-run=client -o yaml | kubectl apply -f -
          
          # Patch default ServiceAccount
          kubectl patch serviceaccount default -n monitoring \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          
          # Patch kube-prometheus-stack-admission ServiceAccount (wird vom Chart erstellt)
          kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          
          # Patch all ServiceAccounts in monitoring namespace (for future ones)
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
            echo "Patching ServiceAccount: $sa"
            kubectl patch serviceaccount "$sa" -n monitoring \
              -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          done

      - name: Install helm dependencies
        shell: bash
        run: |
          set -euo pipefail
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add grafana https://grafana.github.io/helm-charts
          helm repo update

      - name: Deploy monitoring stack
        shell: bash
        run: |
          set -euo pipefail
          
          # Debug: Show the values file being used
          echo "üìã Using values file:"
          cat infra/addons/values/observability.acr.yaml | head -50
          
          # Debug: Check current pods before deployment
          echo "üìä Current pods in monitoring namespace:"
          kubectl get pods -n monitoring || true
          
          # Clean up old failed admission jobs
          echo "üßπ Cleaning up old failed admission jobs..."
          # Delete jobs (including Terminating) - force delete for stuck jobs
          kubectl delete jobs -n monitoring -l app.kubernetes.io/name=kube-prometheus-stack,app.kubernetes.io/component=admission-webhook --ignore-not-found=true --grace-period=0 --force || true
          # Delete pods directly (including Terminating) - important, as pods sometimes get stuck
          kubectl get pods -n monitoring -l app.kubernetes.io/name=kube-prometheus-stack,app.kubernetes.io/component=admission-webhook -o name 2>/dev/null | xargs -r kubectl delete -n monitoring --grace-period=0 --force --ignore-not-found=true || true
          
          # Wait longer for cleanup to complete
          sleep 5
          
          # Deploy asynchronously (without --wait, so workflow doesn't hang)
          echo "üöÄ Deploying monitoring stack (asynchronously)..."
          
          # Aggressive cleanup for stuck Helm releases (ALWAYS execute)
          echo "üîß Force cleaning Helm locks and stuck releases..."
          
          # Try normal uninstall (with --no-hooks to skip hooks)
          helm uninstall kube-prometheus-stack -n monitoring --no-hooks --ignore-not-found=true 2>/dev/null || true
          
          # Debug: Show all secrets in namespace
          echo "üìã Current secrets in monitoring namespace:"
          kubectl get secrets -n monitoring -o name 2>/dev/null || true
          
          # Delete ALL Helm secrets related to the release
          echo "üóëÔ∏è Deleting Helm release secrets..."
          # Helm 3 Format: sh.helm.release.v1.<release-name>.<revision>
          kubectl get secrets -n monitoring -o name 2>/dev/null | grep -E "sh\.helm\.release\.v1\.kube-prometheus-stack\." | xargs -r kubectl delete -n monitoring --ignore-not-found=true || true
          # Alternative Formate
          kubectl get secrets -n monitoring -o name 2>/dev/null | grep -i "helm.*kube-prometheus-stack" | xargs -r kubectl delete -n monitoring --ignore-not-found=true || true
          # Mit Labels
          kubectl delete secret -n monitoring -l owner=helm,name=kube-prometheus-stack --ignore-not-found=true || true
          kubectl delete secret -n monitoring -l name=kube-prometheus-stack --ignore-not-found=true || true
          
          # Delete ConfigMaps
          echo "üóëÔ∏è Deleting Helm release configmaps..."
          kubectl delete configmap -n monitoring -l owner=helm,name=kube-prometheus-stack --ignore-not-found=true || true
          kubectl get configmaps -n monitoring -o name 2>/dev/null | grep "kube-prometheus-stack" | xargs -r kubectl delete -n monitoring --ignore-not-found=true || true
          
          # Wait for cleanup to complete
          sleep 10
          
          # Debug: Show if secrets are still present
          echo "üìã Remaining secrets after cleanup:"
          kubectl get secrets -n monitoring -o name 2>/dev/null | grep -E "(helm|kube-prometheus)" || echo "No Helm secrets found"
          
          # CRITICAL: Create and patch admission ServiceAccount BEFORE Helm deployment
          # Helm Hooks (post-install) create jobs that run immediately and need ImagePullSecrets
          echo "üîê Creating and patching admission ServiceAccount BEFORE Helm deployment..."
          if ! kubectl get serviceaccount kube-prometheus-stack-admission -n monitoring 2>/dev/null; then
            echo "  Creating kube-prometheus-stack-admission ServiceAccount..."
            kubectl create serviceaccount kube-prometheus-stack-admission -n monitoring || true
            kubectl label serviceaccount kube-prometheus-stack-admission -n monitoring \
              app.kubernetes.io/name=kube-prometheus-stack \
              app.kubernetes.io/component=prometheus-operator-webhook \
              app.kubernetes.io/managed-by=Helm --overwrite || true
            kubectl annotate serviceaccount kube-prometheus-stack-admission -n monitoring \
              helm.sh/hook=pre-install,pre-upgrade,post-install,post-upgrade --overwrite || true
          fi
          # Patch den ServiceAccount mit imagePullSecrets
          echo "  Patching kube-prometheus-stack-admission ServiceAccount with acr-pull..."
          kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null || true
          
          # Verifiziere dass der ServiceAccount gepatcht ist
          ADMISSION_IPS=$(kubectl get serviceaccount kube-prometheus-stack-admission -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
          if [[ "$ADMISSION_IPS" == *"acr-pull"* ]]; then
            echo "  ‚úÖ Admission ServiceAccount is patched: $ADMISSION_IPS"
          else
            echo "  ‚ö†Ô∏è Warning: Admission ServiceAccount may not be patched correctly"
            # Versuche nochmal mit JSON Patch (falls bereits ImagePullSecrets vorhanden sind)
            kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
              --type='json' \
              -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null || \
            kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
              -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null || true
          fi
          
          # NOTE: We create ONLY the admission ServiceAccount beforehand (for Helm Hooks)
          # All other ServiceAccounts are created by Helm and then immediately patched
          
          # CRITICAL: Delete ServiceAccounts that are not managed by Helm
          # This prevents import errors on first deployment
          echo "üßπ Cleaning up ServiceAccounts not managed by Helm (to prevent import errors)..."
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
            # Skip default and admission ServiceAccount
            if [[ "$sa" == "default" ]] || [[ "$sa" == "kube-prometheus-stack-admission" ]]; then
              continue
            fi
            
            # Check if ServiceAccount is managed by Helm
            HELM_RELEASE=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.metadata.annotations.meta\.helm\.sh/release-name}' 2>/dev/null || echo "")
            if [ -z "$HELM_RELEASE" ]; then
              echo "  üóëÔ∏è Deleting ServiceAccount $sa (not managed by Helm, will be recreated by Helm)..."
              kubectl delete serviceaccount "$sa" -n monitoring --ignore-not-found=true || true
            else
              echo "  ‚úÖ ServiceAccount $sa is managed by Helm (release: $HELM_RELEASE)"
            fi
          done
          echo "‚úÖ Cleanup completed"
          
          # CRITICAL: Create Helm Post-Renderer script that rewrites ALL images to ACR after rendering
          # This ensures that nested structures (like sidecar.dashboards.image) are correctly overwritten
          echo "üîß Creating Helm Post-Renderer to rewrite all images to ACR..."
          {
            echo '#!/bin/bash'
            echo '# Helm Post-Renderer: Replaces all public registry images with ACR'
            echo 'set -euo pipefail'
            echo '# ACR_REGISTRY is passed as environment variable'
            echo 'ACR_REGISTRY="${ACR_REGISTRY:-}"'
            echo 'if [ -z "$ACR_REGISTRY" ]; then'
            echo '  echo "ERROR: ACR_REGISTRY environment variable is not set" >&2'
            echo '  exit 1'
            echo 'fi'
            echo '# Lese YAML von stdin'
            echo 'YAML_INPUT=$(cat)'
            echo '# Ersetze quay.io Images (einfacher Ansatz: ersetze alle Vorkommen)'
            echo 'YAML_OUTPUT=$(echo "$YAML_INPUT" | sed "s|quay\\.io/\\([^:[:space:]]*\\):\\([^[:space:]]*\\)|${ACR_REGISTRY}/\\1:\\2|g")'
            echo '# Ersetze docker.io Images (nur wenn nicht bereits ACR)'
            echo 'YAML_OUTPUT=$(echo "$YAML_OUTPUT" | sed "s|docker\\.io/\\([^:[:space:]]*\\):\\([^[:space:]]*\\)|${ACR_REGISTRY}/\\1:\\2|g")'
            echo '# Ersetze registry.k8s.io Images'
            echo 'YAML_OUTPUT=$(echo "$YAML_OUTPUT" | sed "s|registry\\.k8s\\.io/\\([^:[:space:]]*\\):\\([^[:space:]]*\\)|${ACR_REGISTRY}/\\1:\\2|g")'
            echo '# Output'
            echo 'echo "$YAML_OUTPUT"'
          } > /tmp/helm-post-render.sh
          chmod +x /tmp/helm-post-render.sh
          echo "‚úÖ Post-Renderer created (will rewrite all public registry images to ${ACR_REGISTRY})"
          
          # Teste den Post-Renderer mit einem Beispiel
          echo "üß™ Testing Post-Renderer..."
          TEST_YAML="image: quay.io/kiwigrid/k8s-sidecar:2.1.2"
          TEST_OUTPUT=$(echo "$TEST_YAML" | ACR_REGISTRY="${ACR_REGISTRY}" /tmp/helm-post-render.sh)
          echo "  Input:  $TEST_YAML"
          echo "  Output: $TEST_OUTPUT"
          if echo "$TEST_OUTPUT" | grep -q "${ACR_REGISTRY}"; then
            echo "  ‚úÖ Post-Renderer test passed"
          else
            echo "  ‚ö†Ô∏è Post-Renderer test failed - output does not contain ACR_REGISTRY"
          fi
          
          # Ensure ACR_REGISTRY is available as environment variable
          export ACR_REGISTRY="${ACR_REGISTRY}"
          
          # First deployment: Will fail due to firewall (only ACR allowed)
          # We deploy WITHOUT --wait and WITHOUT hooks, so it runs quickly and creates pods
          echo "üöÄ First deployment (will fail due to firewall, but creates resources)..."
          set +e  # Temporarily ignore errors
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            -n monitoring \
            -f infra/addons/values/observability.acr.yaml \
            --set grafana.sidecar.dashboards.image.registry="${ACR_REGISTRY}" \
            --set grafana.sidecar.dashboards.image.repository="kiwigrid/k8s-sidecar" \
            --set grafana.sidecar.dashboards.image.tag="2.1.2" \
            --set grafana.sidecar.datasources.image.registry="${ACR_REGISTRY}" \
            --set grafana.sidecar.datasources.image.repository="kiwigrid/k8s-sidecar" \
            --set grafana.sidecar.datasources.image.tag="2.1.2" \
            --set alertmanager.image.registry="${ACR_REGISTRY}" \
            --set alertmanager.image.repository="prometheus/alertmanager" \
            --set alertmanager.image.tag="v0.29.0" \
            --post-renderer /tmp/helm-post-render.sh \
            --timeout 1m \
            --wait=false \
            --no-hooks 2>&1 | head -20 || true  # Show only first part, then abort
          set -e  # Re-enable errors
          
          echo "‚è∏Ô∏è First deployment completed (pods will fail due to firewall, but resources are created)"
          echo "üîß Now patching ServiceAccounts and cleaning up failed pods..."
          
          # Wait briefly for pods to be created
          sleep 5
          
          # Delete failed post-install hook jobs so they are recreated with patched ServiceAccounts
          echo "üßπ Cleaning up failed post-install hook jobs..."
          kubectl delete job -n monitoring -l app=kube-prometheus-stack-admission-patch --ignore-not-found=true || true
          kubectl delete job -n monitoring -l app=kube-prometheus-stack-admission-create --ignore-not-found=true || true
          sleep 2
          echo "‚úÖ Cleaned up failed hook jobs"
          
          echo "‚úÖ Helm deployment command completed. Resources are being created in the background."
          
          # CRITICAL: Patch ALL ServiceAccounts IMMEDIATELY after Helm deployment
          # Helm can overwrite ServiceAccounts, so we must patch them again
          echo "üîê CRITICAL: Patching all ServiceAccounts immediately after Helm deployment..."
          sleep 2  # Wait briefly for Helm to create ServiceAccounts
          PATCHED_COUNT=0
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
            HAS_SECRET=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[?(@.name=="acr-pull")].name}' 2>/dev/null || echo "")
            if [ -z "$HAS_SECRET" ]; then
              echo "  üîê Patching ServiceAccount: $sa"
              CURRENT_IPS=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
              if [ -n "$CURRENT_IPS" ]; then
                kubectl patch serviceaccount "$sa" -n monitoring \
                  --type='json' \
                  -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null && PATCHED_COUNT=$((PATCHED_COUNT + 1)) || true
              else
                kubectl patch serviceaccount "$sa" -n monitoring \
                  -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null && PATCHED_COUNT=$((PATCHED_COUNT + 1)) || true
              fi
            fi
          done
          echo "‚úÖ Immediately patched $PATCHED_COUNT ServiceAccounts after Helm deployment"
          
          # DEBUG: Check which images pods are using
          echo "üîç DEBUG: Checking which images pods are trying to pull..."
          sleep 5  # Wait briefly for pods to be created
          for pod in $(kubectl get pods -n monitoring -o name 2>/dev/null | head -5); do
            echo "üì¶ Pod: $pod"
            IMAGE=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.containers[0].image}' 2>/dev/null || echo "N/A")
            echo "  Image: $IMAGE"
            SA=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.serviceAccountName}' 2>/dev/null || echo "default")
            echo "  ServiceAccount: $SA"
            IPS=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.imagePullSecrets[*].name}' 2>/dev/null || echo "(none)")
            echo "  ImagePullSecrets: $IPS"
            echo "---"
          done
          
          # DEBUG: Check if ServiceAccounts have ImagePullSecrets
          echo "üîç DEBUG: Checking ServiceAccount ImagePullSecrets..."
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null); do
            echo "üìã $sa:"
            IPS=$(kubectl get $sa -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "(none)")
            echo "  ImagePullSecrets: $IPS"
          done
          
          # DEBUG: Show expected ACR image paths
          echo "üîç DEBUG: Expected ACR image paths (should match pod images above):"
          if [ -n "${ACR_REGISTRY:-}" ]; then
            echo "ACR Registry: $ACR_REGISTRY"
            echo "Expected images:"
            echo "  - ${ACR_REGISTRY}/grafana/grafana:10.2.0"
            echo "  - ${ACR_REGISTRY}/prometheus/prometheus:v2.48.0"
            echo "  - ${ACR_REGISTRY}/prometheus-operator/prometheus-operator:v0.86.2"
            echo "  - ${ACR_REGISTRY}/jettech/kube-webhook-certgen:v1.5.1"
            echo "  - ${ACR_REGISTRY}/prometheus/node-exporter:v1.10.2"
            echo "  - ${ACR_REGISTRY}/kube-state-metrics/kube-state-metrics:v2.10.1"
          fi
          
          # Simple cleanup: Delete failed pods once so they are recreated with patched ServiceAccounts
          echo "üßπ Cleaning up failed pods (if any)..."
          sleep 5  # Wait briefly for pods to be created
          
          # Delete pods with ImagePullBackOff or ErrImagePull
          IMAGEPULL_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff" || @.status.containerStatuses[0].state.waiting.reason=="ErrImagePull")].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$IMAGEPULL_PODS" ]; then
            echo "üóëÔ∏è Deleting pods with ImagePullBackOff/ErrImagePull..."
            for pod in $IMAGEPULL_PODS; do
              echo "  üóëÔ∏è Deleting pod: $pod"
              kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          else
            echo "‚úÖ No failed pods found"
          fi
          
          # Delete failed admission jobs and pods aggressively (to prevent hook timeouts)
          echo "üßπ Aggressively cleaning up admission webhook jobs and pods..."
          kubectl delete jobs -n monitoring -l app.kubernetes.io/component=admission-webhook --ignore-not-found=true --grace-period=0 --force || true
          kubectl delete jobs -n monitoring -l app.kubernetes.io/name=kube-prometheus-stack,app.kubernetes.io/component=admission-webhook --ignore-not-found=true --grace-period=0 --force || true
          kubectl delete pods -n monitoring -l app.kubernetes.io/component=admission-webhook --ignore-not-found=true --grace-period=0 --force || true
          kubectl delete pods -n monitoring -l app.kubernetes.io/name=kube-prometheus-stack,app.kubernetes.io/component=admission-webhook --ignore-not-found=true --grace-period=0 --force || true
          sleep 5  # Wait for cleanup to complete
          
          # Second deployment: Now pods with patched ServiceAccounts should work
          # Use --no-hooks to skip admission webhook jobs (they fail anyway and we've already patched ServiceAccounts)
          echo "üöÄ Second deployment (with patched ServiceAccounts, skipping hooks to avoid timeout)..."
          
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            -n monitoring \
            -f infra/addons/values/observability.acr.yaml \
            --set grafana.sidecar.dashboards.image.registry="${ACR_REGISTRY}" \
            --set grafana.sidecar.dashboards.image.repository="kiwigrid/k8s-sidecar" \
            --set grafana.sidecar.dashboards.image.tag="2.1.2" \
            --set grafana.sidecar.datasources.image.registry="${ACR_REGISTRY}" \
            --set grafana.sidecar.datasources.image.repository="kiwigrid/k8s-sidecar" \
            --set grafana.sidecar.datasources.image.tag="2.1.2" \
            --set alertmanager.image.registry="${ACR_REGISTRY}" \
            --set alertmanager.image.repository="prometheus/alertmanager" \
            --set alertmanager.image.tag="v0.29.0" \
            --post-renderer /tmp/helm-post-render.sh \
            --timeout 5m \
            --wait \
            --no-hooks || {
              echo "‚ö†Ô∏è Helm deployment completed with warnings (some resources may still be deploying)"
            }
          
          echo "‚úÖ Second deployment completed (Helm waited for resources with --wait)"
          
          # Show status of all pods
          echo "üìä Current pod status:"
          kubectl get pods -n monitoring -o wide || true
          
          # Show dashboard access URLs
          echo ""
          echo "üåê Dashboard Access URLs (via NodePort):"
          echo "=========================================="
          # Hole Host-IP (erste Node-IP)
          HOST_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}' 2>/dev/null || echo "N/A")
          if [ "$HOST_IP" != "N/A" ]; then
            echo "  üìä Grafana:      http://${HOST_IP}:30000"
            echo "     Login:        admin / admin"
            echo ""
            echo "  üìà Prometheus:   http://${HOST_IP}:30001"
            echo ""
            echo "  üîî Alertmanager: http://${HOST_IP}:30002"
            echo ""
            echo "  üí° Tip: Replace ${HOST_IP} with your actual node IP if different"
          else
            echo "  ‚ö†Ô∏è Could not determine host IP"
            echo "  üìä Grafana:      http://<NODE_IP>:30000"
            echo "  üìà Prometheus:   http://<NODE_IP>:30001"
            echo "  üîî Alertmanager: http://<NODE_IP>:30002"
          fi
          echo "=========================================="
          echo ""
          echo "üì° Networking Metrics:"
          echo "  NGINX Ingress Controller metrics are available via Prometheus"
          echo "  Import NGINX dashboards in Grafana (e.g., dashboard ID 9614)"
          echo "  Prometheus scrapes NGINX metrics automatically (metrics.enabled=true)"

      - name: Patch ServiceAccounts after deployment and restart failed jobs
        shell: bash
        run: |
          set -euo pipefail
          echo "üîê Final patching of all ServiceAccounts and cleanup of failed pods..."
          
          # Patch ALLE ServiceAccounts, die noch nicht gepatcht sind (INKLUSIVE default)
          PATCHED_COUNT=0
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
            # Check if ServiceAccount already has acr-pull as ImagePullSecret
            HAS_SECRET=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[?(@.name=="acr-pull")].name}' 2>/dev/null || echo "")
            if [ -z "$HAS_SECRET" ]; then
              echo "üîê Patching ServiceAccount: $sa"
              # Get current ImagePullSecrets (if present)
              CURRENT_IPS=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
              if [ -n "$CURRENT_IPS" ]; then
                # Add acr-pull to existing secrets
                if kubectl patch serviceaccount "$sa" -n monitoring \
                  --type='json' \
                  -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null; then
                  PATCHED_COUNT=$((PATCHED_COUNT + 1))
                  echo "  ‚úÖ Patched $sa (added to existing)"
                fi
              else
                # Erstelle neue ImagePullSecrets Liste
                if kubectl patch serviceaccount "$sa" -n monitoring \
                  -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null; then
                  PATCHED_COUNT=$((PATCHED_COUNT + 1))
                  echo "  ‚úÖ Patched $sa (created new)"
                fi
              fi
            else
              echo "  ‚úì ServiceAccount $sa already patched"
            fi
          done
          echo "‚úÖ Patched $PATCHED_COUNT ServiceAccounts"
          
          # Wait briefly for patch to be applied
          sleep 3
          
          # Check if failed jobs or pods exist
          echo "üîç Checking for failed jobs and pods..."
          FAILED_JOBS=$(kubectl get jobs -n monitoring -o jsonpath='{.items[?(@.status.failed>0)].metadata.name}' 2>/dev/null || echo "")
          IMAGEPULL_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff" || @.status.containerStatuses[0].state.waiting.reason=="ErrImagePull")].metadata.name}' 2>/dev/null || echo "")
          
          # Restart Jobs/Pods, die ImagePullBackOff oder ErrImagePull haben oder fehlgeschlagen sind
          if [ -n "$FAILED_JOBS" ] || [ -n "$IMAGEPULL_PODS" ]; then
            echo "üîÑ Restarting failed jobs and pods..."
            
            # Delete all admission jobs (will be automatically recreated)
            for job in $(kubectl get jobs -n monitoring -o name 2>/dev/null | grep -E "(admission|webhook)"); do
              echo "  üóëÔ∏è Deleting job: $job"
              kubectl delete "$job" -n monitoring --ignore-not-found=true --grace-period=0 --force || true
            done
            
            # Also delete all pods that have ImagePullBackOff or ErrImagePull (not just jobs)
            for pod in $IMAGEPULL_PODS; do
              echo "  üóëÔ∏è Deleting pod with ImagePullBackOff/ErrImagePull: $pod"
              kubectl delete pod "$pod" -n monitoring --ignore-not-found=true --grace-period=0 --force || true
            done
            
            # CRITICAL: Delete ALL pods that have no ImagePullSecrets (so they are recreated with patched ServiceAccounts)
            echo "üîÑ Checking for pods without ImagePullSecrets..."
            for pod in $(kubectl get pods -n monitoring -o name 2>/dev/null); do
              POD_IPS=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.imagePullSecrets[*].name}' 2>/dev/null || echo "")
              if [ -z "$POD_IPS" ]; then
                echo "  üóëÔ∏è Deleting pod without ImagePullSecrets: $pod"
                kubectl delete $pod -n monitoring --ignore-not-found=true --grace-period=0 --force || true
              fi
            done
            
            # Wait briefly for new jobs/pods to start
            sleep 5
            
            # Show job status
            echo "üìä Job status after restart:"
            kubectl get jobs -n monitoring | grep -E "(admission|webhook)" || echo "No admission jobs found"
          else
            echo "‚úÖ No failed jobs or ImagePullBackOff pods found - all resources are healthy"
          fi
          
          # CRITICAL: Check if pods are still pulling from public registries and patch ServiceAccounts again
          echo "üîç Checking if pods are pulling from ACR or public registries..."
          sleep 5  # Wait briefly for pods to be created
          
          # Find pods that are still pulling from quay.io, docker.io or other public registries
          PUBLIC_REGISTRY_PODS=""
          for pod in $(kubectl get pods -n monitoring -o name 2>/dev/null); do
            # Check all containers in pod (including Init containers)
            CONTAINER_IMAGES=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.containers[*].image}' 2>/dev/null || echo "")
            INIT_IMAGES=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.initContainers[*].image}' 2>/dev/null || echo "")
            ALL_IMAGES="$CONTAINER_IMAGES $INIT_IMAGES"
            
            for image in $ALL_IMAGES; do
              if [[ "$image" == quay.io/* ]] || [[ "$image" == docker.io/* ]] || [[ "$image" == registry.k8s.io/* ]]; then
                if [[ "$PUBLIC_REGISTRY_PODS" != *"$pod"* ]]; then
                  if [ -z "$PUBLIC_REGISTRY_PODS" ]; then
                    PUBLIC_REGISTRY_PODS="$pod"
                  else
                    PUBLIC_REGISTRY_PODS="$PUBLIC_REGISTRY_PODS $pod"
                  fi
                  echo "  ‚ö†Ô∏è Pod $pod is pulling from public registry: $image"
                fi
              fi
            done
          done
          
          # If pods are still pulling from public registries, patch all ServiceAccounts again
          if [ -n "$PUBLIC_REGISTRY_PODS" ]; then
            echo "‚ö†Ô∏è Found pods pulling from public registries. Patching all ServiceAccounts again..."
            PATCHED_AGAIN=0
            for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
              HAS_SECRET=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[?(@.name=="acr-pull")].name}' 2>/dev/null || echo "")
              if [ -z "$HAS_SECRET" ]; then
                echo "  üîê Patching ServiceAccount: $sa"
                CURRENT_IPS=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
                if [ -n "$CURRENT_IPS" ]; then
                  kubectl patch serviceaccount "$sa" -n monitoring \
                    --type='json' \
                    -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null && PATCHED_AGAIN=$((PATCHED_AGAIN + 1)) || true
                else
                  kubectl patch serviceaccount "$sa" -n monitoring \
                    -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null && PATCHED_AGAIN=$((PATCHED_AGAIN + 1)) || true
                fi
              fi
            done
            echo "‚úÖ Repatched $PATCHED_AGAIN ServiceAccounts"
            
            # Delete pods pulling from public registries so they are recreated
            echo "üóëÔ∏è Deleting pods pulling from public registries..."
            for pod in $PUBLIC_REGISTRY_PODS; do
              echo "  üóëÔ∏è Deleting pod: $pod"
              kubectl delete $pod -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
            
            echo "‚è≥ Waiting for pods to be recreated..."
            sleep 10
          else
            echo "‚úÖ All pods are pulling from ACR"
          fi
          
          # Final status check
          echo "üìä Final pod status:"
          kubectl get pods -n monitoring -o wide || true
          
          # Show which images pods are using
          echo "üì¶ Pod images (should all be from ACR):"
          for pod in $(kubectl get pods -n monitoring -o name 2>/dev/null | head -10); do
            IMAGE=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.containers[0].image}' 2>/dev/null || echo "N/A")
            echo "  $pod: $IMAGE"
          done
          
          # Show ServiceAccounts status
          echo "üìä ServiceAccounts with imagePullSecrets:"
          kubectl get serviceaccounts -n monitoring -o jsonpath='{range .items[*]}{.metadata.name}{": "}{.imagePullSecrets[*].name}{"\n"}{end}' || true

      - name: Collect debug logs from failed resources
        shell: bash
        run: |
          set -euo pipefail
          echo "üîç Collecting debug logs from failed resources..."
          
          # 1. Sammle Logs von fehlgeschlagenen Pods
          echo "üìã Collecting logs from failed/error pods..."
          ERROR_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.phase=="Failed" || @.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff" || @.status.containerStatuses[0].state.waiting.reason=="ErrImagePull" || @.status.containerStatuses[0].state.terminated.reason=="Error" || @.status.containerStatuses[0].state.waiting.reason=="CrashLoopBackOff")].metadata.name}' 2>/dev/null || echo "")
          
          if [ -n "$ERROR_PODS" ]; then
            for pod in $ERROR_PODS; do
              echo ""
              echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
              echo "üêõ DEBUG: Pod $pod"
              echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
              
              # Show pod details
              echo "üìã Pod Details:"
              kubectl get pod "$pod" -n monitoring -o wide || true
              
              # Show pod events
              echo ""
              echo "üìã Pod Events:"
              kubectl describe pod "$pod" -n monitoring | grep -A 20 "Events:" || true
              
              # Show container images
              echo ""
              echo "üì¶ Container Images:"
              kubectl get pod "$pod" -n monitoring -o jsonpath='{range .spec.containers[*]}{.name}{": "}{.image}{"\n"}{end}' || true
              kubectl get pod "$pod" -n monitoring -o jsonpath='{range .spec.initContainers[*]}{.name}{" (init): "}{.image}{"\n"}{end}' || true
              
              # Show ServiceAccount and ImagePullSecrets
              echo ""
              echo "üîê ServiceAccount Info:"
              SA=$(kubectl get pod "$pod" -n monitoring -o jsonpath='{.spec.serviceAccountName}' 2>/dev/null || echo "N/A")
              echo "  ServiceAccount: $SA"
              IPS=$(kubectl get pod "$pod" -n monitoring -o jsonpath='{.spec.imagePullSecrets[*].name}' 2>/dev/null || echo "none")
              echo "  ImagePullSecrets: $IPS"
              if [ "$SA" != "N/A" ] && [ "$SA" != "default" ]; then
                SA_IPS=$(kubectl get serviceaccount "$SA" -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "none")
                echo "  ServiceAccount ImagePullSecrets: $SA_IPS"
              fi
              
              # Try to collect logs (if containers were started)
              echo ""
              echo "üìã Container Logs (if available):"
              for container in $(kubectl get pod "$pod" -n monitoring -o jsonpath='{.spec.containers[*].name}' 2>/dev/null); do
                echo "  Container: $container"
                kubectl logs "$pod" -n monitoring -c "$container" --tail=50 2>&1 || echo "    (logs not available)"
              done
              
              # Show image pull error details
              echo ""
              echo "üîç Image Pull Status:"
              kubectl get pod "$pod" -n monitoring -o jsonpath='{range .status.containerStatuses[*]}{.name}{": "}{.state.waiting.reason}{" - "}{.state.waiting.message}{"\n"}{end}' || true
              kubectl get pod "$pod" -n monitoring -o jsonpath='{range .status.containerStatuses[*]}{.name}{": "}{.state.terminated.reason}{" - "}{.state.terminated.message}{"\n"}{end}' || true
            done
          else
            echo "‚úÖ No failed pods found"
          fi
          
          # 2. Sammle Logs von fehlgeschlagenen Jobs
          echo ""
          echo "üìã Collecting logs from failed jobs..."
          FAILED_JOBS=$(kubectl get jobs -n monitoring -o jsonpath='{.items[?(@.status.failed>0)].metadata.name}' 2>/dev/null || echo "")
          
          if [ -n "$FAILED_JOBS" ]; then
            for job in $FAILED_JOBS; do
              echo ""
              echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
              echo "üêõ DEBUG: Job $job"
              echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
              
              # Show job details
              echo "üìã Job Details:"
              kubectl get job "$job" -n monitoring -o wide || true
              kubectl describe job "$job" -n monitoring || true
              
              # Show job pod logs
              echo ""
              echo "üìã Job Pod Logs:"
              JOB_PODS=$(kubectl get pods -n monitoring -l job-name="$job" -o name 2>/dev/null || echo "")
              if [ -n "$JOB_PODS" ]; then
                for pod in $JOB_PODS; do
                  echo "  Pod: $pod"
                  kubectl logs $pod -n monitoring --tail=100 2>&1 || echo "    (logs not available)"
                done
              else
                echo "  (no pods found for this job)"
              fi
            done
          else
            echo "‚úÖ No failed jobs found"
          fi
          
          # 3. Show terminating pods (for info)
          echo ""
          echo "üìã Terminating pods (will be cleaned up):"
          TERMINATING_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.metadata.deletionTimestamp!=null)].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$TERMINATING_PODS" ]; then
            for pod in $TERMINATING_PODS; do
              echo "  ‚è≥ $pod (Terminating since: $(kubectl get pod "$pod" -n monitoring -o jsonpath='{.metadata.deletionTimestamp}' 2>/dev/null || echo "unknown"))"
            done
          else
            echo "  ‚úÖ No terminating pods"
          fi
          
          echo ""
          echo "‚úÖ Debug log collection completed"

      - name: Cleanup failed resources
        shell: bash
        run: |
          set -euo pipefail
          echo "üßπ Cleaning up failed and terminated resources..."
          
          # 1. Delete all terminating pods
          echo "üóëÔ∏è Cleaning up Terminating pods..."
          TERMINATING_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.metadata.deletionTimestamp!=null)].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$TERMINATING_PODS" ]; then
            for pod in $TERMINATING_PODS; do
              echo "  üóëÔ∏è Force deleting terminating pod: $pod"
              kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          fi
          
          # 2. Delete all pods with Error/ImagePullBackOff/ErrImagePull status
          echo "üóëÔ∏è Cleaning up pods with Error/ImagePullBackOff/ErrImagePull status..."
          ERROR_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.phase=="Failed" || @.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff" || @.status.containerStatuses[0].state.waiting.reason=="ErrImagePull" || @.status.containerStatuses[0].state.terminated.reason=="Error")].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$ERROR_PODS" ]; then
            for pod in $ERROR_PODS; do
              echo "  üóëÔ∏è Deleting error pod: $pod"
              kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          fi
          
          # 3. Delete all failed jobs
          echo "üóëÔ∏è Cleaning up failed jobs..."
          FAILED_JOBS=$(kubectl get jobs -n monitoring -o jsonpath='{.items[?(@.status.failed>0)].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$FAILED_JOBS" ]; then
            for job in $FAILED_JOBS; do
              echo "  üóëÔ∏è Deleting failed job: $job"
              kubectl delete job "$job" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          fi
          
          # 4. Delete all completed jobs (no longer needed, especially admission/webhook)
          echo "üóëÔ∏è Cleaning up completed admission/webhook jobs..."
          for job in $(kubectl get jobs -n monitoring -o name 2>/dev/null | grep -E "(admission|webhook|patch)"); do
            JOB_STATUS=$(kubectl get $job -n monitoring -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null || echo "")
            if [ "$JOB_STATUS" = "True" ]; then
              echo "  üóëÔ∏è Deleting completed job: $job"
              kubectl delete $job -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            fi
          done
          
          # 5. Delete all pods in CrashLoopBackOff status
          echo "üóëÔ∏è Cleaning up CrashLoopBackOff pods..."
          CRASHLOOP_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.containerStatuses[0].state.waiting.reason=="CrashLoopBackOff")].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$CRASHLOOP_PODS" ]; then
            for pod in $CRASHLOOP_PODS; do
              echo "  üóëÔ∏è Deleting CrashLoopBackOff pod: $pod"
              kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          fi
          
          # 6. Delete all pending pods waiting longer than 5 minutes (probably stuck)
          echo "üóëÔ∏è Cleaning up stuck pending pods..."
          PENDING_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.phase=="Pending")].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$PENDING_PODS" ]; then
            for pod in $PENDING_PODS; do
              POD_AGE=$(kubectl get pod "$pod" -n monitoring -o jsonpath='{.metadata.creationTimestamp}' 2>/dev/null || echo "")
              if [ -n "$POD_AGE" ]; then
                # Check if pod is older than 5 minutes (simplified - only checks if it exists)
                echo "  üóëÔ∏è Deleting pending pod (may be stuck): $pod"
                kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
              fi
            done
          fi
          
          # 7. Wait briefly for cleanup to complete
          sleep 5
          
          # 8. Final status display
          echo "üìä Final cleanup status:"
          TERMINATING_COUNT=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.metadata.deletionTimestamp!=null)].metadata.name}' 2>/dev/null | wc -w || echo 0)
          ERROR_COUNT=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.phase=="Failed")].metadata.name}' 2>/dev/null | wc -w || echo 0)
          IMAGEPULL_COUNT=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff")].metadata.name}' 2>/dev/null | wc -w || echo 0)
          FAILED_JOBS_COUNT=$(kubectl get jobs -n monitoring -o jsonpath='{.items[?(@.status.failed>0)].metadata.name}' 2>/dev/null | wc -w || echo 0)
          
          echo "  Terminating pods: $TERMINATING_COUNT"
          echo "  Error pods: $ERROR_COUNT"
          echo "  ImagePullBackOff pods: $IMAGEPULL_COUNT"
          echo "  Failed jobs: $FAILED_JOBS_COUNT"
          
          if [ "$TERMINATING_COUNT" -eq 0 ] && [ "$ERROR_COUNT" -eq 0 ] && [ "$IMAGEPULL_COUNT" -eq 0 ] && [ "$FAILED_JOBS_COUNT" -eq 0 ]; then
            echo "‚úÖ All cleanup completed - workspace is clean!"
          else
            echo "‚ö†Ô∏è Some resources may still need cleanup"
          fi

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl get pods -n monitoring