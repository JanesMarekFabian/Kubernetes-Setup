name: Deploy Base Kubernetes-Infrastructure

on:
  push:
    branches: [ main ]
    paths:
      - 'infra/addons/**'
      - 'infra/rbac/**'
      - '.github/workflows/build-push.yml'
  workflow_dispatch:

jobs:
  mirror:
    runs-on: ubuntu-latest
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python venv
        shell: bash
        run: |
          set -euo pipefail
          python3 -m venv .venv
          source .venv/bin/activate
          pip install --upgrade pip PyYAML
          echo "VENV_PYTHON=$(pwd)/.venv/bin/python" >> "$GITHUB_ENV"

      - name: Login to ACR
        uses: docker/login-action@v3
        with:
          registry: ${{ secrets.ACR_REGISTRY }}
          username: ${{ secrets.ACR_USERNAME }}
          password: ${{ secrets.ACR_PASSWORD }}

      - name: Setup Helm
        shell: bash
        run: |
          set -euo pipefail
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          helm version
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update prometheus-community

      - name: Generate mirror artifacts
        id: map
        run: |
          set -euo pipefail
          "$VENV_PYTHON" scripts/generate_addon_artifacts.py
          echo "map-file=image-map.txt" >> "$GITHUB_OUTPUT"
          echo "values-file=infra/addons/values/observability.acr.yaml" >> "$GITHUB_OUTPUT"
          
          # VALIDIERUNG: Pr√ºfe ob observability.acr.yaml alle Images korrekt auf ACR umgeschrieben hat
          echo "üîç Validating observability.acr.yaml - checking if all images are rewritten to ACR..."
          if [ -f "infra/addons/values/observability.acr.yaml" ]; then
            # Pr√ºfe ob noch quay.io, docker.io oder registry.k8s.io in der Datei sind (sollte nicht vorkommen)
            PUBLIC_REGISTRIES=$(grep -E "(quay\.io|docker\.io|registry\.k8s\.io)" infra/addons/values/observability.acr.yaml 2>/dev/null || echo "")
            if [ -n "$PUBLIC_REGISTRIES" ]; then
              echo "‚ö†Ô∏è WARNING: Found public registries in observability.acr.yaml:"
              echo "$PUBLIC_REGISTRIES"
              echo "‚ö†Ô∏è These should be normalized (registry prefix removed) and registry field set to ACR"
            else
              echo "‚úÖ No public registries found in observability.acr.yaml (all images should use ACR registry)"
            fi
            
            # Pr√ºfe ob alle registry-Felder auf ACR gesetzt sind
            MISSING_REGISTRY=$(grep -A 2 "repository:" infra/addons/values/observability.acr.yaml | grep -B 1 "repository:" | grep -v "registry:" | grep "repository:" | head -1 || echo "")
            if [ -n "$MISSING_REGISTRY" ]; then
              echo "‚ö†Ô∏è WARNING: Some image entries may be missing registry field"
            else
              echo "‚úÖ All image entries have registry field set"
            fi
            
            # Zeige ein paar Beispiel-Images aus der Datei
            echo "üìã Sample images from observability.acr.yaml:"
            grep -A 3 "repository:" infra/addons/values/observability.acr.yaml | head -20 || true
          else
            echo "‚ö†Ô∏è WARNING: observability.acr.yaml not found!"
          fi
          
          # KRITISCH: Extrahiere ALLE Images die Helm Chart tats√§chlich verwenden wird (inkl. Defaults)
          echo "üîç Extracting all images from Helm Chart (including defaults)..."
          
          # Template das Chart mit unseren Values, um alle tats√§chlich verwendeten Images zu sehen
          if [ -f "infra/addons/values/observability.acr.yaml" ]; then
            VALUES_FILE="infra/addons/values/observability.acr.yaml"
          elif [ -f "infra/addons/values/observability.yaml" ]; then
            VALUES_FILE="infra/addons/values/observability.yaml"
          else
            VALUES_FILE=""
          fi
          
          if [ -n "$VALUES_FILE" ]; then
            # Template das Chart (ohne es zu installieren)
            HELM_TEMPLATE_OUTPUT=$(helm template kube-prometheus-stack prometheus-community/kube-prometheus-stack \
              -n monitoring \
              -f "$VALUES_FILE" 2>/dev/null || echo "")
            
            if [ -n "$HELM_TEMPLATE_OUTPUT" ]; then
              # Extrahiere alle Image-Referenzen aus dem gerenderten YAML
              HELM_IMAGES=$(echo "$HELM_TEMPLATE_OUTPUT" | grep -E "^\s+image:" | sed 's/.*image:[[:space:]]*//' | sed 's/"//g' | sed "s/'//g" | sort -u || echo "")
              
              if [ -n "$HELM_IMAGES" ]; then
                echo "üì¶ Found images from Helm template:"
                echo "$HELM_IMAGES" | head -20
                
                # F√ºge fehlende Images zum image-map.txt hinzu
                MISSING_COUNT=0
                while IFS= read -r image; do
                  [[ -z "$image" ]] && continue
                  
                  # Parse image (kann registry/repo:tag oder repo:tag sein)
                  if [[ "$image" == *"/"* ]]; then
                    # Hat Registry: registry/repo:tag
                    REGISTRY=$(echo "$image" | cut -d'/' -f1)
                    REPO_AND_TAG=$(echo "$image" | cut -d'/' -f2-)
                  else
                    # Keine Registry: repo:tag
                    REGISTRY=""
                    REPO_AND_TAG="$image"
                  fi
                  
                  if [[ "$REPO_AND_TAG" == *":"* ]]; then
                    REPO=$(echo "$REPO_AND_TAG" | cut -d':' -f1)
                    TAG=$(echo "$REPO_AND_TAG" | cut -d':' -f2-)
                  else
                    REPO="$REPO_AND_TAG"
                    TAG="latest"
                  fi
                  
                  # Normalisiere Registry
                  if [[ "$REGISTRY" == "docker.io" ]] || [[ "$REGISTRY" == "quay.io" ]] || [[ "$REGISTRY" == "registry.k8s.io" ]]; then
                    SOURCE_REPO="${REGISTRY}/${REPO}"
                    TARGET_REPO="$REPO"
                  elif [ -z "$REGISTRY" ]; then
                    SOURCE_REPO="docker.io/${REPO}"
                    TARGET_REPO="$REPO"
                  else
                    # Bereits ACR oder andere Registry
                    continue
                  fi
                  
                  # Pr√ºfe ob Image bereits im image-map.txt ist (flexibler Match)
                  if ! grep -qE "^${SOURCE_REPO}\|.*\|${TAG}$|^.*\|${TARGET_REPO}\|${TAG}$" image-map.txt 2>/dev/null; then
                    echo "${SOURCE_REPO}|${TARGET_REPO}|${TAG}" >> image-map.txt
                    echo "  ‚ûï Added missing image: ${SOURCE_REPO}:${TAG} -> ${ACR_REGISTRY}/${TARGET_REPO}:${TAG}"
                    MISSING_COUNT=$((MISSING_COUNT + 1))
                  fi
                done <<< "$HELM_IMAGES"
                
                if [ $MISSING_COUNT -gt 0 ]; then
                  echo "‚úÖ Added $MISSING_COUNT missing images from Helm Chart defaults to image-map.txt"
                  echo "üìã Updated image-map.txt now contains:"
                  wc -l image-map.txt || true
                else
                  echo "‚úÖ All images from Helm Chart are already in image-map.txt"
                fi
              else
                echo "‚ö†Ô∏è No images found in Helm template output"
              fi
            else
              echo "‚ö†Ô∏è Helm template failed or returned empty output"
            fi
          else
            echo "‚ö†Ô∏è Values file not found, skipping Helm template extraction"
          fi

      - name: Mirror container images
        run: |
          set -euo pipefail
          while IFS='|' read -r SOURCE_REPO TARGET_REPO TAG; do
            [[ -z "${SOURCE_REPO:-}" ]] && continue
            SOURCE_IMAGE="${SOURCE_REPO}:${TAG}"
            TARGET_IMAGE="${ACR_REGISTRY}/${TARGET_REPO}:${TAG}"
            echo "‚Üí Mirroring $SOURCE_IMAGE to $TARGET_IMAGE"
            docker pull "$SOURCE_IMAGE"
            docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
            docker push "$TARGET_IMAGE"
          done < image-map.txt

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: addon-mirror-metadata
          path: |
            image-map.txt
            infra/addons/values/observability.acr.yaml

  local-path:
    name: Deploy Local Path Provisioner
    needs: mirror
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Prepare namespace and pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns local-path-storage >/dev/null 2>&1 || kubectl create namespace local-path-storage
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n local-path-storage \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Check if Local Path Provisioner already exists
        id: check_provisioner
        shell: bash
        run: |
          set -euo pipefail
          # Check for deployment in local-path-storage namespace
          if kubectl get deployment local-path-provisioner -n local-path-storage >/dev/null 2>&1; then
            echo "exists=true" >> "$GITHUB_OUTPUT"
            echo "namespace=local-path-storage" >> "$GITHUB_OUTPUT"
            echo "‚úÖ Local Path Provisioner already exists in local-path-storage namespace"
            exit 0
          fi
          # Check for deployment in kube-system namespace (k3s default)
          if kubectl get deployment local-path-provisioner -n kube-system >/dev/null 2>&1; then
            echo "exists=true" >> "$GITHUB_OUTPUT"
            echo "namespace=kube-system" >> "$GITHUB_OUTPUT"
            echo "‚úÖ Local Path Provisioner already exists in kube-system namespace (k3s default)"
            exit 0
          fi
          # Check for StorageClass
          if kubectl get storageclass local-path >/dev/null 2>&1; then
            echo "storageclass_exists=true" >> "$GITHUB_OUTPUT"
            echo "‚úÖ StorageClass 'local-path' already exists"
          else
            echo "storageclass_exists=false" >> "$GITHUB_OUTPUT"
          fi
          echo "exists=false" >> "$GITHUB_OUTPUT"

      - name: Deploy Local Path Provisioner
        if: steps.check_provisioner.outputs.exists != 'true'
        shell: bash
        run: |
          set -euo pipefail
          curl -sL https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml | \
            sed "s|rancher/local-path-provisioner:v0.0.26|${ACR_REGISTRY}/rancher/local-path-provisioner:v0.0.26|g" | \
            kubectl apply -f -
          sleep 3
          kubectl patch serviceaccount local-path-provisioner-service-account -n local-path-storage \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          kubectl -n local-path-storage rollout status deployment/local-path-provisioner --timeout=5m

      - name: Fix k3s Local Path Provisioner RBAC and StorageClass
        shell: bash
        run: |
          set -euo pipefail
          # Check if k3s provisioner exists in kube-system namespace
          if kubectl get deployment local-path-provisioner -n kube-system >/dev/null 2>&1; then
            echo "üîß Fixing RBAC for k3s Local Path Provisioner in kube-system..."
            
            # Apply RBAC for local-path-provisioner-service-account
            cat <<EOF | kubectl apply -f -
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          metadata:
            name: local-path-provisioner-role
          rules:
          - apiGroups: [""]
            resources: ["nodes"]
            verbs: ["get", "list"]
          - apiGroups: [""]
            resources: ["persistentvolumes"]
            verbs: ["get", "list", "watch", "create", "delete"]
          - apiGroups: [""]
            resources: ["persistentvolumeclaims"]
            verbs: ["get", "list", "watch", "update"]
          - apiGroups: [""]
            resources: ["pods"]
            verbs: ["create", "delete", "get", "list", "watch"]
          - apiGroups: ["storage.k8s.io"]
            resources: ["storageclasses"]
            verbs: ["get", "list", "watch"]
          - apiGroups: [""]
            resources: ["events"]
            verbs: ["create", "patch"]
          - apiGroups: [""]
            resources: ["endpoints"]
            verbs: ["get", "list", "watch", "create", "update", "patch"]
          ---
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: local-path-provisioner-binding
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: local-path-provisioner-role
          subjects:
          - kind: ServiceAccount
            name: local-path-provisioner-service-account
            namespace: kube-system
          EOF
            echo "‚úÖ RBAC for k3s Local Path Provisioner applied"
          fi
          
          # Ensure StorageClass exists (for both k3s and manual deployments)
          if ! kubectl get storageclass local-path >/dev/null 2>&1; then
            echo "üì¶ Creating StorageClass local-path..."
            cat <<EOF | kubectl apply -f -
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: local-path
            annotations:
              storageclass.kubernetes.io/is-default-class: "true"
          provisioner: rancher.io/local-path
          volumeBindingMode: WaitForFirstConsumer
          reclaimPolicy: Delete
          EOF
            echo "‚úÖ StorageClass local-path created"
          else
            echo "‚úÖ StorageClass local-path already exists"
          fi

      - name: Ensure StorageClass is default
        shell: bash
        run: |
          set -euo pipefail
          if kubectl get storageclass local-path >/dev/null 2>&1; then
            kubectl patch storageclass local-path \
              -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' \
              || echo "StorageClass already default"
          else
            echo "‚ö†Ô∏è StorageClass 'local-path' not found, skipping default annotation"
          fi

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl get pods -n local-path-storage
          kubectl get storageclass

  metrics:
    name: Deploy Metrics Server
    needs: local-path
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Ensure pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n kube-system \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl patch serviceaccount default -n kube-system \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true

      - name: Deploy Metrics Server
        shell: bash
        run: |
          set -euo pipefail
          echo "üì¶ Deploying Metrics Server from latest GitHub release..."
          curl -sL https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml | \
            sed "s|registry.k8s.io/metrics-server/metrics-server|${ACR_REGISTRY}/metrics-server/metrics-server|g" | \
            kubectl apply -f -
          
          # Patch metrics-server ServiceAccount with pull secret (must be done after deployment creates the SA)
          echo "üîê Patching metrics-server ServiceAccount with ACR pull secret..."
          sleep 2
          kubectl patch serviceaccount metrics-server -n kube-system \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          
          # Restart deployment to pick up the new pull secret
          kubectl rollout restart deployment/metrics-server -n kube-system || true
          
          echo "‚è≥ Waiting for Metrics Server to be ready..."
          kubectl -n kube-system rollout status deployment/metrics-server --timeout=5m

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n kube-system get deploy metrics-server
          kubectl top nodes || echo "Metrics may take a few moments to appear."

  ingress:
    name: Deploy Ingress Controller
    needs: metrics
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Remove Traefik (k3s default) to avoid conflicts
        shell: bash
        run: |
          set -euo pipefail
          echo "üóëÔ∏è Removing Traefik components to avoid port conflicts with NGINX..."
          
          # Delete Traefik Service (LoadBalancer)
          kubectl delete -n kube-system svc traefik --ignore-not-found=true || true
          
          # Delete Traefik Deployment
          kubectl delete -n kube-system deployment traefik --ignore-not-found=true || true
          
          # Delete Traefik DaemonSet (svclb)
          kubectl delete -n kube-system daemonset svclb-traefik --ignore-not-found=true || true
          
          # Delete Traefik Helm Install Jobs (CRD and main install) - delete all matching jobs
          kubectl delete -n kube-system jobs -l app=helm,name=traefik --ignore-not-found=true || true
          kubectl delete -n kube-system job helm-install-traefik-crd --ignore-not-found=true || true
          kubectl delete -n kube-system job helm-install-traefik --ignore-not-found=true || true
          
          # Wait a bit for resources to be cleaned up
          sleep 5
          
          echo "‚úÖ Traefik components and Helm install jobs removed (if they existed)"
          echo "‚ÑπÔ∏è  Note: Traefik jobs will be deleted on each workflow run. To permanently disable Traefik,"
          echo "   manually add '--disable traefik' to k3s startup flags or configure /etc/rancher/k3s/config.yaml"

      - name: Ensure namespace and pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns ingress-nginx >/dev/null 2>&1 || kubectl create namespace ingress-nginx
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n ingress-nginx \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl patch serviceaccount default -n ingress-nginx \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true

      - name: Install helm dependencies
        shell: bash
        run: |
          set -euo pipefail
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo update

      - name: Deploy ingress controller
        shell: bash
        run: |
          set -euo pipefail
          ACR_IMAGE="${ACR_REGISTRY}/ingress-nginx/controller:v1.9.4"
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            -n ingress-nginx \
            -f infra/addons/values/ingress-nginx.yaml \
            --set controller.image.image="$ACR_IMAGE" \
            --timeout 5m
          kubectl patch serviceaccount ingress-nginx -n ingress-nginx \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          kubectl rollout status deployment/ingress-nginx-controller -n ingress-nginx --timeout=5m

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl get pods -n ingress-nginx

  monitoring:
    name: Deploy Monitoring Stack
    needs: ingress
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download mirror artifacts
        uses: actions/download-artifact@v4
        with:
          name: addon-mirror-metadata

      - name: Restore generated values
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p infra/addons/values
          # Artifacts preserve directory structure, so check both locations
          if [ -f "infra/addons/values/observability.acr.yaml" ]; then
            echo "‚úÖ observability.acr.yaml found in infra/addons/values/"
          elif [ -f "observability.acr.yaml" ]; then
            cp observability.acr.yaml infra/addons/values/observability.acr.yaml
            echo "‚úÖ observability.acr.yaml copied to infra/addons/values/"
          else
            echo "‚ö†Ô∏è observability.acr.yaml not found, will be generated if needed"
          fi
          
          # Validierung: Pr√ºfe ob observability.acr.yaml alle Images auf ACR umschreibt
          if [ -f "infra/addons/values/observability.acr.yaml" ]; then
            echo "üîç Validating observability.acr.yaml..."
            # Pr√ºfe ob noch quay.io oder docker.io Referenzen vorhanden sind (sollten nicht sein)
            PUBLIC_REFS=$(grep -E "(quay\.io|docker\.io)" infra/addons/values/observability.acr.yaml | grep -v "^#" | grep -v "registry:" || echo "")
            if [ -n "$PUBLIC_REFS" ]; then
              echo "‚ö†Ô∏è Warning: Found public registry references in observability.acr.yaml:"
              echo "$PUBLIC_REFS"
              echo "‚ö†Ô∏è These should be rewritten to use ACR registry"
            else
              echo "‚úÖ All image references appear to use ACR registry"
            fi
            
            # Zeige ein paar Image-Referenzen zur Verifikation
            echo "üìã Sample image references from observability.acr.yaml:"
            grep -A 2 "repository:" infra/addons/values/observability.acr.yaml | head -10 || true
          fi

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Prepare namespace and pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns monitoring >/dev/null 2>&1 || kubectl create namespace monitoring
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n monitoring \
            --dry-run=client -o yaml | kubectl apply -f -
          
          # Patch default ServiceAccount
          kubectl patch serviceaccount default -n monitoring \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          
          # Patch kube-prometheus-stack-admission ServiceAccount (wird vom Chart erstellt)
          kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          
          # Patch alle ServiceAccounts im monitoring Namespace (f√ºr zuk√ºnftige)
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
            echo "Patching ServiceAccount: $sa"
            kubectl patch serviceaccount "$sa" -n monitoring \
              -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          done

      - name: Install helm dependencies
        shell: bash
        run: |
          set -euo pipefail
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add grafana https://grafana.github.io/helm-charts
          helm repo update

      - name: Deploy monitoring stack
        shell: bash
        run: |
          set -euo pipefail
          
          # Debug: Zeige die verwendete Values-Datei
          echo "üìã Using values file:"
          cat infra/addons/values/observability.acr.yaml | head -50
          
          # Debug: Pr√ºfe aktuelle Pods vor Deployment
          echo "üìä Current pods in monitoring namespace:"
          kubectl get pods -n monitoring || true
          
          # Bereinige alte fehlgeschlagene Admission-Jobs
          echo "üßπ Cleaning up old failed admission jobs..."
          # L√∂sche Jobs (auch Terminating) - force delete f√ºr h√§ngende Jobs
          kubectl delete jobs -n monitoring -l app.kubernetes.io/name=kube-prometheus-stack,app.kubernetes.io/component=admission-webhook --ignore-not-found=true --grace-period=0 --force || true
          # L√∂sche Pods direkt (auch Terminating) - wichtig, da Pods manchmal h√§ngen bleiben
          kubectl get pods -n monitoring -l app.kubernetes.io/name=kube-prometheus-stack,app.kubernetes.io/component=admission-webhook -o name 2>/dev/null | xargs -r kubectl delete -n monitoring --grace-period=0 --force --ignore-not-found=true || true
          
          # Warte l√§nger, damit Cleanup abgeschlossen ist
          sleep 5
          
          # Deploy asynchron (ohne --wait, damit Workflow nicht h√§ngt)
          echo "üöÄ Deploying monitoring stack (asynchron)..."
          
          # Aggressives Cleanup f√ºr stuck Helm-Releases (IMMER ausf√ºhren)
          echo "üîß Force cleaning Helm locks and stuck releases..."
          
          # Versuche normale Uninstall (mit --no-hooks um Hooks zu √ºberspringen)
          helm uninstall kube-prometheus-stack -n monitoring --no-hooks --ignore-not-found=true 2>/dev/null || true
          
          # Debug: Zeige alle Secrets im Namespace
          echo "üìã Current secrets in monitoring namespace:"
          kubectl get secrets -n monitoring -o name 2>/dev/null || true
          
          # L√∂sche ALLE Helm-Secrets die mit dem Release zu tun haben
          echo "üóëÔ∏è Deleting Helm release secrets..."
          # Helm 3 Format: sh.helm.release.v1.<release-name>.<revision>
          kubectl get secrets -n monitoring -o name 2>/dev/null | grep -E "sh\.helm\.release\.v1\.kube-prometheus-stack\." | xargs -r kubectl delete -n monitoring --ignore-not-found=true || true
          # Alternative Formate
          kubectl get secrets -n monitoring -o name 2>/dev/null | grep -i "helm.*kube-prometheus-stack" | xargs -r kubectl delete -n monitoring --ignore-not-found=true || true
          # Mit Labels
          kubectl delete secret -n monitoring -l owner=helm,name=kube-prometheus-stack --ignore-not-found=true || true
          kubectl delete secret -n monitoring -l name=kube-prometheus-stack --ignore-not-found=true || true
          
          # L√∂sche ConfigMaps
          echo "üóëÔ∏è Deleting Helm release configmaps..."
          kubectl delete configmap -n monitoring -l owner=helm,name=kube-prometheus-stack --ignore-not-found=true || true
          kubectl get configmaps -n monitoring -o name 2>/dev/null | grep "kube-prometheus-stack" | xargs -r kubectl delete -n monitoring --ignore-not-found=true || true
          
          # Warte damit Cleanup abgeschlossen ist
          sleep 10
          
          # Debug: Zeige ob noch Secrets vorhanden sind
          echo "üìã Remaining secrets after cleanup:"
          kubectl get secrets -n monitoring -o name 2>/dev/null | grep -E "(helm|kube-prometheus)" || echo "No Helm secrets found"
          
          # KRITISCH: Erstelle und patche admission ServiceAccount VOR Helm-Deployment
          # Helm Hooks (post-install) erstellen Jobs, die sofort laufen und ImagePullSecrets ben√∂tigen
          echo "üîê Creating and patching admission ServiceAccount BEFORE Helm deployment..."
          if ! kubectl get serviceaccount kube-prometheus-stack-admission -n monitoring 2>/dev/null; then
            echo "  Creating kube-prometheus-stack-admission ServiceAccount..."
            kubectl create serviceaccount kube-prometheus-stack-admission -n monitoring || true
            kubectl label serviceaccount kube-prometheus-stack-admission -n monitoring \
              app.kubernetes.io/name=kube-prometheus-stack \
              app.kubernetes.io/component=prometheus-operator-webhook \
              app.kubernetes.io/managed-by=Helm --overwrite || true
            kubectl annotate serviceaccount kube-prometheus-stack-admission -n monitoring \
              helm.sh/hook=pre-install,pre-upgrade,post-install,post-upgrade --overwrite || true
          fi
          # Patch den ServiceAccount mit imagePullSecrets
          echo "  Patching kube-prometheus-stack-admission ServiceAccount with acr-pull..."
          kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null || true
          
          # Verifiziere dass der ServiceAccount gepatcht ist
          ADMISSION_IPS=$(kubectl get serviceaccount kube-prometheus-stack-admission -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
          if [[ "$ADMISSION_IPS" == *"acr-pull"* ]]; then
            echo "  ‚úÖ Admission ServiceAccount is patched: $ADMISSION_IPS"
          else
            echo "  ‚ö†Ô∏è Warning: Admission ServiceAccount may not be patched correctly"
            # Versuche nochmal mit JSON Patch (falls bereits ImagePullSecrets vorhanden sind)
            kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
              --type='json' \
              -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null || \
            kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
              -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null || true
          fi
          
          # HINWEIS: Wir erstellen NUR den admission ServiceAccount vorher (f√ºr Helm Hooks)
          # Alle anderen ServiceAccounts werden von Helm erstellt und dann sofort gepatcht
          
          # KRITISCH: L√∂sche ServiceAccounts, die nicht von Helm verwaltet werden
          # Das verhindert Import-Fehler beim ersten Deployment
          echo "üßπ Cleaning up ServiceAccounts not managed by Helm (to prevent import errors)..."
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
            # √úberspringe default und admission ServiceAccount
            if [[ "$sa" == "default" ]] || [[ "$sa" == "kube-prometheus-stack-admission" ]]; then
              continue
            fi
            
            # Pr√ºfe ob ServiceAccount von Helm verwaltet wird
            HELM_RELEASE=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.metadata.annotations.meta\.helm\.sh/release-name}' 2>/dev/null || echo "")
            if [ -z "$HELM_RELEASE" ]; then
              echo "  üóëÔ∏è Deleting ServiceAccount $sa (not managed by Helm, will be recreated by Helm)..."
              kubectl delete serviceaccount "$sa" -n monitoring --ignore-not-found=true || true
            else
              echo "  ‚úÖ ServiceAccount $sa is managed by Helm (release: $HELM_RELEASE)"
            fi
          done
          echo "‚úÖ Cleanup completed"
          
          # KRITISCH: Erstelle Helm Post-Renderer Script, der ALLE Images nach dem Rendern auf ACR umschreibt
          # Dies stellt sicher, dass auch verschachtelte Strukturen (wie sidecar.dashboards.image) korrekt √ºberschrieben werden
          echo "üîß Creating Helm Post-Renderer to rewrite all images to ACR..."
          {
            echo '#!/bin/bash'
            echo '# Helm Post-Renderer: Ersetzt alle √∂ffentlichen Registry-Images durch ACR'
            echo 'set -euo pipefail'
            echo '# ACR_REGISTRY wird als Environment-Variable √ºbergeben'
            echo 'ACR_REGISTRY="${ACR_REGISTRY:-}"'
            echo 'if [ -z "$ACR_REGISTRY" ]; then'
            echo '  echo "ERROR: ACR_REGISTRY environment variable is not set" >&2'
            echo '  exit 1'
            echo 'fi'
            echo '# Lese YAML von stdin'
            echo 'YAML_INPUT=$(cat)'
            echo '# Ersetze quay.io Images (einfacher Ansatz: ersetze alle Vorkommen)'
            echo 'YAML_OUTPUT=$(echo "$YAML_INPUT" | sed "s|quay\\.io/\\([^:[:space:]]*\\):\\([^[:space:]]*\\)|${ACR_REGISTRY}/\\1:\\2|g")'
            echo '# Ersetze docker.io Images (nur wenn nicht bereits ACR)'
            echo 'YAML_OUTPUT=$(echo "$YAML_OUTPUT" | sed "s|docker\\.io/\\([^:[:space:]]*\\):\\([^[:space:]]*\\)|${ACR_REGISTRY}/\\1:\\2|g")'
            echo '# Ersetze registry.k8s.io Images'
            echo 'YAML_OUTPUT=$(echo "$YAML_OUTPUT" | sed "s|registry\\.k8s\\.io/\\([^:[:space:]]*\\):\\([^[:space:]]*\\)|${ACR_REGISTRY}/\\1:\\2|g")'
            echo '# Output'
            echo 'echo "$YAML_OUTPUT"'
          } > /tmp/helm-post-render.sh
          chmod +x /tmp/helm-post-render.sh
          echo "‚úÖ Post-Renderer created (will rewrite all public registry images to ${ACR_REGISTRY})"
          
          # Teste den Post-Renderer mit einem Beispiel
          echo "üß™ Testing Post-Renderer..."
          TEST_YAML="image: quay.io/kiwigrid/k8s-sidecar:2.1.2"
          TEST_OUTPUT=$(echo "$TEST_YAML" | ACR_REGISTRY="${ACR_REGISTRY}" /tmp/helm-post-render.sh)
          echo "  Input:  $TEST_YAML"
          echo "  Output: $TEST_OUTPUT"
          if echo "$TEST_OUTPUT" | grep -q "${ACR_REGISTRY}"; then
            echo "  ‚úÖ Post-Renderer test passed"
          else
            echo "  ‚ö†Ô∏è Post-Renderer test failed - output does not contain ACR_REGISTRY"
          fi
          
          # Stelle sicher, dass ACR_REGISTRY als Environment-Variable verf√ºgbar ist
          export ACR_REGISTRY="${ACR_REGISTRY}"
          
          # Erstes Deployment: Wird fehlschlagen wegen Firewall (nur ACR erlaubt)
          # Wir deployen OHNE --wait, damit es schnell durchl√§uft und Pods erstellt werden
          echo "üöÄ First deployment (will fail due to firewall, but creates resources)..."
          set +e  # Tempor√§r Fehler ignorieren
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            -n monitoring \
            -f infra/addons/values/observability.acr.yaml \
            --set grafana.sidecar.dashboards.image.registry="${ACR_REGISTRY}" \
            --set grafana.sidecar.dashboards.image.repository="kiwigrid/k8s-sidecar" \
            --set grafana.sidecar.dashboards.image.tag="2.1.2" \
            --set grafana.sidecar.datasources.image.registry="${ACR_REGISTRY}" \
            --set grafana.sidecar.datasources.image.repository="kiwigrid/k8s-sidecar" \
            --set grafana.sidecar.datasources.image.tag="2.1.2" \
            --set alertmanager.image.registry="${ACR_REGISTRY}" \
            --set alertmanager.image.repository="prometheus/alertmanager" \
            --set alertmanager.image.tag="v0.29.0" \
            --post-renderer /tmp/helm-post-render.sh \
            --timeout 2m \
            --wait=false 2>&1 | head -20 || true  # Zeige nur ersten Teil, dann abbrechen
          set -e  # Fehler wieder aktivieren
          
          echo "‚è∏Ô∏è First deployment completed (pods will fail due to firewall, but resources are created)"
          echo "üîß Now patching ServiceAccounts and cleaning up failed pods..."
          
          # Warte kurz, damit Pods erstellt wurden
          sleep 5
          
          # L√∂sche fehlgeschlagene Post-Install-Hook-Jobs, damit sie mit gepatchten ServiceAccounts neu erstellt werden
          echo "üßπ Cleaning up failed post-install hook jobs..."
          kubectl delete job -n monitoring -l app=kube-prometheus-stack-admission-patch --ignore-not-found=true || true
          kubectl delete job -n monitoring -l app=kube-prometheus-stack-admission-create --ignore-not-found=true || true
          sleep 2
          echo "‚úÖ Cleaned up failed hook jobs"
          
          echo "‚úÖ Helm deployment command completed. Resources are being created in the background."
          
          # KRITISCH: Patch ALLE ServiceAccounts SOFORT nach Helm-Deployment
          # Helm kann ServiceAccounts √ºberschreiben, daher m√ºssen wir sie erneut patchen
          echo "üîê CRITICAL: Patching all ServiceAccounts immediately after Helm deployment..."
          sleep 2  # Kurz warten, damit Helm ServiceAccounts erstellt hat
          PATCHED_COUNT=0
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
            HAS_SECRET=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[?(@.name=="acr-pull")].name}' 2>/dev/null || echo "")
            if [ -z "$HAS_SECRET" ]; then
              echo "  üîê Patching ServiceAccount: $sa"
              CURRENT_IPS=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
              if [ -n "$CURRENT_IPS" ]; then
                kubectl patch serviceaccount "$sa" -n monitoring \
                  --type='json' \
                  -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null && PATCHED_COUNT=$((PATCHED_COUNT + 1)) || true
              else
                kubectl patch serviceaccount "$sa" -n monitoring \
                  -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null && PATCHED_COUNT=$((PATCHED_COUNT + 1)) || true
              fi
            fi
          done
          echo "‚úÖ Immediately patched $PATCHED_COUNT ServiceAccounts after Helm deployment"
          
          # DEBUG: Pr√ºfe welche Images die Pods verwenden
          echo "üîç DEBUG: Checking which images pods are trying to pull..."
          sleep 5  # Warte kurz, damit Pods erstellt werden
          for pod in $(kubectl get pods -n monitoring -o name 2>/dev/null | head -5); do
            echo "üì¶ Pod: $pod"
            IMAGE=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.containers[0].image}' 2>/dev/null || echo "N/A")
            echo "  Image: $IMAGE"
            SA=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.serviceAccountName}' 2>/dev/null || echo "default")
            echo "  ServiceAccount: $SA"
            IPS=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.imagePullSecrets[*].name}' 2>/dev/null || echo "(none)")
            echo "  ImagePullSecrets: $IPS"
            echo "---"
          done
          
          # DEBUG: Pr√ºfe ob ServiceAccounts ImagePullSecrets haben
          echo "üîç DEBUG: Checking ServiceAccount ImagePullSecrets..."
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null); do
            echo "üìã $sa:"
            IPS=$(kubectl get $sa -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "(none)")
            echo "  ImagePullSecrets: $IPS"
          done
          
          # DEBUG: Zeige erwartete ACR Image-Pfade
          echo "üîç DEBUG: Expected ACR image paths (should match pod images above):"
          if [ -n "${ACR_REGISTRY:-}" ]; then
            echo "ACR Registry: $ACR_REGISTRY"
            echo "Expected images:"
            echo "  - ${ACR_REGISTRY}/grafana/grafana:10.2.0"
            echo "  - ${ACR_REGISTRY}/prometheus/prometheus:v2.48.0"
            echo "  - ${ACR_REGISTRY}/prometheus-operator/prometheus-operator:v0.86.2"
            echo "  - ${ACR_REGISTRY}/jettech/kube-webhook-certgen:v1.5.1"
            echo "  - ${ACR_REGISTRY}/prometheus/node-exporter:v1.10.2"
            echo "  - ${ACR_REGISTRY}/kube-state-metrics/kube-state-metrics:v2.10.1"
          fi
          
          # Einfaches Cleanup: L√∂sche fehlgeschlagene Pods einmalig, damit sie mit gepatchten ServiceAccounts neu erstellt werden
          echo "üßπ Cleaning up failed pods (if any)..."
          sleep 5  # Warte kurz, damit Pods erstellt werden
          
          # L√∂sche Pods mit ImagePullBackOff oder ErrImagePull
          IMAGEPULL_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff" || @.status.containerStatuses[0].state.waiting.reason=="ErrImagePull")].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$IMAGEPULL_PODS" ]; then
            echo "üóëÔ∏è Deleting pods with ImagePullBackOff/ErrImagePull..."
            for pod in $IMAGEPULL_PODS; do
              echo "  üóëÔ∏è Deleting pod: $pod"
              kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          else
            echo "‚úÖ No failed pods found"
          fi
          
          # L√∂sche fehlgeschlagene Admission-Jobs
          kubectl delete jobs -n monitoring -l app.kubernetes.io/component=admission-webhook --ignore-not-found=true || true
          kubectl delete jobs -n monitoring -l app.kubernetes.io/name=kube-prometheus-stack,app.kubernetes.io/component=admission-webhook --ignore-not-found=true || true
          
          # Zweites Deployment: Jetzt sollten Pods mit gepatchten ServiceAccounts funktionieren
          echo "üöÄ Second deployment (with patched ServiceAccounts, should succeed)..."
          sleep 3  # Kurz warten, damit Pods gel√∂scht wurden
          
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            -n monitoring \
            -f infra/addons/values/observability.acr.yaml \
            --set grafana.sidecar.dashboards.image.registry="${ACR_REGISTRY}" \
            --set grafana.sidecar.dashboards.image.repository="kiwigrid/k8s-sidecar" \
            --set grafana.sidecar.dashboards.image.tag="2.1.2" \
            --set grafana.sidecar.datasources.image.registry="${ACR_REGISTRY}" \
            --set grafana.sidecar.datasources.image.repository="kiwigrid/k8s-sidecar" \
            --set grafana.sidecar.datasources.image.tag="2.1.2" \
            --set alertmanager.image.registry="${ACR_REGISTRY}" \
            --set alertmanager.image.repository="prometheus/alertmanager" \
            --set alertmanager.image.tag="v0.29.0" \
            --post-renderer /tmp/helm-post-render.sh \
            --timeout 10m \
            --wait --wait-for-jobs=false || {
              echo "‚ö†Ô∏è Helm deployment completed with warnings (some jobs may have failed)"
            }
          
          echo "‚úÖ Second deployment completed (Helm waited for resources with --wait)"
          
          # Zeige Status aller Pods
          echo "üìä Current pod status:"
          kubectl get pods -n monitoring -o wide || true

      - name: Patch ServiceAccounts after deployment and restart failed jobs
        shell: bash
        run: |
          set -euo pipefail
          echo "üîê Final patching of all ServiceAccounts and cleanup of failed pods..."
          
          # Patch ALLE ServiceAccounts, die noch nicht gepatcht sind (INKLUSIVE default)
          PATCHED_COUNT=0
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
            # Pr√ºfe ob ServiceAccount bereits acr-pull als ImagePullSecret hat
            HAS_SECRET=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[?(@.name=="acr-pull")].name}' 2>/dev/null || echo "")
            if [ -z "$HAS_SECRET" ]; then
              echo "üîê Patching ServiceAccount: $sa"
              # Hole aktuelle ImagePullSecrets (falls vorhanden)
              CURRENT_IPS=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
              if [ -n "$CURRENT_IPS" ]; then
                # F√ºge acr-pull zu bestehenden Secrets hinzu
                if kubectl patch serviceaccount "$sa" -n monitoring \
                  --type='json' \
                  -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null; then
                  PATCHED_COUNT=$((PATCHED_COUNT + 1))
                  echo "  ‚úÖ Patched $sa (added to existing)"
                fi
              else
                # Erstelle neue ImagePullSecrets Liste
                if kubectl patch serviceaccount "$sa" -n monitoring \
                  -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null; then
                  PATCHED_COUNT=$((PATCHED_COUNT + 1))
                  echo "  ‚úÖ Patched $sa (created new)"
                fi
              fi
            else
              echo "  ‚úì ServiceAccount $sa already patched"
            fi
          done
          echo "‚úÖ Patched $PATCHED_COUNT ServiceAccounts"
          
          # Warte kurz, damit Patch angewendet ist
          sleep 3
          
          # Pr√ºfe ob fehlgeschlagene Jobs oder Pods existieren
          echo "üîç Checking for failed jobs and pods..."
          FAILED_JOBS=$(kubectl get jobs -n monitoring -o jsonpath='{.items[?(@.status.failed>0)].metadata.name}' 2>/dev/null || echo "")
          IMAGEPULL_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff" || @.status.containerStatuses[0].state.waiting.reason=="ErrImagePull")].metadata.name}' 2>/dev/null || echo "")
          
          # Restart Jobs/Pods, die ImagePullBackOff oder ErrImagePull haben oder fehlgeschlagen sind
          if [ -n "$FAILED_JOBS" ] || [ -n "$IMAGEPULL_PODS" ]; then
            echo "üîÑ Restarting failed jobs and pods..."
            
            # L√∂sche alle Admission-Jobs (werden automatisch neu erstellt)
            for job in $(kubectl get jobs -n monitoring -o name 2>/dev/null | grep -E "(admission|webhook)"); do
              echo "  üóëÔ∏è Deleting job: $job"
              kubectl delete "$job" -n monitoring --ignore-not-found=true --grace-period=0 --force || true
            done
            
            # L√∂sche auch alle Pods die ImagePullBackOff oder ErrImagePull haben (nicht nur Jobs)
            for pod in $IMAGEPULL_PODS; do
              echo "  üóëÔ∏è Deleting pod with ImagePullBackOff/ErrImagePull: $pod"
              kubectl delete pod "$pod" -n monitoring --ignore-not-found=true --grace-period=0 --force || true
            done
            
            # KRITISCH: L√∂sche ALLE Pods, die keine ImagePullSecrets haben (damit sie mit gepatchten ServiceAccounts neu erstellt werden)
            echo "üîÑ Checking for pods without ImagePullSecrets..."
            for pod in $(kubectl get pods -n monitoring -o name 2>/dev/null); do
              POD_IPS=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.imagePullSecrets[*].name}' 2>/dev/null || echo "")
              if [ -z "$POD_IPS" ]; then
                echo "  üóëÔ∏è Deleting pod without ImagePullSecrets: $pod"
                kubectl delete $pod -n monitoring --ignore-not-found=true --grace-period=0 --force || true
              fi
            done
            
            # Warte kurz, damit neue Jobs/Pods gestartet werden
            sleep 5
            
            # Zeige Status der Jobs
            echo "üìä Job status after restart:"
            kubectl get jobs -n monitoring | grep -E "(admission|webhook)" || echo "No admission jobs found"
          else
            echo "‚úÖ No failed jobs or ImagePullBackOff pods found - all resources are healthy"
          fi
          
          # KRITISCH: Pr√ºfe ob Pods noch von √∂ffentlichen Registries ziehen und patche ServiceAccounts erneut
          echo "üîç Checking if pods are pulling from ACR or public registries..."
          sleep 5  # Warte kurz, damit Pods erstellt sind
          
          # Finde Pods die noch von quay.io, docker.io oder anderen √∂ffentlichen Registries ziehen
          PUBLIC_REGISTRY_PODS=""
          for pod in $(kubectl get pods -n monitoring -o name 2>/dev/null); do
            # Pr√ºfe alle Container im Pod (auch Init-Container)
            CONTAINER_IMAGES=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.containers[*].image}' 2>/dev/null || echo "")
            INIT_IMAGES=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.initContainers[*].image}' 2>/dev/null || echo "")
            ALL_IMAGES="$CONTAINER_IMAGES $INIT_IMAGES"
            
            for image in $ALL_IMAGES; do
              if [[ "$image" == quay.io/* ]] || [[ "$image" == docker.io/* ]] || [[ "$image" == registry.k8s.io/* ]]; then
                if [[ "$PUBLIC_REGISTRY_PODS" != *"$pod"* ]]; then
                  if [ -z "$PUBLIC_REGISTRY_PODS" ]; then
                    PUBLIC_REGISTRY_PODS="$pod"
                  else
                    PUBLIC_REGISTRY_PODS="$PUBLIC_REGISTRY_PODS $pod"
                  fi
                  echo "  ‚ö†Ô∏è Pod $pod is pulling from public registry: $image"
                fi
              fi
            done
          done
          
          # Wenn Pods noch von √∂ffentlichen Registries ziehen, patche alle ServiceAccounts erneut
          if [ -n "$PUBLIC_REGISTRY_PODS" ]; then
            echo "‚ö†Ô∏è Found pods pulling from public registries. Patching all ServiceAccounts again..."
            PATCHED_AGAIN=0
            for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
              HAS_SECRET=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[?(@.name=="acr-pull")].name}' 2>/dev/null || echo "")
              if [ -z "$HAS_SECRET" ]; then
                echo "  üîê Patching ServiceAccount: $sa"
                CURRENT_IPS=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
                if [ -n "$CURRENT_IPS" ]; then
                  kubectl patch serviceaccount "$sa" -n monitoring \
                    --type='json' \
                    -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null && PATCHED_AGAIN=$((PATCHED_AGAIN + 1)) || true
                else
                  kubectl patch serviceaccount "$sa" -n monitoring \
                    -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null && PATCHED_AGAIN=$((PATCHED_AGAIN + 1)) || true
                fi
              fi
            done
            echo "‚úÖ Repatched $PATCHED_AGAIN ServiceAccounts"
            
            # L√∂sche Pods die von √∂ffentlichen Registries ziehen, damit sie neu erstellt werden
            echo "üóëÔ∏è Deleting pods pulling from public registries..."
            for pod in $PUBLIC_REGISTRY_PODS; do
              echo "  üóëÔ∏è Deleting pod: $pod"
              kubectl delete $pod -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
            
            echo "‚è≥ Waiting for pods to be recreated..."
            sleep 10
          else
            echo "‚úÖ All pods are pulling from ACR"
          fi
          
          # Finaler Status-Check
          echo "üìä Final pod status:"
          kubectl get pods -n monitoring -o wide || true
          
          # Zeige welche Images die Pods verwenden
          echo "üì¶ Pod images (should all be from ACR):"
          for pod in $(kubectl get pods -n monitoring -o name 2>/dev/null | head -10); do
            IMAGE=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.containers[0].image}' 2>/dev/null || echo "N/A")
            echo "  $pod: $IMAGE"
          done
          
          # Zeige ServiceAccounts Status
          echo "üìä ServiceAccounts with imagePullSecrets:"
          kubectl get serviceaccounts -n monitoring -o jsonpath='{range .items[*]}{.metadata.name}{": "}{.imagePullSecrets[*].name}{"\n"}{end}' || true

      - name: Collect debug logs from failed resources
        shell: bash
        run: |
          set -euo pipefail
          echo "üîç Collecting debug logs from failed resources..."
          
          # 1. Sammle Logs von fehlgeschlagenen Pods
          echo "üìã Collecting logs from failed/error pods..."
          ERROR_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.phase=="Failed" || @.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff" || @.status.containerStatuses[0].state.waiting.reason=="ErrImagePull" || @.status.containerStatuses[0].state.terminated.reason=="Error" || @.status.containerStatuses[0].state.waiting.reason=="CrashLoopBackOff")].metadata.name}' 2>/dev/null || echo "")
          
          if [ -n "$ERROR_PODS" ]; then
            for pod in $ERROR_PODS; do
              echo ""
              echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
              echo "üêõ DEBUG: Pod $pod"
              echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
              
              # Zeige Pod Details
              echo "üìã Pod Details:"
              kubectl get pod "$pod" -n monitoring -o wide || true
              
              # Zeige Pod Events
              echo ""
              echo "üìã Pod Events:"
              kubectl describe pod "$pod" -n monitoring | grep -A 20 "Events:" || true
              
              # Zeige Container Images
              echo ""
              echo "üì¶ Container Images:"
              kubectl get pod "$pod" -n monitoring -o jsonpath='{range .spec.containers[*]}{.name}{": "}{.image}{"\n"}{end}' || true
              kubectl get pod "$pod" -n monitoring -o jsonpath='{range .spec.initContainers[*]}{.name}{" (init): "}{.image}{"\n"}{end}' || true
              
              # Zeige ServiceAccount und ImagePullSecrets
              echo ""
              echo "üîê ServiceAccount Info:"
              SA=$(kubectl get pod "$pod" -n monitoring -o jsonpath='{.spec.serviceAccountName}' 2>/dev/null || echo "N/A")
              echo "  ServiceAccount: $SA"
              IPS=$(kubectl get pod "$pod" -n monitoring -o jsonpath='{.spec.imagePullSecrets[*].name}' 2>/dev/null || echo "none")
              echo "  ImagePullSecrets: $IPS"
              if [ "$SA" != "N/A" ] && [ "$SA" != "default" ]; then
                SA_IPS=$(kubectl get serviceaccount "$SA" -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "none")
                echo "  ServiceAccount ImagePullSecrets: $SA_IPS"
              fi
              
              # Versuche Logs zu sammeln (falls Container gestartet wurden)
              echo ""
              echo "üìã Container Logs (if available):"
              for container in $(kubectl get pod "$pod" -n monitoring -o jsonpath='{.spec.containers[*].name}' 2>/dev/null); do
                echo "  Container: $container"
                kubectl logs "$pod" -n monitoring -c "$container" --tail=50 2>&1 || echo "    (logs not available)"
              done
              
              # Zeige Image Pull Error Details
              echo ""
              echo "üîç Image Pull Status:"
              kubectl get pod "$pod" -n monitoring -o jsonpath='{range .status.containerStatuses[*]}{.name}{": "}{.state.waiting.reason}{" - "}{.state.waiting.message}{"\n"}{end}' || true
              kubectl get pod "$pod" -n monitoring -o jsonpath='{range .status.containerStatuses[*]}{.name}{": "}{.state.terminated.reason}{" - "}{.state.terminated.message}{"\n"}{end}' || true
            done
          else
            echo "‚úÖ No failed pods found"
          fi
          
          # 2. Sammle Logs von fehlgeschlagenen Jobs
          echo ""
          echo "üìã Collecting logs from failed jobs..."
          FAILED_JOBS=$(kubectl get jobs -n monitoring -o jsonpath='{.items[?(@.status.failed>0)].metadata.name}' 2>/dev/null || echo "")
          
          if [ -n "$FAILED_JOBS" ]; then
            for job in $FAILED_JOBS; do
              echo ""
              echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
              echo "üêõ DEBUG: Job $job"
              echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
              
              # Zeige Job Details
              echo "üìã Job Details:"
              kubectl get job "$job" -n monitoring -o wide || true
              kubectl describe job "$job" -n monitoring || true
              
              # Zeige Job Pod Logs
              echo ""
              echo "üìã Job Pod Logs:"
              JOB_PODS=$(kubectl get pods -n monitoring -l job-name="$job" -o name 2>/dev/null || echo "")
              if [ -n "$JOB_PODS" ]; then
                for pod in $JOB_PODS; do
                  echo "  Pod: $pod"
                  kubectl logs $pod -n monitoring --tail=100 2>&1 || echo "    (logs not available)"
                done
              else
                echo "  (no pods found for this job)"
              fi
            done
          else
            echo "‚úÖ No failed jobs found"
          fi
          
          # 3. Zeige Terminating Pods (f√ºr Info)
          echo ""
          echo "üìã Terminating pods (will be cleaned up):"
          TERMINATING_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.metadata.deletionTimestamp!=null)].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$TERMINATING_PODS" ]; then
            for pod in $TERMINATING_PODS; do
              echo "  ‚è≥ $pod (Terminating since: $(kubectl get pod "$pod" -n monitoring -o jsonpath='{.metadata.deletionTimestamp}' 2>/dev/null || echo "unknown"))"
            done
          else
            echo "  ‚úÖ No terminating pods"
          fi
          
          echo ""
          echo "‚úÖ Debug log collection completed"

      - name: Cleanup failed resources
        shell: bash
        run: |
          set -euo pipefail
          echo "üßπ Cleaning up failed and terminated resources..."
          
          # 1. L√∂sche alle Terminating Pods
          echo "üóëÔ∏è Cleaning up Terminating pods..."
          TERMINATING_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.metadata.deletionTimestamp!=null)].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$TERMINATING_PODS" ]; then
            for pod in $TERMINATING_PODS; do
              echo "  üóëÔ∏è Force deleting terminating pod: $pod"
              kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          fi
          
          # 2. L√∂sche alle Pods mit Error/ImagePullBackOff/ErrImagePull Status
          echo "üóëÔ∏è Cleaning up pods with Error/ImagePullBackOff/ErrImagePull status..."
          ERROR_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.phase=="Failed" || @.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff" || @.status.containerStatuses[0].state.waiting.reason=="ErrImagePull" || @.status.containerStatuses[0].state.terminated.reason=="Error")].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$ERROR_PODS" ]; then
            for pod in $ERROR_PODS; do
              echo "  üóëÔ∏è Deleting error pod: $pod"
              kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          fi
          
          # 3. L√∂sche alle fehlgeschlagenen Jobs
          echo "üóëÔ∏è Cleaning up failed jobs..."
          FAILED_JOBS=$(kubectl get jobs -n monitoring -o jsonpath='{.items[?(@.status.failed>0)].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$FAILED_JOBS" ]; then
            for job in $FAILED_JOBS; do
              echo "  üóëÔ∏è Deleting failed job: $job"
              kubectl delete job "$job" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          fi
          
          # 4. L√∂sche alle Completed Jobs (die nicht mehr ben√∂tigt werden, besonders admission/webhook)
          echo "üóëÔ∏è Cleaning up completed admission/webhook jobs..."
          for job in $(kubectl get jobs -n monitoring -o name 2>/dev/null | grep -E "(admission|webhook|patch)"); do
            JOB_STATUS=$(kubectl get $job -n monitoring -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null || echo "")
            if [ "$JOB_STATUS" = "True" ]; then
              echo "  üóëÔ∏è Deleting completed job: $job"
              kubectl delete $job -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            fi
          done
          
          # 5. L√∂sche alle Pods im CrashLoopBackOff Status
          echo "üóëÔ∏è Cleaning up CrashLoopBackOff pods..."
          CRASHLOOP_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.containerStatuses[0].state.waiting.reason=="CrashLoopBackOff")].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$CRASHLOOP_PODS" ]; then
            for pod in $CRASHLOOP_PODS; do
              echo "  üóëÔ∏è Deleting CrashLoopBackOff pod: $pod"
              kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          fi
          
          # 6. L√∂sche alle Pending Pods die l√§nger als 5 Minuten warten (wahrscheinlich h√§ngend)
          echo "üóëÔ∏è Cleaning up stuck pending pods..."
          PENDING_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.phase=="Pending")].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$PENDING_PODS" ]; then
            for pod in $PENDING_PODS; do
              POD_AGE=$(kubectl get pod "$pod" -n monitoring -o jsonpath='{.metadata.creationTimestamp}' 2>/dev/null || echo "")
              if [ -n "$POD_AGE" ]; then
                # Pr√ºfe ob Pod √§lter als 5 Minuten ist (vereinfacht - pr√ºft nur ob er existiert)
                echo "  üóëÔ∏è Deleting pending pod (may be stuck): $pod"
                kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
              fi
            done
          fi
          
          # 7. Warte kurz, damit Cleanup abgeschlossen ist
          sleep 5
          
          # 8. Finale Status-Anzeige
          echo "üìä Final cleanup status:"
          TERMINATING_COUNT=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.metadata.deletionTimestamp!=null)].metadata.name}' 2>/dev/null | wc -w || echo 0)
          ERROR_COUNT=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.phase=="Failed")].metadata.name}' 2>/dev/null | wc -w || echo 0)
          IMAGEPULL_COUNT=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff")].metadata.name}' 2>/dev/null | wc -w || echo 0)
          FAILED_JOBS_COUNT=$(kubectl get jobs -n monitoring -o jsonpath='{.items[?(@.status.failed>0)].metadata.name}' 2>/dev/null | wc -w || echo 0)
          
          echo "  Terminating pods: $TERMINATING_COUNT"
          echo "  Error pods: $ERROR_COUNT"
          echo "  ImagePullBackOff pods: $IMAGEPULL_COUNT"
          echo "  Failed jobs: $FAILED_JOBS_COUNT"
          
          if [ "$TERMINATING_COUNT" -eq 0 ] && [ "$ERROR_COUNT" -eq 0 ] && [ "$IMAGEPULL_COUNT" -eq 0 ] && [ "$FAILED_JOBS_COUNT" -eq 0 ]; then
            echo "‚úÖ All cleanup completed - workspace is clean!"
          else
            echo "‚ö†Ô∏è Some resources may still need cleanup"
          fi

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl get pods -n monitoring