name: Deploy Base Kubernetes-Infrastructure

on:
  push:
    branches: [ main ]
    paths:
      - 'infra/addons/**'
      - 'infra/rbac/**'
      - '.github/workflows/build-push.yml'
  workflow_dispatch:

jobs:
  mirror:
    runs-on: ubuntu-latest
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python venv
        shell: bash
        run: |
          set -euo pipefail
          python3 -m venv .venv
          source .venv/bin/activate
          pip install --upgrade pip PyYAML
          echo "VENV_PYTHON=$(pwd)/.venv/bin/python" >> "$GITHUB_ENV"

      - name: Login to ACR
        uses: docker/login-action@v3
        with:
          registry: ${{ secrets.ACR_REGISTRY }}
          username: ${{ secrets.ACR_USERNAME }}
          password: ${{ secrets.ACR_PASSWORD }}

      - name: Generate mirror artifacts
        id: map
        run: |
          set -euo pipefail
          "$VENV_PYTHON" scripts/generate_addon_artifacts.py
          echo "map-file=image-map.txt" >> "$GITHUB_OUTPUT"
          echo "values-file=infra/addons/values/observability.acr.yaml" >> "$GITHUB_OUTPUT"

      - name: Mirror container images
        run: |
          set -euo pipefail
          while IFS='|' read -r SOURCE_REPO TARGET_REPO TAG; do
            [[ -z "${SOURCE_REPO:-}" ]] && continue
            SOURCE_IMAGE="${SOURCE_REPO}:${TAG}"
            TARGET_IMAGE="${ACR_REGISTRY}/${TARGET_REPO}:${TAG}"
            echo "‚Üí Mirroring $SOURCE_IMAGE to $TARGET_IMAGE"
            docker pull "$SOURCE_IMAGE"
            docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
            docker push "$TARGET_IMAGE"
          done < image-map.txt

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: addon-mirror-metadata
          path: |
            image-map.txt
            infra/addons/values/observability.acr.yaml

  local-path:
    name: Deploy Local Path Provisioner
    needs: mirror
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Prepare namespace and pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns local-path-storage >/dev/null 2>&1 || kubectl create namespace local-path-storage
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n local-path-storage \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Check if Local Path Provisioner already exists
        id: check_provisioner
        shell: bash
        run: |
          set -euo pipefail
          # Check for deployment in local-path-storage namespace
          if kubectl get deployment local-path-provisioner -n local-path-storage >/dev/null 2>&1; then
            echo "exists=true" >> "$GITHUB_OUTPUT"
            echo "namespace=local-path-storage" >> "$GITHUB_OUTPUT"
            echo "‚úÖ Local Path Provisioner already exists in local-path-storage namespace"
            exit 0
          fi
          # Check for deployment in kube-system namespace (k3s default)
          if kubectl get deployment local-path-provisioner -n kube-system >/dev/null 2>&1; then
            echo "exists=true" >> "$GITHUB_OUTPUT"
            echo "namespace=kube-system" >> "$GITHUB_OUTPUT"
            echo "‚úÖ Local Path Provisioner already exists in kube-system namespace (k3s default)"
            exit 0
          fi
          # Check for StorageClass
          if kubectl get storageclass local-path >/dev/null 2>&1; then
            echo "storageclass_exists=true" >> "$GITHUB_OUTPUT"
            echo "‚úÖ StorageClass 'local-path' already exists"
          else
            echo "storageclass_exists=false" >> "$GITHUB_OUTPUT"
          fi
          echo "exists=false" >> "$GITHUB_OUTPUT"

      - name: Deploy Local Path Provisioner
        if: steps.check_provisioner.outputs.exists != 'true'
        shell: bash
        run: |
          set -euo pipefail
          curl -sL https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml | \
            sed "s|rancher/local-path-provisioner:v0.0.26|${ACR_REGISTRY}/rancher/local-path-provisioner:v0.0.26|g" | \
            kubectl apply -f -
          sleep 3
          kubectl patch serviceaccount local-path-provisioner-service-account -n local-path-storage \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          kubectl -n local-path-storage rollout status deployment/local-path-provisioner --timeout=5m

      - name: Fix k3s Local Path Provisioner RBAC and StorageClass
        shell: bash
        run: |
          set -euo pipefail
          # Check if k3s provisioner exists in kube-system namespace
          if kubectl get deployment local-path-provisioner -n kube-system >/dev/null 2>&1; then
            echo "üîß Fixing RBAC for k3s Local Path Provisioner in kube-system..."
            
            # Apply RBAC for local-path-provisioner-service-account
            cat <<EOF | kubectl apply -f -
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          metadata:
            name: local-path-provisioner-role
          rules:
          - apiGroups: [""]
            resources: ["nodes"]
            verbs: ["get", "list"]
          - apiGroups: [""]
            resources: ["persistentvolumes"]
            verbs: ["get", "list", "watch", "create", "delete"]
          - apiGroups: [""]
            resources: ["persistentvolumeclaims"]
            verbs: ["get", "list", "watch", "update"]
          - apiGroups: [""]
            resources: ["pods"]
            verbs: ["create", "delete", "get", "list", "watch"]
          - apiGroups: ["storage.k8s.io"]
            resources: ["storageclasses"]
            verbs: ["get", "list", "watch"]
          - apiGroups: [""]
            resources: ["events"]
            verbs: ["create", "patch"]
          - apiGroups: [""]
            resources: ["endpoints"]
            verbs: ["get", "list", "watch", "create", "update", "patch"]
          ---
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: local-path-provisioner-binding
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: local-path-provisioner-role
          subjects:
          - kind: ServiceAccount
            name: local-path-provisioner-service-account
            namespace: kube-system
          EOF
            echo "‚úÖ RBAC for k3s Local Path Provisioner applied"
          fi
          
          # Ensure StorageClass exists (for both k3s and manual deployments)
          if ! kubectl get storageclass local-path >/dev/null 2>&1; then
            echo "üì¶ Creating StorageClass local-path..."
            cat <<EOF | kubectl apply -f -
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: local-path
            annotations:
              storageclass.kubernetes.io/is-default-class: "true"
          provisioner: rancher.io/local-path
          volumeBindingMode: WaitForFirstConsumer
          reclaimPolicy: Delete
          EOF
            echo "‚úÖ StorageClass local-path created"
          else
            echo "‚úÖ StorageClass local-path already exists"
          fi

      - name: Ensure StorageClass is default
        shell: bash
        run: |
          set -euo pipefail
          if kubectl get storageclass local-path >/dev/null 2>&1; then
            kubectl patch storageclass local-path \
              -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' \
              || echo "StorageClass already default"
          else
            echo "‚ö†Ô∏è StorageClass 'local-path' not found, skipping default annotation"
          fi

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl get pods -n local-path-storage
          kubectl get storageclass

  metrics:
    name: Deploy Metrics Server
    needs: local-path
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Ensure pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n kube-system \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl patch serviceaccount default -n kube-system \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true

      - name: Deploy Metrics Server
        shell: bash
        run: |
          set -euo pipefail
          echo "üì¶ Deploying Metrics Server from latest GitHub release..."
          curl -sL https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml | \
            sed "s|registry.k8s.io/metrics-server/metrics-server|${ACR_REGISTRY}/metrics-server/metrics-server|g" | \
            kubectl apply -f -
          
          # Patch metrics-server ServiceAccount with pull secret (must be done after deployment creates the SA)
          echo "üîê Patching metrics-server ServiceAccount with ACR pull secret..."
          sleep 2
          kubectl patch serviceaccount metrics-server -n kube-system \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          
          # Restart deployment to pick up the new pull secret
          kubectl rollout restart deployment/metrics-server -n kube-system || true
          
          echo "‚è≥ Waiting for Metrics Server to be ready..."
          kubectl -n kube-system rollout status deployment/metrics-server --timeout=5m

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n kube-system get deploy metrics-server
          kubectl top nodes || echo "Metrics may take a few moments to appear."

  ingress:
    name: Deploy Ingress Controller
    needs: metrics
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Remove Traefik (k3s default) to avoid conflicts
        shell: bash
        run: |
          set -euo pipefail
          echo "üóëÔ∏è Removing Traefik components to avoid port conflicts with NGINX..."
          
          # Delete Traefik Service (LoadBalancer)
          kubectl delete -n kube-system svc traefik --ignore-not-found=true || true
          
          # Delete Traefik Deployment
          kubectl delete -n kube-system deployment traefik --ignore-not-found=true || true
          
          # Delete Traefik DaemonSet (svclb)
          kubectl delete -n kube-system daemonset svclb-traefik --ignore-not-found=true || true
          
          # Delete Traefik Helm Install Jobs (CRD and main install) - delete all matching jobs
          kubectl delete -n kube-system jobs -l app=helm,name=traefik --ignore-not-found=true || true
          kubectl delete -n kube-system job helm-install-traefik-crd --ignore-not-found=true || true
          kubectl delete -n kube-system job helm-install-traefik --ignore-not-found=true || true
          
          # Wait a bit for resources to be cleaned up
          sleep 5
          
          echo "‚úÖ Traefik components and Helm install jobs removed (if they existed)"
          echo "‚ÑπÔ∏è  Note: Traefik jobs will be deleted on each workflow run. To permanently disable Traefik,"
          echo "   manually add '--disable traefik' to k3s startup flags or configure /etc/rancher/k3s/config.yaml"

      - name: Ensure namespace and pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns ingress-nginx >/dev/null 2>&1 || kubectl create namespace ingress-nginx
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n ingress-nginx \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl patch serviceaccount default -n ingress-nginx \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true

      - name: Install helm dependencies
        shell: bash
        run: |
          set -euo pipefail
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo update

      - name: Deploy ingress controller
        shell: bash
        run: |
          set -euo pipefail
          ACR_IMAGE="${ACR_REGISTRY}/ingress-nginx/controller:v1.9.4"
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            -n ingress-nginx \
            -f infra/addons/values/ingress-nginx.yaml \
            --set controller.image.image="$ACR_IMAGE" \
            --timeout 5m
          kubectl patch serviceaccount ingress-nginx -n ingress-nginx \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          kubectl rollout status deployment/ingress-nginx-controller -n ingress-nginx --timeout=5m

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl get pods -n ingress-nginx

  monitoring:
    name: Deploy Monitoring Stack
    needs: ingress
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download mirror artifacts
        uses: actions/download-artifact@v4
        with:
          name: addon-mirror-metadata

      - name: Restore generated values
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p infra/addons/values
          # Artifacts preserve directory structure, so check both locations
          if [ -f "infra/addons/values/observability.acr.yaml" ]; then
            echo "‚úÖ observability.acr.yaml found in infra/addons/values/"
          elif [ -f "observability.acr.yaml" ]; then
            cp observability.acr.yaml infra/addons/values/observability.acr.yaml
            echo "‚úÖ observability.acr.yaml copied to infra/addons/values/"
          else
            echo "‚ö†Ô∏è observability.acr.yaml not found, will be generated if needed"
          fi

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Prepare namespace and pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns monitoring >/dev/null 2>&1 || kubectl create namespace monitoring
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n monitoring \
            --dry-run=client -o yaml | kubectl apply -f -
          
          # Patch default ServiceAccount
          kubectl patch serviceaccount default -n monitoring \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          
          # Patch kube-prometheus-stack-admission ServiceAccount (wird vom Chart erstellt)
          kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          
          # Patch alle ServiceAccounts im monitoring Namespace (f√ºr zuk√ºnftige)
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
            echo "Patching ServiceAccount: $sa"
            kubectl patch serviceaccount "$sa" -n monitoring \
              -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          done

      - name: Install helm dependencies
        shell: bash
        run: |
          set -euo pipefail
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add grafana https://grafana.github.io/helm-charts
          helm repo update

      - name: Deploy monitoring stack
        shell: bash
        run: |
          set -euo pipefail
          
          # Debug: Zeige die verwendete Values-Datei
          echo "üìã Using values file:"
          cat infra/addons/values/observability.acr.yaml | head -50
          
          # Debug: Pr√ºfe aktuelle Pods vor Deployment
          echo "üìä Current pods in monitoring namespace:"
          kubectl get pods -n monitoring || true
          
          # Bereinige alte fehlgeschlagene Admission-Jobs
          echo "üßπ Cleaning up old failed admission jobs..."
          # L√∂sche Jobs (auch Terminating) - force delete f√ºr h√§ngende Jobs
          kubectl delete jobs -n monitoring -l app.kubernetes.io/name=kube-prometheus-stack,app.kubernetes.io/component=admission-webhook --ignore-not-found=true --grace-period=0 --force || true
          # L√∂sche Pods direkt (auch Terminating) - wichtig, da Pods manchmal h√§ngen bleiben
          kubectl get pods -n monitoring -l app.kubernetes.io/name=kube-prometheus-stack,app.kubernetes.io/component=admission-webhook -o name 2>/dev/null | xargs -r kubectl delete -n monitoring --grace-period=0 --force --ignore-not-found=true || true
          
          # Warte l√§nger, damit Cleanup abgeschlossen ist
          sleep 5
          
          # Deploy asynchron (ohne --wait, damit Workflow nicht h√§ngt)
          echo "üöÄ Deploying monitoring stack (asynchron)..."
          
          # Aggressives Cleanup f√ºr stuck Helm-Releases (IMMER ausf√ºhren)
          echo "üîß Force cleaning Helm locks and stuck releases..."
          
          # Versuche normale Uninstall (mit --no-hooks um Hooks zu √ºberspringen)
          helm uninstall kube-prometheus-stack -n monitoring --no-hooks --ignore-not-found=true 2>/dev/null || true
          
          # Debug: Zeige alle Secrets im Namespace
          echo "üìã Current secrets in monitoring namespace:"
          kubectl get secrets -n monitoring -o name 2>/dev/null || true
          
          # L√∂sche ALLE Helm-Secrets die mit dem Release zu tun haben
          echo "üóëÔ∏è Deleting Helm release secrets..."
          # Helm 3 Format: sh.helm.release.v1.<release-name>.<revision>
          kubectl get secrets -n monitoring -o name 2>/dev/null | grep -E "sh\.helm\.release\.v1\.kube-prometheus-stack\." | xargs -r kubectl delete -n monitoring --ignore-not-found=true || true
          # Alternative Formate
          kubectl get secrets -n monitoring -o name 2>/dev/null | grep -i "helm.*kube-prometheus-stack" | xargs -r kubectl delete -n monitoring --ignore-not-found=true || true
          # Mit Labels
          kubectl delete secret -n monitoring -l owner=helm,name=kube-prometheus-stack --ignore-not-found=true || true
          kubectl delete secret -n monitoring -l name=kube-prometheus-stack --ignore-not-found=true || true
          
          # L√∂sche ConfigMaps
          echo "üóëÔ∏è Deleting Helm release configmaps..."
          kubectl delete configmap -n monitoring -l owner=helm,name=kube-prometheus-stack --ignore-not-found=true || true
          kubectl get configmaps -n monitoring -o name 2>/dev/null | grep "kube-prometheus-stack" | xargs -r kubectl delete -n monitoring --ignore-not-found=true || true
          
          # Warte damit Cleanup abgeschlossen ist
          sleep 10
          
          # Debug: Zeige ob noch Secrets vorhanden sind
          echo "üìã Remaining secrets after cleanup:"
          kubectl get secrets -n monitoring -o name 2>/dev/null | grep -E "(helm|kube-prometheus)" || echo "No Helm secrets found"
          
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            -n monitoring \
            -f infra/addons/values/observability.acr.yaml \
            --timeout 10m
          
          echo "‚úÖ Helm deployment command completed. Resources are being created in the background."
          
          # KRITISCH: Patch ServiceAccounts SOFORT nach Helm-Deployment (bevor Jobs starten!)
          echo "üîê CRITICAL: Patching ServiceAccounts IMMEDIATELY after Helm deployment..."
          echo "‚ö†Ô∏è This must happen BEFORE admission jobs start, otherwise they will fail with ImagePullBackOff"
          
          # Warte bis ServiceAccounts existieren (mit Retry) - aber nicht zu lange!
          MAX_SA_RETRIES=15  # 15 * 2s = 30 Sekunden max
          SA_RETRY_COUNT=0
          SERVICE_ACCOUNTS_FOUND=false
          
          while [ $SA_RETRY_COUNT -lt $MAX_SA_RETRIES ]; do
            SERVICE_ACCOUNTS=$(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | grep -v "default" | wc -l || echo "0")
            if [ "$SERVICE_ACCOUNTS" -gt "0" ]; then
              SERVICE_ACCOUNTS_FOUND=true
              echo "‚úÖ Found $SERVICE_ACCOUNTS ServiceAccounts to patch"
              break
            fi
            SA_RETRY_COUNT=$((SA_RETRY_COUNT + 1))
            echo "‚è≥ Waiting for ServiceAccounts to be created ($SA_RETRY_COUNT/$MAX_SA_RETRIES)..."
            sleep 2  # Kurzere Intervalle f√ºr schnelleres Patching
          done
          
          if [ "$SERVICE_ACCOUNTS_FOUND" = true ]; then
            # Patch alle ServiceAccounts SOFORT
            PATCHED_COUNT=0
            for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2 | grep -v "^default$"); do
              echo "üîê Patching ServiceAccount: $sa"
              if kubectl patch serviceaccount "$sa" -n monitoring \
                -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null; then
                PATCHED_COUNT=$((PATCHED_COUNT + 1))
                echo "  ‚úÖ Patched $sa"
              else
                echo "  ‚ö†Ô∏è Failed to patch $sa (may not exist yet)"
              fi
            done
            echo "‚úÖ Patched $PATCHED_COUNT ServiceAccounts"
          else
            echo "‚ö†Ô∏è No ServiceAccounts found after waiting - they may be created later"
          fi
          
          # L√∂sche bereits laufende Admission-Jobs, damit sie mit gepatchten ServiceAccounts neu erstellt werden
          echo "üîÑ Deleting any existing admission jobs to trigger restart with patched ServiceAccounts..."
          kubectl delete jobs -n monitoring -l app.kubernetes.io/component=admission-webhook --ignore-not-found=true || true
          kubectl delete jobs -n monitoring -l app.kubernetes.io/name=kube-prometheus-stack,app.kubernetes.io/component=admission-webhook --ignore-not-found=true || true
          sleep 3  # Kurz warten, damit Jobs gel√∂scht sind
          
          # Pr√ºfe ob Ressourcen erstellt wurden (falls nicht, ist Helm stuck)
          echo "üîç Checking if resources were created..."
          sleep 2  # Kurz warten, damit Helm Ressourcen erstellt hat
          
          OPERATOR_EXISTS=$(kubectl get deployment kube-prometheus-stack-operator -n monitoring -o name 2>/dev/null || echo "")
          if [ -z "$OPERATOR_EXISTS" ]; then
            echo "‚ö†Ô∏è Operator deployment not found! Helm may be stuck. Checking status..."
            echo "üìä Current resources in monitoring namespace:"
            kubectl get all -n monitoring || true
            echo "üìä Current jobs:"
            kubectl get jobs -n monitoring || true
            
            # Pr√ºfe ob fehlgeschlagene Admission-Jobs das Deployment blockieren
            FAILED_JOBS=$(kubectl get jobs -n monitoring -o jsonpath='{.items[?(@.status.failed>0)].metadata.name}' 2>/dev/null || echo "")
            if [ -n "$FAILED_JOBS" ]; then
              echo "üîß Found failed jobs blocking deployment. Cleaning up..."
              kubectl delete jobs -n monitoring --all --ignore-not-found=true || true
              sleep 5
            fi
            
            # Versuche nochmal zu warten (manchmal dauert es etwas l√§nger)
            echo "‚è≥ Waiting additional 30s for resources to be created..."
            sleep 30
            
            OPERATOR_EXISTS=$(kubectl get deployment kube-prometheus-stack-operator -n monitoring -o name 2>/dev/null || echo "")
            if [ -z "$OPERATOR_EXISTS" ]; then
              echo "‚ùå Operator still not found after cleanup. Helm deployment may have failed."
              echo "üìä Current state:"
              kubectl get all -n monitoring || true
              echo "‚ö†Ô∏è Continuing anyway - resources may be created later..."
            fi
          fi
          
          # Warte gezielt auf den Operator (wichtigster Teil) - mit Retry-Logik
          echo "‚è≥ Waiting for operator to be ready..."
          MAX_RETRIES=12  # 12 * 10s = 2 Minuten
          RETRY_COUNT=0
          OPERATOR_READY=false
          
          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            if kubectl rollout status deployment/kube-prometheus-stack-operator -n monitoring --timeout=10s 2>/dev/null; then
              OPERATOR_READY=true
              echo "‚úÖ Operator is ready!"
              break
            fi
            RETRY_COUNT=$((RETRY_COUNT + 1))
            echo "‚è≥ Operator not ready yet (attempt $RETRY_COUNT/$MAX_RETRIES)..."
            sleep 10
          done
          
          if [ "$OPERATOR_READY" = false ]; then
            echo "‚ö†Ô∏è Operator rollout status check failed or timed out"
            echo "üìä Checking operator pod status:"
            kubectl get pods -n monitoring -l app.kubernetes.io/name=kube-prometheus-stack,app.kubernetes.io/component=operator || true
            echo "‚ö†Ô∏è Continuing anyway - ServiceAccounts are already patched..."
          fi
          
          # Patch ServiceAccounts nochmal (falls neue erstellt wurden)
          echo "üîê Re-patching ServiceAccounts (in case new ones were created)..."
          sleep 2
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2 | grep -v "^default$"); do
            echo "Re-patching ServiceAccount: $sa"
            kubectl patch serviceaccount "$sa" -n monitoring \
              -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          done
          
          # Zeige Status aller Pods
          echo "üìä Current pod status:"
          kubectl get pods -n monitoring -o wide || true

      - name: Patch ServiceAccounts after deployment and restart failed jobs
        shell: bash
        run: |
          set -euo pipefail
          echo "üîê Patching all ServiceAccounts in monitoring namespace with ACR pull secret..."
          
          # Patch alle ServiceAccounts, die Helm erstellt hat (falls noch nicht gepatcht)
          PATCHED_COUNT=0
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2 | grep -v "^default$"); do
            echo "Patching ServiceAccount: $sa"
            if kubectl patch serviceaccount "$sa" -n monitoring \
              -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null; then
              PATCHED_COUNT=$((PATCHED_COUNT + 1))
            fi
          done
          echo "‚úÖ Patched $PATCHED_COUNT ServiceAccounts"
          
          # Warte kurz, damit Patch angewendet ist
          sleep 3
          
          # Pr√ºfe ob fehlgeschlagene Jobs existieren
          echo "üîç Checking for failed jobs..."
          FAILED_JOBS=$(kubectl get jobs -n monitoring -o jsonpath='{.items[?(@.status.failed>0)].metadata.name}' 2>/dev/null || echo "")
          IMAGEPULL_JOBS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff")].metadata.labels.job-name}' 2>/dev/null | tr ' ' '\n' | sort -u || echo "")
          
          # Restart Jobs/Pods, die ImagePullBackOff haben oder fehlgeschlagen sind
          if [ -n "$FAILED_JOBS" ] || [ -n "$IMAGEPULL_JOBS" ]; then
            echo "üîÑ Restarting failed admission jobs..."
            
            # L√∂sche alle Admission-Jobs (werden automatisch neu erstellt)
            for job in $(kubectl get jobs -n monitoring -o name 2>/dev/null | grep -E "(admission|webhook)"); do
              echo "Deleting job to trigger restart: $job"
              kubectl delete "$job" -n monitoring --ignore-not-found=true --grace-period=0 --force || true
            done
            
            # L√∂sche auch Pods die ImagePullBackOff haben
            for pod in $(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff")].metadata.name}' 2>/dev/null); do
              echo "Deleting pod with ImagePullBackOff: $pod"
              kubectl delete pod "$pod" -n monitoring --ignore-not-found=true --grace-period=0 --force || true
            done
            
            # Warte kurz, damit neue Jobs gestartet werden
            sleep 5
            
            # Zeige Status der Jobs
            echo "üìä Admission job status after restart:"
            kubectl get jobs -n monitoring | grep -E "(admission|webhook)" || echo "No admission jobs found"
          else
            echo "‚úÖ No failed jobs found - all jobs are healthy"
          fi
          
          # Finaler Status-Check
          echo "üìä Final pod status:"
          kubectl get pods -n monitoring -o wide || true

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl get pods -n monitoring