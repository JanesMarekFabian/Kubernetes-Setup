name: Deploy Base Kubernetes-Infrastructure

on:
  push:
    branches: [ main ]
    paths:
      - 'infra/addons/**'
      - 'infra/rbac/**'
      - '.github/workflows/build-push.yml'
  workflow_dispatch:

jobs:
  mirror:
    runs-on: ubuntu-latest
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python venv
        shell: bash
        run: |
          set -euo pipefail
          python3 -m venv .venv
          source .venv/bin/activate
          pip install --upgrade pip PyYAML
          echo "VENV_PYTHON=$(pwd)/.venv/bin/python" >> "$GITHUB_ENV"

      - name: Login to ACR
        uses: docker/login-action@v3
        with:
          registry: ${{ secrets.ACR_REGISTRY }}
          username: ${{ secrets.ACR_USERNAME }}
          password: ${{ secrets.ACR_PASSWORD }}

      - name: Setup Helm
        shell: bash
        run: |
          set -euo pipefail
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          helm version
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update prometheus-community

      - name: Generate mirror artifacts
        id: map
        run: |
          set -euo pipefail
          "$VENV_PYTHON" scripts/generate_addon_artifacts.py
          echo "map-file=image-map.txt" >> "$GITHUB_OUTPUT"
          echo "values-file=infra/addons/values/observability.acr.yaml" >> "$GITHUB_OUTPUT"
          
          # VALIDIERUNG: PrÃ¼fe ob observability.acr.yaml alle Images korrekt auf ACR umgeschrieben hat
          echo "ğŸ” Validating observability.acr.yaml - checking if all images are rewritten to ACR..."
          if [ -f "infra/addons/values/observability.acr.yaml" ]; then
            # PrÃ¼fe ob noch quay.io, docker.io oder registry.k8s.io in der Datei sind (sollte nicht vorkommen)
            PUBLIC_REGISTRIES=$(grep -E "(quay\.io|docker\.io|registry\.k8s\.io)" infra/addons/values/observability.acr.yaml 2>/dev/null || echo "")
            if [ -n "$PUBLIC_REGISTRIES" ]; then
              echo "âš ï¸ WARNING: Found public registries in observability.acr.yaml:"
              echo "$PUBLIC_REGISTRIES"
              echo "âš ï¸ These should be normalized (registry prefix removed) and registry field set to ACR"
            else
              echo "âœ… No public registries found in observability.acr.yaml (all images should use ACR registry)"
            fi
            
            # PrÃ¼fe ob alle registry-Felder auf ACR gesetzt sind
            MISSING_REGISTRY=$(grep -A 2 "repository:" infra/addons/values/observability.acr.yaml | grep -B 1 "repository:" | grep -v "registry:" | grep "repository:" | head -1 || echo "")
            if [ -n "$MISSING_REGISTRY" ]; then
              echo "âš ï¸ WARNING: Some image entries may be missing registry field"
            else
              echo "âœ… All image entries have registry field set"
            fi
            
            # Zeige ein paar Beispiel-Images aus der Datei
            echo "ğŸ“‹ Sample images from observability.acr.yaml:"
            grep -A 3 "repository:" infra/addons/values/observability.acr.yaml | head -20 || true
          else
            echo "âš ï¸ WARNING: observability.acr.yaml not found!"
          fi
          
          # KRITISCH: Extrahiere ALLE Images die Helm Chart tatsÃ¤chlich verwenden wird (inkl. Defaults)
          echo "ğŸ” Extracting all images from Helm Chart (including defaults)..."
          
          # Template das Chart mit unseren Values, um alle tatsÃ¤chlich verwendeten Images zu sehen
          if [ -f "infra/addons/values/observability.acr.yaml" ]; then
            VALUES_FILE="infra/addons/values/observability.acr.yaml"
          elif [ -f "infra/addons/values/observability.yaml" ]; then
            VALUES_FILE="infra/addons/values/observability.yaml"
          else
            VALUES_FILE=""
          fi
          
          if [ -n "$VALUES_FILE" ]; then
            # Template das Chart (ohne es zu installieren)
            HELM_TEMPLATE_OUTPUT=$(helm template kube-prometheus-stack prometheus-community/kube-prometheus-stack \
              -n monitoring \
              -f "$VALUES_FILE" 2>/dev/null || echo "")
            
            if [ -n "$HELM_TEMPLATE_OUTPUT" ]; then
              # Extrahiere alle Image-Referenzen aus dem gerenderten YAML
              HELM_IMAGES=$(echo "$HELM_TEMPLATE_OUTPUT" | grep -E "^\s+image:" | sed 's/.*image:[[:space:]]*//' | sed 's/"//g' | sed "s/'//g" | sort -u || echo "")
              
              if [ -n "$HELM_IMAGES" ]; then
                echo "ğŸ“¦ Found images from Helm template:"
                echo "$HELM_IMAGES" | head -20
                
                # FÃ¼ge fehlende Images zum image-map.txt hinzu
                MISSING_COUNT=0
                while IFS= read -r image; do
                  [[ -z "$image" ]] && continue
                  
                  # Parse image (kann registry/repo:tag oder repo:tag sein)
                  if [[ "$image" == *"/"* ]]; then
                    # Hat Registry: registry/repo:tag
                    REGISTRY=$(echo "$image" | cut -d'/' -f1)
                    REPO_AND_TAG=$(echo "$image" | cut -d'/' -f2-)
                  else
                    # Keine Registry: repo:tag
                    REGISTRY=""
                    REPO_AND_TAG="$image"
                  fi
                  
                  if [[ "$REPO_AND_TAG" == *":"* ]]; then
                    REPO=$(echo "$REPO_AND_TAG" | cut -d':' -f1)
                    TAG=$(echo "$REPO_AND_TAG" | cut -d':' -f2-)
                  else
                    REPO="$REPO_AND_TAG"
                    TAG="latest"
                  fi
                  
                  # Normalisiere Registry
                  if [[ "$REGISTRY" == "docker.io" ]] || [[ "$REGISTRY" == "quay.io" ]] || [[ "$REGISTRY" == "registry.k8s.io" ]]; then
                    SOURCE_REPO="${REGISTRY}/${REPO}"
                    TARGET_REPO="$REPO"
                  elif [ -z "$REGISTRY" ]; then
                    SOURCE_REPO="docker.io/${REPO}"
                    TARGET_REPO="$REPO"
                  else
                    # Bereits ACR oder andere Registry
                    continue
                  fi
                  
                  # PrÃ¼fe ob Image bereits im image-map.txt ist (flexibler Match)
                  if ! grep -qE "^${SOURCE_REPO}\|.*\|${TAG}$|^.*\|${TARGET_REPO}\|${TAG}$" image-map.txt 2>/dev/null; then
                    echo "${SOURCE_REPO}|${TARGET_REPO}|${TAG}" >> image-map.txt
                    echo "  â• Added missing image: ${SOURCE_REPO}:${TAG} -> ${ACR_REGISTRY}/${TARGET_REPO}:${TAG}"
                    MISSING_COUNT=$((MISSING_COUNT + 1))
                  fi
                done <<< "$HELM_IMAGES"
                
                if [ $MISSING_COUNT -gt 0 ]; then
                  echo "âœ… Added $MISSING_COUNT missing images from Helm Chart defaults to image-map.txt"
                  echo "ğŸ“‹ Updated image-map.txt now contains:"
                  wc -l image-map.txt || true
                else
                  echo "âœ… All images from Helm Chart are already in image-map.txt"
                fi
              else
                echo "âš ï¸ No images found in Helm template output"
              fi
            else
              echo "âš ï¸ Helm template failed or returned empty output"
            fi
          else
            echo "âš ï¸ Values file not found, skipping Helm template extraction"
          fi

      - name: Mirror container images
        run: |
          set -euo pipefail
          while IFS='|' read -r SOURCE_REPO TARGET_REPO TAG; do
            [[ -z "${SOURCE_REPO:-}" ]] && continue
            SOURCE_IMAGE="${SOURCE_REPO}:${TAG}"
            TARGET_IMAGE="${ACR_REGISTRY}/${TARGET_REPO}:${TAG}"
            echo "â†’ Mirroring $SOURCE_IMAGE to $TARGET_IMAGE"
            docker pull "$SOURCE_IMAGE"
            docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
            docker push "$TARGET_IMAGE"
          done < image-map.txt

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: addon-mirror-metadata
          path: |
            image-map.txt
            infra/addons/values/observability.acr.yaml

  local-path:
    name: Deploy Local Path Provisioner
    needs: mirror
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Prepare namespace and pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns local-path-storage >/dev/null 2>&1 || kubectl create namespace local-path-storage
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n local-path-storage \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Check if Local Path Provisioner already exists
        id: check_provisioner
        shell: bash
        run: |
          set -euo pipefail
          # Check for deployment in local-path-storage namespace
          if kubectl get deployment local-path-provisioner -n local-path-storage >/dev/null 2>&1; then
            echo "exists=true" >> "$GITHUB_OUTPUT"
            echo "namespace=local-path-storage" >> "$GITHUB_OUTPUT"
            echo "âœ… Local Path Provisioner already exists in local-path-storage namespace"
            exit 0
          fi
          # Check for deployment in kube-system namespace (k3s default)
          if kubectl get deployment local-path-provisioner -n kube-system >/dev/null 2>&1; then
            echo "exists=true" >> "$GITHUB_OUTPUT"
            echo "namespace=kube-system" >> "$GITHUB_OUTPUT"
            echo "âœ… Local Path Provisioner already exists in kube-system namespace (k3s default)"
            exit 0
          fi
          # Check for StorageClass
          if kubectl get storageclass local-path >/dev/null 2>&1; then
            echo "storageclass_exists=true" >> "$GITHUB_OUTPUT"
            echo "âœ… StorageClass 'local-path' already exists"
          else
            echo "storageclass_exists=false" >> "$GITHUB_OUTPUT"
          fi
          echo "exists=false" >> "$GITHUB_OUTPUT"

      - name: Deploy Local Path Provisioner
        if: steps.check_provisioner.outputs.exists != 'true'
        shell: bash
        run: |
          set -euo pipefail
          curl -sL https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml | \
            sed "s|rancher/local-path-provisioner:v0.0.26|${ACR_REGISTRY}/rancher/local-path-provisioner:v0.0.26|g" | \
            kubectl apply -f -
          sleep 3
          kubectl patch serviceaccount local-path-provisioner-service-account -n local-path-storage \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          kubectl -n local-path-storage rollout status deployment/local-path-provisioner --timeout=5m

      - name: Fix k3s Local Path Provisioner RBAC and StorageClass
        shell: bash
        run: |
          set -euo pipefail
          # Check if k3s provisioner exists in kube-system namespace
          if kubectl get deployment local-path-provisioner -n kube-system >/dev/null 2>&1; then
            echo "ğŸ”§ Fixing RBAC for k3s Local Path Provisioner in kube-system..."
            
            # Apply RBAC for local-path-provisioner-service-account
            cat <<EOF | kubectl apply -f -
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          metadata:
            name: local-path-provisioner-role
          rules:
          - apiGroups: [""]
            resources: ["nodes"]
            verbs: ["get", "list"]
          - apiGroups: [""]
            resources: ["persistentvolumes"]
            verbs: ["get", "list", "watch", "create", "delete"]
          - apiGroups: [""]
            resources: ["persistentvolumeclaims"]
            verbs: ["get", "list", "watch", "update"]
          - apiGroups: [""]
            resources: ["pods"]
            verbs: ["create", "delete", "get", "list", "watch"]
          - apiGroups: ["storage.k8s.io"]
            resources: ["storageclasses"]
            verbs: ["get", "list", "watch"]
          - apiGroups: [""]
            resources: ["events"]
            verbs: ["create", "patch"]
          - apiGroups: [""]
            resources: ["endpoints"]
            verbs: ["get", "list", "watch", "create", "update", "patch"]
          ---
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: local-path-provisioner-binding
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: local-path-provisioner-role
          subjects:
          - kind: ServiceAccount
            name: local-path-provisioner-service-account
            namespace: kube-system
          EOF
            echo "âœ… RBAC for k3s Local Path Provisioner applied"
          fi
          
          # Ensure StorageClass exists (for both k3s and manual deployments)
          if ! kubectl get storageclass local-path >/dev/null 2>&1; then
            echo "ğŸ“¦ Creating StorageClass local-path..."
            cat <<EOF | kubectl apply -f -
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: local-path
            annotations:
              storageclass.kubernetes.io/is-default-class: "true"
          provisioner: rancher.io/local-path
          volumeBindingMode: WaitForFirstConsumer
          reclaimPolicy: Delete
          EOF
            echo "âœ… StorageClass local-path created"
          else
            echo "âœ… StorageClass local-path already exists"
          fi

      - name: Ensure StorageClass is default
        shell: bash
        run: |
          set -euo pipefail
          if kubectl get storageclass local-path >/dev/null 2>&1; then
            kubectl patch storageclass local-path \
              -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' \
              || echo "StorageClass already default"
          else
            echo "âš ï¸ StorageClass 'local-path' not found, skipping default annotation"
          fi

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl get pods -n local-path-storage
          kubectl get storageclass

  metrics:
    name: Deploy Metrics Server
    needs: local-path
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Ensure pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n kube-system \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl patch serviceaccount default -n kube-system \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true

      - name: Deploy Metrics Server
        shell: bash
        run: |
          set -euo pipefail
          echo "ğŸ“¦ Deploying Metrics Server from latest GitHub release..."
          curl -sL https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml | \
            sed "s|registry.k8s.io/metrics-server/metrics-server|${ACR_REGISTRY}/metrics-server/metrics-server|g" | \
            kubectl apply -f -
          
          # Patch metrics-server ServiceAccount with pull secret (must be done after deployment creates the SA)
          echo "ğŸ” Patching metrics-server ServiceAccount with ACR pull secret..."
          sleep 2
          kubectl patch serviceaccount metrics-server -n kube-system \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          
          # Restart deployment to pick up the new pull secret
          kubectl rollout restart deployment/metrics-server -n kube-system || true
          
          echo "â³ Waiting for Metrics Server to be ready..."
          kubectl -n kube-system rollout status deployment/metrics-server --timeout=5m

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n kube-system get deploy metrics-server
          kubectl top nodes || echo "Metrics may take a few moments to appear."

  ingress:
    name: Deploy Ingress Controller
    needs: metrics
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Remove Traefik (k3s default) to avoid conflicts
        shell: bash
        run: |
          set -euo pipefail
          echo "ğŸ—‘ï¸ Removing Traefik components to avoid port conflicts with NGINX..."
          
          # Delete Traefik Service (LoadBalancer)
          kubectl delete -n kube-system svc traefik --ignore-not-found=true || true
          
          # Delete Traefik Deployment
          kubectl delete -n kube-system deployment traefik --ignore-not-found=true || true
          
          # Delete Traefik DaemonSet (svclb)
          kubectl delete -n kube-system daemonset svclb-traefik --ignore-not-found=true || true
          
          # Delete Traefik Helm Install Jobs (CRD and main install) - delete all matching jobs
          kubectl delete -n kube-system jobs -l app=helm,name=traefik --ignore-not-found=true || true
          kubectl delete -n kube-system job helm-install-traefik-crd --ignore-not-found=true || true
          kubectl delete -n kube-system job helm-install-traefik --ignore-not-found=true || true
          
          # Wait a bit for resources to be cleaned up
          sleep 5
          
          echo "âœ… Traefik components and Helm install jobs removed (if they existed)"
          echo "â„¹ï¸  Note: Traefik jobs will be deleted on each workflow run. To permanently disable Traefik,"
          echo "   manually add '--disable traefik' to k3s startup flags or configure /etc/rancher/k3s/config.yaml"

      - name: Ensure namespace and pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns ingress-nginx >/dev/null 2>&1 || kubectl create namespace ingress-nginx
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n ingress-nginx \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl patch serviceaccount default -n ingress-nginx \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true

      - name: Install helm dependencies
        shell: bash
        run: |
          set -euo pipefail
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo update

      - name: Deploy ingress controller
        shell: bash
        run: |
          set -euo pipefail
          ACR_IMAGE="${ACR_REGISTRY}/ingress-nginx/controller:v1.9.4"
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            -n ingress-nginx \
            -f infra/addons/values/ingress-nginx.yaml \
            --set controller.image.image="$ACR_IMAGE" \
            --timeout 5m
          kubectl patch serviceaccount ingress-nginx -n ingress-nginx \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          kubectl rollout status deployment/ingress-nginx-controller -n ingress-nginx --timeout=5m

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl get pods -n ingress-nginx

  monitoring:
    name: Deploy Monitoring Stack
    needs: ingress
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download mirror artifacts
        uses: actions/download-artifact@v4
        with:
          name: addon-mirror-metadata

      - name: Restore generated values
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p infra/addons/values
          # Artifacts preserve directory structure, so check both locations
          if [ -f "infra/addons/values/observability.acr.yaml" ]; then
            echo "âœ… observability.acr.yaml found in infra/addons/values/"
          elif [ -f "observability.acr.yaml" ]; then
            cp observability.acr.yaml infra/addons/values/observability.acr.yaml
            echo "âœ… observability.acr.yaml copied to infra/addons/values/"
          else
            echo "âš ï¸ observability.acr.yaml not found, will be generated if needed"
          fi
          
          # Validierung: PrÃ¼fe ob observability.acr.yaml alle Images auf ACR umschreibt
          if [ -f "infra/addons/values/observability.acr.yaml" ]; then
            echo "ğŸ” Validating observability.acr.yaml..."
            # PrÃ¼fe ob noch quay.io oder docker.io Referenzen vorhanden sind (sollten nicht sein)
            PUBLIC_REFS=$(grep -E "(quay\.io|docker\.io)" infra/addons/values/observability.acr.yaml | grep -v "^#" | grep -v "registry:" || echo "")
            if [ -n "$PUBLIC_REFS" ]; then
              echo "âš ï¸ Warning: Found public registry references in observability.acr.yaml:"
              echo "$PUBLIC_REFS"
              echo "âš ï¸ These should be rewritten to use ACR registry"
            else
              echo "âœ… All image references appear to use ACR registry"
            fi
            
            # Zeige ein paar Image-Referenzen zur Verifikation
            echo "ğŸ“‹ Sample image references from observability.acr.yaml:"
            grep -A 2 "repository:" infra/addons/values/observability.acr.yaml | head -10 || true
          fi

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Prepare namespace and pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns monitoring >/dev/null 2>&1 || kubectl create namespace monitoring
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n monitoring \
            --dry-run=client -o yaml | kubectl apply -f -
          
          # Patch default ServiceAccount
          kubectl patch serviceaccount default -n monitoring \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          
          # Patch kube-prometheus-stack-admission ServiceAccount (wird vom Chart erstellt)
          kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          
          # Patch alle ServiceAccounts im monitoring Namespace (fÃ¼r zukÃ¼nftige)
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
            echo "Patching ServiceAccount: $sa"
            kubectl patch serviceaccount "$sa" -n monitoring \
              -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          done

      - name: Install helm dependencies
        shell: bash
        run: |
          set -euo pipefail
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add grafana https://grafana.github.io/helm-charts
          helm repo update

      - name: Deploy monitoring stack
        shell: bash
        run: |
          set -euo pipefail
          
          # Debug: Zeige die verwendete Values-Datei
          echo "ğŸ“‹ Using values file:"
          cat infra/addons/values/observability.acr.yaml | head -50
          
          # Debug: PrÃ¼fe aktuelle Pods vor Deployment
          echo "ğŸ“Š Current pods in monitoring namespace:"
          kubectl get pods -n monitoring || true
          
          # Bereinige alte fehlgeschlagene Admission-Jobs
          echo "ğŸ§¹ Cleaning up old failed admission jobs..."
          # LÃ¶sche Jobs (auch Terminating) - force delete fÃ¼r hÃ¤ngende Jobs
          kubectl delete jobs -n monitoring -l app.kubernetes.io/name=kube-prometheus-stack,app.kubernetes.io/component=admission-webhook --ignore-not-found=true --grace-period=0 --force || true
          # LÃ¶sche Pods direkt (auch Terminating) - wichtig, da Pods manchmal hÃ¤ngen bleiben
          kubectl get pods -n monitoring -l app.kubernetes.io/name=kube-prometheus-stack,app.kubernetes.io/component=admission-webhook -o name 2>/dev/null | xargs -r kubectl delete -n monitoring --grace-period=0 --force --ignore-not-found=true || true
          
          # Warte lÃ¤nger, damit Cleanup abgeschlossen ist
          sleep 5
          
          # Deploy asynchron (ohne --wait, damit Workflow nicht hÃ¤ngt)
          echo "ğŸš€ Deploying monitoring stack (asynchron)..."
          
          # Aggressives Cleanup fÃ¼r stuck Helm-Releases (IMMER ausfÃ¼hren)
          echo "ğŸ”§ Force cleaning Helm locks and stuck releases..."
          
          # Versuche normale Uninstall (mit --no-hooks um Hooks zu Ã¼berspringen)
          helm uninstall kube-prometheus-stack -n monitoring --no-hooks --ignore-not-found=true 2>/dev/null || true
          
          # Debug: Zeige alle Secrets im Namespace
          echo "ğŸ“‹ Current secrets in monitoring namespace:"
          kubectl get secrets -n monitoring -o name 2>/dev/null || true
          
          # LÃ¶sche ALLE Helm-Secrets die mit dem Release zu tun haben
          echo "ğŸ—‘ï¸ Deleting Helm release secrets..."
          # Helm 3 Format: sh.helm.release.v1.<release-name>.<revision>
          kubectl get secrets -n monitoring -o name 2>/dev/null | grep -E "sh\.helm\.release\.v1\.kube-prometheus-stack\." | xargs -r kubectl delete -n monitoring --ignore-not-found=true || true
          # Alternative Formate
          kubectl get secrets -n monitoring -o name 2>/dev/null | grep -i "helm.*kube-prometheus-stack" | xargs -r kubectl delete -n monitoring --ignore-not-found=true || true
          # Mit Labels
          kubectl delete secret -n monitoring -l owner=helm,name=kube-prometheus-stack --ignore-not-found=true || true
          kubectl delete secret -n monitoring -l name=kube-prometheus-stack --ignore-not-found=true || true
          
          # LÃ¶sche ConfigMaps
          echo "ğŸ—‘ï¸ Deleting Helm release configmaps..."
          kubectl delete configmap -n monitoring -l owner=helm,name=kube-prometheus-stack --ignore-not-found=true || true
          kubectl get configmaps -n monitoring -o name 2>/dev/null | grep "kube-prometheus-stack" | xargs -r kubectl delete -n monitoring --ignore-not-found=true || true
          
          # Warte damit Cleanup abgeschlossen ist
          sleep 10
          
          # Debug: Zeige ob noch Secrets vorhanden sind
          echo "ğŸ“‹ Remaining secrets after cleanup:"
          kubectl get secrets -n monitoring -o name 2>/dev/null | grep -E "(helm|kube-prometheus)" || echo "No Helm secrets found"
          
          # KRITISCH: Erstelle und patche admission ServiceAccount VOR Helm-Deployment
          # Helm Hooks (post-install) erstellen Jobs, die sofort laufen und ImagePullSecrets benÃ¶tigen
          echo "ğŸ” Creating and patching admission ServiceAccount BEFORE Helm deployment..."
          if ! kubectl get serviceaccount kube-prometheus-stack-admission -n monitoring 2>/dev/null; then
            echo "  Creating kube-prometheus-stack-admission ServiceAccount..."
            kubectl create serviceaccount kube-prometheus-stack-admission -n monitoring || true
            kubectl label serviceaccount kube-prometheus-stack-admission -n monitoring \
              app.kubernetes.io/name=kube-prometheus-stack \
              app.kubernetes.io/component=prometheus-operator-webhook \
              app.kubernetes.io/managed-by=Helm --overwrite || true
            kubectl annotate serviceaccount kube-prometheus-stack-admission -n monitoring \
              helm.sh/hook=pre-install,pre-upgrade,post-install,post-upgrade --overwrite || true
          fi
          # Patch den ServiceAccount mit imagePullSecrets
          echo "  Patching kube-prometheus-stack-admission ServiceAccount with acr-pull..."
          kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null || true
          
          # Verifiziere dass der ServiceAccount gepatcht ist
          ADMISSION_IPS=$(kubectl get serviceaccount kube-prometheus-stack-admission -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
          if [[ "$ADMISSION_IPS" == *"acr-pull"* ]]; then
            echo "  âœ… Admission ServiceAccount is patched: $ADMISSION_IPS"
          else
            echo "  âš ï¸ Warning: Admission ServiceAccount may not be patched correctly"
            # Versuche nochmal mit JSON Patch (falls bereits ImagePullSecrets vorhanden sind)
            kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
              --type='json' \
              -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null || \
            kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
              -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null || true
          fi
          
          # HINWEIS: Wir erstellen NUR den admission ServiceAccount vorher (fÃ¼r Helm Hooks)
          # Alle anderen ServiceAccounts werden von Helm erstellt und dann sofort gepatcht
          
          # KRITISCH: LÃ¶sche ServiceAccounts, die nicht von Helm verwaltet werden
          # Das verhindert Import-Fehler beim ersten Deployment
          echo "ğŸ§¹ Cleaning up ServiceAccounts not managed by Helm (to prevent import errors)..."
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
            # Ãœberspringe default und admission ServiceAccount
            if [[ "$sa" == "default" ]] || [[ "$sa" == "kube-prometheus-stack-admission" ]]; then
              continue
            fi
            
            # PrÃ¼fe ob ServiceAccount von Helm verwaltet wird
            HELM_RELEASE=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.metadata.annotations.meta\.helm\.sh/release-name}' 2>/dev/null || echo "")
            if [ -z "$HELM_RELEASE" ]; then
              echo "  ğŸ—‘ï¸ Deleting ServiceAccount $sa (not managed by Helm, will be recreated by Helm)..."
              kubectl delete serviceaccount "$sa" -n monitoring --ignore-not-found=true || true
            else
              echo "  âœ… ServiceAccount $sa is managed by Helm (release: $HELM_RELEASE)"
            fi
          done
          echo "âœ… Cleanup completed"
          
          # KRITISCH: Erstelle Helm Post-Renderer Script, der ALLE Images nach dem Rendern auf ACR umschreibt
          # Dies stellt sicher, dass auch verschachtelte Strukturen (wie sidecar.dashboards.image) korrekt Ã¼berschrieben werden
          echo "ğŸ”§ Creating Helm Post-Renderer to rewrite all images to ACR..."
          {
            echo '#!/bin/bash'
            echo '# Helm Post-Renderer: Ersetzt alle Ã¶ffentlichen Registry-Images durch ACR'
            echo 'set -euo pipefail'
            echo '# ACR_REGISTRY wird als Environment-Variable Ã¼bergeben'
            echo 'ACR_REGISTRY="${ACR_REGISTRY:-}"'
            echo 'if [ -z "$ACR_REGISTRY" ]; then'
            echo '  echo "ERROR: ACR_REGISTRY environment variable is not set" >&2'
            echo '  exit 1'
            echo 'fi'
            echo '# Lese YAML von stdin'
            echo 'YAML_INPUT=$(cat)'
            echo '# Ersetze quay.io Images (einfacher Ansatz: ersetze alle Vorkommen)'
            echo 'YAML_OUTPUT=$(echo "$YAML_INPUT" | sed "s|quay\\.io/\\([^:[:space:]]*\\):\\([^[:space:]]*\\)|${ACR_REGISTRY}/\\1:\\2|g")'
            echo '# Ersetze docker.io Images (nur wenn nicht bereits ACR)'
            echo 'YAML_OUTPUT=$(echo "$YAML_OUTPUT" | sed "s|docker\\.io/\\([^:[:space:]]*\\):\\([^[:space:]]*\\)|${ACR_REGISTRY}/\\1:\\2|g")'
            echo '# Ersetze registry.k8s.io Images'
            echo 'YAML_OUTPUT=$(echo "$YAML_OUTPUT" | sed "s|registry\\.k8s\\.io/\\([^:[:space:]]*\\):\\([^[:space:]]*\\)|${ACR_REGISTRY}/\\1:\\2|g")'
            echo '# Output'
            echo 'echo "$YAML_OUTPUT"'
          } > /tmp/helm-post-render.sh
          chmod +x /tmp/helm-post-render.sh
          echo "âœ… Post-Renderer created (will rewrite all public registry images to ${ACR_REGISTRY})"
          
          # Teste den Post-Renderer mit einem Beispiel
          echo "ğŸ§ª Testing Post-Renderer..."
          TEST_YAML="image: quay.io/kiwigrid/k8s-sidecar:2.1.2"
          TEST_OUTPUT=$(echo "$TEST_YAML" | ACR_REGISTRY="${ACR_REGISTRY}" /tmp/helm-post-render.sh)
          echo "  Input:  $TEST_YAML"
          echo "  Output: $TEST_OUTPUT"
          if echo "$TEST_OUTPUT" | grep -q "${ACR_REGISTRY}"; then
            echo "  âœ… Post-Renderer test passed"
          else
            echo "  âš ï¸ Post-Renderer test failed - output does not contain ACR_REGISTRY"
          fi
          
          # Stelle sicher, dass ACR_REGISTRY als Environment-Variable verfÃ¼gbar ist
          export ACR_REGISTRY="${ACR_REGISTRY}"
          
          # Erstes Deployment: Wird fehlschlagen wegen Firewall (nur ACR erlaubt)
          # Wir deployen OHNE --wait, damit es schnell durchlÃ¤uft und Pods erstellt werden
          echo "ğŸš€ First deployment (will fail due to firewall, but creates resources)..."
          set +e  # TemporÃ¤r Fehler ignorieren
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            -n monitoring \
            -f infra/addons/values/observability.acr.yaml \
            --set grafana.sidecar.dashboards.image.registry="${ACR_REGISTRY}" \
            --set grafana.sidecar.dashboards.image.repository="kiwigrid/k8s-sidecar" \
            --set grafana.sidecar.dashboards.image.tag="2.1.2" \
            --set grafana.sidecar.datasources.image.registry="${ACR_REGISTRY}" \
            --set grafana.sidecar.datasources.image.repository="kiwigrid/k8s-sidecar" \
            --set grafana.sidecar.datasources.image.tag="2.1.2" \
            --set alertmanager.image.registry="${ACR_REGISTRY}" \
            --set alertmanager.image.repository="prometheus/alertmanager" \
            --set alertmanager.image.tag="v0.29.0" \
            --post-renderer /tmp/helm-post-render.sh \
            --timeout 2m \
            --wait=false 2>&1 | head -20 || true  # Zeige nur ersten Teil, dann abbrechen
          set -e  # Fehler wieder aktivieren
          
          echo "â¸ï¸ First deployment completed (pods will fail due to firewall, but resources are created)"
          echo "ğŸ”§ Now patching ServiceAccounts and cleaning up failed pods..."
          
          # Warte kurz, damit Pods erstellt wurden
          sleep 5
          
          # LÃ¶sche fehlgeschlagene Post-Install-Hook-Jobs, damit sie mit gepatchten ServiceAccounts neu erstellt werden
          echo "ğŸ§¹ Cleaning up failed post-install hook jobs..."
          kubectl delete job -n monitoring -l app=kube-prometheus-stack-admission-patch --ignore-not-found=true || true
          kubectl delete job -n monitoring -l app=kube-prometheus-stack-admission-create --ignore-not-found=true || true
          sleep 2
          echo "âœ… Cleaned up failed hook jobs"
          
          echo "âœ… Helm deployment command completed. Resources are being created in the background."
          
          # KRITISCH: Patch ALLE ServiceAccounts SOFORT nach Helm-Deployment
          # Helm kann ServiceAccounts Ã¼berschreiben, daher mÃ¼ssen wir sie erneut patchen
          echo "ğŸ” CRITICAL: Patching all ServiceAccounts immediately after Helm deployment..."
          sleep 2  # Kurz warten, damit Helm ServiceAccounts erstellt hat
          PATCHED_COUNT=0
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
            HAS_SECRET=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[?(@.name=="acr-pull")].name}' 2>/dev/null || echo "")
            if [ -z "$HAS_SECRET" ]; then
              echo "  ğŸ” Patching ServiceAccount: $sa"
              CURRENT_IPS=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
              if [ -n "$CURRENT_IPS" ]; then
                kubectl patch serviceaccount "$sa" -n monitoring \
                  --type='json' \
                  -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null && PATCHED_COUNT=$((PATCHED_COUNT + 1)) || true
              else
                kubectl patch serviceaccount "$sa" -n monitoring \
                  -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null && PATCHED_COUNT=$((PATCHED_COUNT + 1)) || true
              fi
            fi
          done
          echo "âœ… Immediately patched $PATCHED_COUNT ServiceAccounts after Helm deployment"
          
          # DEBUG: PrÃ¼fe welche Images die Pods verwenden
          echo "ğŸ” DEBUG: Checking which images pods are trying to pull..."
          sleep 5  # Warte kurz, damit Pods erstellt werden
          for pod in $(kubectl get pods -n monitoring -o name 2>/dev/null | head -5); do
            echo "ğŸ“¦ Pod: $pod"
            IMAGE=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.containers[0].image}' 2>/dev/null || echo "N/A")
            echo "  Image: $IMAGE"
            SA=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.serviceAccountName}' 2>/dev/null || echo "default")
            echo "  ServiceAccount: $SA"
            IPS=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.imagePullSecrets[*].name}' 2>/dev/null || echo "(none)")
            echo "  ImagePullSecrets: $IPS"
            echo "---"
          done
          
          # DEBUG: PrÃ¼fe ob ServiceAccounts ImagePullSecrets haben
          echo "ğŸ” DEBUG: Checking ServiceAccount ImagePullSecrets..."
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null); do
            echo "ğŸ“‹ $sa:"
            IPS=$(kubectl get $sa -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "(none)")
            echo "  ImagePullSecrets: $IPS"
          done
          
          # DEBUG: Zeige erwartete ACR Image-Pfade
          echo "ğŸ” DEBUG: Expected ACR image paths (should match pod images above):"
          if [ -n "${ACR_REGISTRY:-}" ]; then
            echo "ACR Registry: $ACR_REGISTRY"
            echo "Expected images:"
            echo "  - ${ACR_REGISTRY}/grafana/grafana:10.2.0"
            echo "  - ${ACR_REGISTRY}/prometheus/prometheus:v2.48.0"
            echo "  - ${ACR_REGISTRY}/prometheus-operator/prometheus-operator:v0.86.2"
            echo "  - ${ACR_REGISTRY}/jettech/kube-webhook-certgen:v1.5.1"
            echo "  - ${ACR_REGISTRY}/prometheus/node-exporter:v1.10.2"
            echo "  - ${ACR_REGISTRY}/kube-state-metrics/kube-state-metrics:v2.10.1"
          fi
          
          # Einfaches Cleanup: LÃ¶sche fehlgeschlagene Pods einmalig, damit sie mit gepatchten ServiceAccounts neu erstellt werden
          echo "ğŸ§¹ Cleaning up failed pods (if any)..."
          sleep 5  # Warte kurz, damit Pods erstellt werden
          
          # LÃ¶sche Pods mit ImagePullBackOff oder ErrImagePull
          IMAGEPULL_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff" || @.status.containerStatuses[0].state.waiting.reason=="ErrImagePull")].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$IMAGEPULL_PODS" ]; then
            echo "ğŸ—‘ï¸ Deleting pods with ImagePullBackOff/ErrImagePull..."
            for pod in $IMAGEPULL_PODS; do
              echo "  ğŸ—‘ï¸ Deleting pod: $pod"
              kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          else
            echo "âœ… No failed pods found"
          fi
          
          # LÃ¶sche fehlgeschlagene Admission-Jobs
          kubectl delete jobs -n monitoring -l app.kubernetes.io/component=admission-webhook --ignore-not-found=true || true
          kubectl delete jobs -n monitoring -l app.kubernetes.io/name=kube-prometheus-stack,app.kubernetes.io/component=admission-webhook --ignore-not-found=true || true
          
          # Zweites Deployment: Jetzt sollten Pods mit gepatchten ServiceAccounts funktionieren
          echo "ğŸš€ Second deployment (with patched ServiceAccounts, should succeed)..."
          sleep 3  # Kurz warten, damit Pods gelÃ¶scht wurden
          
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            -n monitoring \
            -f infra/addons/values/observability.acr.yaml \
            --set grafana.sidecar.dashboards.image.registry="${ACR_REGISTRY}" \
            --set grafana.sidecar.dashboards.image.repository="kiwigrid/k8s-sidecar" \
            --set grafana.sidecar.dashboards.image.tag="2.1.2" \
            --set grafana.sidecar.datasources.image.registry="${ACR_REGISTRY}" \
            --set grafana.sidecar.datasources.image.repository="kiwigrid/k8s-sidecar" \
            --set grafana.sidecar.datasources.image.tag="2.1.2" \
            --set alertmanager.image.registry="${ACR_REGISTRY}" \
            --set alertmanager.image.repository="prometheus/alertmanager" \
            --set alertmanager.image.tag="v0.29.0" \
            --post-renderer /tmp/helm-post-render.sh \
            --timeout 10m \
            --wait --wait-for-jobs=false || {
              echo "âš ï¸ Helm deployment completed with warnings (some jobs may have failed)"
            }
          
          echo "âœ… Second deployment completed (Helm waited for resources with --wait)"
          
          # Zeige Status aller Pods
          echo "ğŸ“Š Current pod status:"
          kubectl get pods -n monitoring -o wide || true
          
          # Zeige Zugriffs-URLs fÃ¼r Dashboards
          echo ""
          echo "ğŸŒ Dashboard Access URLs (via NodePort):"
          echo "=========================================="
          # Hole Host-IP (erste Node-IP)
          HOST_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}' 2>/dev/null || echo "N/A")
          if [ "$HOST_IP" != "N/A" ]; then
            echo "  ğŸ“Š Grafana:      http://${HOST_IP}:30000"
            echo "     Login:        admin / admin"
            echo ""
            echo "  ğŸ“ˆ Prometheus:   http://${HOST_IP}:30001"
            echo ""
            echo "  ğŸ”” Alertmanager: http://${HOST_IP}:30002"
            echo ""
            echo "  ğŸ’¡ Tip: Replace ${HOST_IP} with your actual node IP if different"
          else
            echo "  âš ï¸ Could not determine host IP"
            echo "  ğŸ“Š Grafana:      http://<NODE_IP>:30000"
            echo "  ğŸ“ˆ Prometheus:   http://<NODE_IP>:30001"
            echo "  ğŸ”” Alertmanager: http://<NODE_IP>:30002"
          fi
          echo "=========================================="
          echo ""
          echo "ğŸ“¡ Networking Metrics:"
          echo "  NGINX Ingress Controller metrics are available via Prometheus"
          echo "  Import NGINX dashboards in Grafana (e.g., dashboard ID 9614)"
          echo "  Prometheus scrapes NGINX metrics automatically (metrics.enabled=true)"

      - name: Patch ServiceAccounts after deployment and restart failed jobs
        shell: bash
        run: |
          set -euo pipefail
          echo "ğŸ” Final patching of all ServiceAccounts and cleanup of failed pods..."
          
          # Patch ALLE ServiceAccounts, die noch nicht gepatcht sind (INKLUSIVE default)
          PATCHED_COUNT=0
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
            # PrÃ¼fe ob ServiceAccount bereits acr-pull als ImagePullSecret hat
            HAS_SECRET=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[?(@.name=="acr-pull")].name}' 2>/dev/null || echo "")
            if [ -z "$HAS_SECRET" ]; then
              echo "ğŸ” Patching ServiceAccount: $sa"
              # Hole aktuelle ImagePullSecrets (falls vorhanden)
              CURRENT_IPS=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
              if [ -n "$CURRENT_IPS" ]; then
                # FÃ¼ge acr-pull zu bestehenden Secrets hinzu
                if kubectl patch serviceaccount "$sa" -n monitoring \
                  --type='json' \
                  -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null; then
                  PATCHED_COUNT=$((PATCHED_COUNT + 1))
                  echo "  âœ… Patched $sa (added to existing)"
                fi
              else
                # Erstelle neue ImagePullSecrets Liste
                if kubectl patch serviceaccount "$sa" -n monitoring \
                  -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null; then
                  PATCHED_COUNT=$((PATCHED_COUNT + 1))
                  echo "  âœ… Patched $sa (created new)"
                fi
              fi
            else
              echo "  âœ“ ServiceAccount $sa already patched"
            fi
          done
          echo "âœ… Patched $PATCHED_COUNT ServiceAccounts"
          
          # Warte kurz, damit Patch angewendet ist
          sleep 3
          
          # PrÃ¼fe ob fehlgeschlagene Jobs oder Pods existieren
          echo "ğŸ” Checking for failed jobs and pods..."
          FAILED_JOBS=$(kubectl get jobs -n monitoring -o jsonpath='{.items[?(@.status.failed>0)].metadata.name}' 2>/dev/null || echo "")
          IMAGEPULL_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff" || @.status.containerStatuses[0].state.waiting.reason=="ErrImagePull")].metadata.name}' 2>/dev/null || echo "")
          
          # Restart Jobs/Pods, die ImagePullBackOff oder ErrImagePull haben oder fehlgeschlagen sind
          if [ -n "$FAILED_JOBS" ] || [ -n "$IMAGEPULL_PODS" ]; then
            echo "ğŸ”„ Restarting failed jobs and pods..."
            
            # LÃ¶sche alle Admission-Jobs (werden automatisch neu erstellt)
            for job in $(kubectl get jobs -n monitoring -o name 2>/dev/null | grep -E "(admission|webhook)"); do
              echo "  ğŸ—‘ï¸ Deleting job: $job"
              kubectl delete "$job" -n monitoring --ignore-not-found=true --grace-period=0 --force || true
            done
            
            # LÃ¶sche auch alle Pods die ImagePullBackOff oder ErrImagePull haben (nicht nur Jobs)
            for pod in $IMAGEPULL_PODS; do
              echo "  ğŸ—‘ï¸ Deleting pod with ImagePullBackOff/ErrImagePull: $pod"
              kubectl delete pod "$pod" -n monitoring --ignore-not-found=true --grace-period=0 --force || true
            done
            
            # KRITISCH: LÃ¶sche ALLE Pods, die keine ImagePullSecrets haben (damit sie mit gepatchten ServiceAccounts neu erstellt werden)
            echo "ğŸ”„ Checking for pods without ImagePullSecrets..."
            for pod in $(kubectl get pods -n monitoring -o name 2>/dev/null); do
              POD_IPS=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.imagePullSecrets[*].name}' 2>/dev/null || echo "")
              if [ -z "$POD_IPS" ]; then
                echo "  ğŸ—‘ï¸ Deleting pod without ImagePullSecrets: $pod"
                kubectl delete $pod -n monitoring --ignore-not-found=true --grace-period=0 --force || true
              fi
            done
            
            # Warte kurz, damit neue Jobs/Pods gestartet werden
            sleep 5
            
            # Zeige Status der Jobs
            echo "ğŸ“Š Job status after restart:"
            kubectl get jobs -n monitoring | grep -E "(admission|webhook)" || echo "No admission jobs found"
          else
            echo "âœ… No failed jobs or ImagePullBackOff pods found - all resources are healthy"
          fi
          
          # KRITISCH: PrÃ¼fe ob Pods noch von Ã¶ffentlichen Registries ziehen und patche ServiceAccounts erneut
          echo "ğŸ” Checking if pods are pulling from ACR or public registries..."
          sleep 5  # Warte kurz, damit Pods erstellt sind
          
          # Finde Pods die noch von quay.io, docker.io oder anderen Ã¶ffentlichen Registries ziehen
          PUBLIC_REGISTRY_PODS=""
          for pod in $(kubectl get pods -n monitoring -o name 2>/dev/null); do
            # PrÃ¼fe alle Container im Pod (auch Init-Container)
            CONTAINER_IMAGES=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.containers[*].image}' 2>/dev/null || echo "")
            INIT_IMAGES=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.initContainers[*].image}' 2>/dev/null || echo "")
            ALL_IMAGES="$CONTAINER_IMAGES $INIT_IMAGES"
            
            for image in $ALL_IMAGES; do
              if [[ "$image" == quay.io/* ]] || [[ "$image" == docker.io/* ]] || [[ "$image" == registry.k8s.io/* ]]; then
                if [[ "$PUBLIC_REGISTRY_PODS" != *"$pod"* ]]; then
                  if [ -z "$PUBLIC_REGISTRY_PODS" ]; then
                    PUBLIC_REGISTRY_PODS="$pod"
                  else
                    PUBLIC_REGISTRY_PODS="$PUBLIC_REGISTRY_PODS $pod"
                  fi
                  echo "  âš ï¸ Pod $pod is pulling from public registry: $image"
                fi
              fi
            done
          done
          
          # Wenn Pods noch von Ã¶ffentlichen Registries ziehen, patche alle ServiceAccounts erneut
          if [ -n "$PUBLIC_REGISTRY_PODS" ]; then
            echo "âš ï¸ Found pods pulling from public registries. Patching all ServiceAccounts again..."
            PATCHED_AGAIN=0
            for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
              HAS_SECRET=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[?(@.name=="acr-pull")].name}' 2>/dev/null || echo "")
              if [ -z "$HAS_SECRET" ]; then
                echo "  ğŸ” Patching ServiceAccount: $sa"
                CURRENT_IPS=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
                if [ -n "$CURRENT_IPS" ]; then
                  kubectl patch serviceaccount "$sa" -n monitoring \
                    --type='json' \
                    -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null && PATCHED_AGAIN=$((PATCHED_AGAIN + 1)) || true
                else
                  kubectl patch serviceaccount "$sa" -n monitoring \
                    -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null && PATCHED_AGAIN=$((PATCHED_AGAIN + 1)) || true
                fi
              fi
            done
            echo "âœ… Repatched $PATCHED_AGAIN ServiceAccounts"
            
            # LÃ¶sche Pods die von Ã¶ffentlichen Registries ziehen, damit sie neu erstellt werden
            echo "ğŸ—‘ï¸ Deleting pods pulling from public registries..."
            for pod in $PUBLIC_REGISTRY_PODS; do
              echo "  ğŸ—‘ï¸ Deleting pod: $pod"
              kubectl delete $pod -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
            
            echo "â³ Waiting for pods to be recreated..."
            sleep 10
          else
            echo "âœ… All pods are pulling from ACR"
          fi
          
          # Finaler Status-Check
          echo "ğŸ“Š Final pod status:"
          kubectl get pods -n monitoring -o wide || true
          
          # Zeige welche Images die Pods verwenden
          echo "ğŸ“¦ Pod images (should all be from ACR):"
          for pod in $(kubectl get pods -n monitoring -o name 2>/dev/null | head -10); do
            IMAGE=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.containers[0].image}' 2>/dev/null || echo "N/A")
            echo "  $pod: $IMAGE"
          done
          
          # Zeige ServiceAccounts Status
          echo "ğŸ“Š ServiceAccounts with imagePullSecrets:"
          kubectl get serviceaccounts -n monitoring -o jsonpath='{range .items[*]}{.metadata.name}{": "}{.imagePullSecrets[*].name}{"\n"}{end}' || true

      - name: Collect debug logs from failed resources
        shell: bash
        run: |
          set -euo pipefail
          echo "ğŸ” Collecting debug logs from failed resources..."
          
          # 1. Sammle Logs von fehlgeschlagenen Pods
          echo "ğŸ“‹ Collecting logs from failed/error pods..."
          ERROR_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.phase=="Failed" || @.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff" || @.status.containerStatuses[0].state.waiting.reason=="ErrImagePull" || @.status.containerStatuses[0].state.terminated.reason=="Error" || @.status.containerStatuses[0].state.waiting.reason=="CrashLoopBackOff")].metadata.name}' 2>/dev/null || echo "")
          
          if [ -n "$ERROR_PODS" ]; then
            for pod in $ERROR_PODS; do
              echo ""
              echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
              echo "ğŸ› DEBUG: Pod $pod"
              echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
              
              # Zeige Pod Details
              echo "ğŸ“‹ Pod Details:"
              kubectl get pod "$pod" -n monitoring -o wide || true
              
              # Zeige Pod Events
              echo ""
              echo "ğŸ“‹ Pod Events:"
              kubectl describe pod "$pod" -n monitoring | grep -A 20 "Events:" || true
              
              # Zeige Container Images
              echo ""
              echo "ğŸ“¦ Container Images:"
              kubectl get pod "$pod" -n monitoring -o jsonpath='{range .spec.containers[*]}{.name}{": "}{.image}{"\n"}{end}' || true
              kubectl get pod "$pod" -n monitoring -o jsonpath='{range .spec.initContainers[*]}{.name}{" (init): "}{.image}{"\n"}{end}' || true
              
              # Zeige ServiceAccount und ImagePullSecrets
              echo ""
              echo "ğŸ” ServiceAccount Info:"
              SA=$(kubectl get pod "$pod" -n monitoring -o jsonpath='{.spec.serviceAccountName}' 2>/dev/null || echo "N/A")
              echo "  ServiceAccount: $SA"
              IPS=$(kubectl get pod "$pod" -n monitoring -o jsonpath='{.spec.imagePullSecrets[*].name}' 2>/dev/null || echo "none")
              echo "  ImagePullSecrets: $IPS"
              if [ "$SA" != "N/A" ] && [ "$SA" != "default" ]; then
                SA_IPS=$(kubectl get serviceaccount "$SA" -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "none")
                echo "  ServiceAccount ImagePullSecrets: $SA_IPS"
              fi
              
              # Versuche Logs zu sammeln (falls Container gestartet wurden)
              echo ""
              echo "ğŸ“‹ Container Logs (if available):"
              for container in $(kubectl get pod "$pod" -n monitoring -o jsonpath='{.spec.containers[*].name}' 2>/dev/null); do
                echo "  Container: $container"
                kubectl logs "$pod" -n monitoring -c "$container" --tail=50 2>&1 || echo "    (logs not available)"
              done
              
              # Zeige Image Pull Error Details
              echo ""
              echo "ğŸ” Image Pull Status:"
              kubectl get pod "$pod" -n monitoring -o jsonpath='{range .status.containerStatuses[*]}{.name}{": "}{.state.waiting.reason}{" - "}{.state.waiting.message}{"\n"}{end}' || true
              kubectl get pod "$pod" -n monitoring -o jsonpath='{range .status.containerStatuses[*]}{.name}{": "}{.state.terminated.reason}{" - "}{.state.terminated.message}{"\n"}{end}' || true
            done
          else
            echo "âœ… No failed pods found"
          fi
          
          # 2. Sammle Logs von fehlgeschlagenen Jobs
          echo ""
          echo "ğŸ“‹ Collecting logs from failed jobs..."
          FAILED_JOBS=$(kubectl get jobs -n monitoring -o jsonpath='{.items[?(@.status.failed>0)].metadata.name}' 2>/dev/null || echo "")
          
          if [ -n "$FAILED_JOBS" ]; then
            for job in $FAILED_JOBS; do
              echo ""
              echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
              echo "ğŸ› DEBUG: Job $job"
              echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
              
              # Zeige Job Details
              echo "ğŸ“‹ Job Details:"
              kubectl get job "$job" -n monitoring -o wide || true
              kubectl describe job "$job" -n monitoring || true
              
              # Zeige Job Pod Logs
              echo ""
              echo "ğŸ“‹ Job Pod Logs:"
              JOB_PODS=$(kubectl get pods -n monitoring -l job-name="$job" -o name 2>/dev/null || echo "")
              if [ -n "$JOB_PODS" ]; then
                for pod in $JOB_PODS; do
                  echo "  Pod: $pod"
                  kubectl logs $pod -n monitoring --tail=100 2>&1 || echo "    (logs not available)"
                done
              else
                echo "  (no pods found for this job)"
              fi
            done
          else
            echo "âœ… No failed jobs found"
          fi
          
          # 3. Zeige Terminating Pods (fÃ¼r Info)
          echo ""
          echo "ğŸ“‹ Terminating pods (will be cleaned up):"
          TERMINATING_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.metadata.deletionTimestamp!=null)].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$TERMINATING_PODS" ]; then
            for pod in $TERMINATING_PODS; do
              echo "  â³ $pod (Terminating since: $(kubectl get pod "$pod" -n monitoring -o jsonpath='{.metadata.deletionTimestamp}' 2>/dev/null || echo "unknown"))"
            done
          else
            echo "  âœ… No terminating pods"
          fi
          
          echo ""
          echo "âœ… Debug log collection completed"

      - name: Cleanup failed resources
        shell: bash
        run: |
          set -euo pipefail
          echo "ğŸ§¹ Cleaning up failed and terminated resources..."
          
          # 1. LÃ¶sche alle Terminating Pods
          echo "ğŸ—‘ï¸ Cleaning up Terminating pods..."
          TERMINATING_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.metadata.deletionTimestamp!=null)].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$TERMINATING_PODS" ]; then
            for pod in $TERMINATING_PODS; do
              echo "  ğŸ—‘ï¸ Force deleting terminating pod: $pod"
              kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          fi
          
          # 2. LÃ¶sche alle Pods mit Error/ImagePullBackOff/ErrImagePull Status
          echo "ğŸ—‘ï¸ Cleaning up pods with Error/ImagePullBackOff/ErrImagePull status..."
          ERROR_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.phase=="Failed" || @.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff" || @.status.containerStatuses[0].state.waiting.reason=="ErrImagePull" || @.status.containerStatuses[0].state.terminated.reason=="Error")].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$ERROR_PODS" ]; then
            for pod in $ERROR_PODS; do
              echo "  ğŸ—‘ï¸ Deleting error pod: $pod"
              kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          fi
          
          # 3. LÃ¶sche alle fehlgeschlagenen Jobs
          echo "ğŸ—‘ï¸ Cleaning up failed jobs..."
          FAILED_JOBS=$(kubectl get jobs -n monitoring -o jsonpath='{.items[?(@.status.failed>0)].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$FAILED_JOBS" ]; then
            for job in $FAILED_JOBS; do
              echo "  ğŸ—‘ï¸ Deleting failed job: $job"
              kubectl delete job "$job" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          fi
          
          # 4. LÃ¶sche alle Completed Jobs (die nicht mehr benÃ¶tigt werden, besonders admission/webhook)
          echo "ğŸ—‘ï¸ Cleaning up completed admission/webhook jobs..."
          for job in $(kubectl get jobs -n monitoring -o name 2>/dev/null | grep -E "(admission|webhook|patch)"); do
            JOB_STATUS=$(kubectl get $job -n monitoring -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null || echo "")
            if [ "$JOB_STATUS" = "True" ]; then
              echo "  ğŸ—‘ï¸ Deleting completed job: $job"
              kubectl delete $job -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            fi
          done
          
          # 5. LÃ¶sche alle Pods im CrashLoopBackOff Status
          echo "ğŸ—‘ï¸ Cleaning up CrashLoopBackOff pods..."
          CRASHLOOP_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.containerStatuses[0].state.waiting.reason=="CrashLoopBackOff")].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$CRASHLOOP_PODS" ]; then
            for pod in $CRASHLOOP_PODS; do
              echo "  ğŸ—‘ï¸ Deleting CrashLoopBackOff pod: $pod"
              kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          fi
          
          # 6. LÃ¶sche alle Pending Pods die lÃ¤nger als 5 Minuten warten (wahrscheinlich hÃ¤ngend)
          echo "ğŸ—‘ï¸ Cleaning up stuck pending pods..."
          PENDING_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.phase=="Pending")].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$PENDING_PODS" ]; then
            for pod in $PENDING_PODS; do
              POD_AGE=$(kubectl get pod "$pod" -n monitoring -o jsonpath='{.metadata.creationTimestamp}' 2>/dev/null || echo "")
              if [ -n "$POD_AGE" ]; then
                # PrÃ¼fe ob Pod Ã¤lter als 5 Minuten ist (vereinfacht - prÃ¼ft nur ob er existiert)
                echo "  ğŸ—‘ï¸ Deleting pending pod (may be stuck): $pod"
                kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
              fi
            done
          fi
          
          # 7. Warte kurz, damit Cleanup abgeschlossen ist
          sleep 5
          
          # 8. Finale Status-Anzeige
          echo "ğŸ“Š Final cleanup status:"
          TERMINATING_COUNT=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.metadata.deletionTimestamp!=null)].metadata.name}' 2>/dev/null | wc -w || echo 0)
          ERROR_COUNT=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.phase=="Failed")].metadata.name}' 2>/dev/null | wc -w || echo 0)
          IMAGEPULL_COUNT=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff")].metadata.name}' 2>/dev/null | wc -w || echo 0)
          FAILED_JOBS_COUNT=$(kubectl get jobs -n monitoring -o jsonpath='{.items[?(@.status.failed>0)].metadata.name}' 2>/dev/null | wc -w || echo 0)
          
          echo "  Terminating pods: $TERMINATING_COUNT"
          echo "  Error pods: $ERROR_COUNT"
          echo "  ImagePullBackOff pods: $IMAGEPULL_COUNT"
          echo "  Failed jobs: $FAILED_JOBS_COUNT"
          
          if [ "$TERMINATING_COUNT" -eq 0 ] && [ "$ERROR_COUNT" -eq 0 ] && [ "$IMAGEPULL_COUNT" -eq 0 ] && [ "$FAILED_JOBS_COUNT" -eq 0 ]; then
            echo "âœ… All cleanup completed - workspace is clean!"
          else
            echo "âš ï¸ Some resources may still need cleanup"
          fi

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl get pods -n monitoring