name: Deploy Base Kubernetes-Infrastructure

on:
  push:
    branches: [ main ]
    paths:
      - 'infra/addons/**'
      - 'infra/rbac/**'
      - '.github/workflows/build-push.yml'
  workflow_dispatch:

jobs:
  mirror:
    runs-on: ubuntu-latest
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python venv
        shell: bash
        run: |
          set -euo pipefail
          python3 -m venv .venv
          source .venv/bin/activate
          pip install --upgrade pip PyYAML
          echo "VENV_PYTHON=$(pwd)/.venv/bin/python" >> "$GITHUB_ENV"

      - name: Login to ACR
        uses: docker/login-action@v3
        with:
          registry: ${{ secrets.ACR_REGISTRY }}
          username: ${{ secrets.ACR_USERNAME }}
          password: ${{ secrets.ACR_PASSWORD }}

      - name: Setup Helm
        shell: bash
        run: |
          set -euo pipefail
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          helm version
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update prometheus-community

      - name: Generate mirror artifacts
        id: map
        run: |
          set -euo pipefail
          "$VENV_PYTHON" scripts/generate_addon_artifacts.py
          echo "map-file=image-map.txt" >> "$GITHUB_OUTPUT"
          echo "values-file=infra/addons/values/observability.acr.yaml" >> "$GITHUB_OUTPUT"
          
          # KRITISCH: Extrahiere ALLE Images die Helm Chart tats√§chlich verwenden wird (inkl. Defaults)
          echo "üîç Extracting all images from Helm Chart (including defaults)..."
          
          # Template das Chart mit unseren Values, um alle tats√§chlich verwendeten Images zu sehen
          if [ -f "infra/addons/values/observability.acr.yaml" ]; then
            VALUES_FILE="infra/addons/values/observability.acr.yaml"
          elif [ -f "infra/addons/values/observability.yaml" ]; then
            VALUES_FILE="infra/addons/values/observability.yaml"
          else
            VALUES_FILE=""
          fi
          
          if [ -n "$VALUES_FILE" ]; then
            # Template das Chart (ohne es zu installieren)
            HELM_TEMPLATE_OUTPUT=$(helm template kube-prometheus-stack prometheus-community/kube-prometheus-stack \
              -n monitoring \
              -f "$VALUES_FILE" 2>/dev/null || echo "")
            
            if [ -n "$HELM_TEMPLATE_OUTPUT" ]; then
              # Extrahiere alle Image-Referenzen aus dem gerenderten YAML
              HELM_IMAGES=$(echo "$HELM_TEMPLATE_OUTPUT" | grep -E "^\s+image:" | sed 's/.*image:[[:space:]]*//' | sed 's/"//g' | sed "s/'//g" | sort -u || echo "")
              
              if [ -n "$HELM_IMAGES" ]; then
                echo "üì¶ Found images from Helm template:"
                echo "$HELM_IMAGES" | head -20
                
                # F√ºge fehlende Images zum image-map.txt hinzu
                MISSING_COUNT=0
                while IFS= read -r image; do
                  [[ -z "$image" ]] && continue
                  
                  # Parse image (kann registry/repo:tag oder repo:tag sein)
                  if [[ "$image" == *"/"* ]]; then
                    # Hat Registry: registry/repo:tag
                    REGISTRY=$(echo "$image" | cut -d'/' -f1)
                    REPO_AND_TAG=$(echo "$image" | cut -d'/' -f2-)
                  else
                    # Keine Registry: repo:tag
                    REGISTRY=""
                    REPO_AND_TAG="$image"
                  fi
                  
                  if [[ "$REPO_AND_TAG" == *":"* ]]; then
                    REPO=$(echo "$REPO_AND_TAG" | cut -d':' -f1)
                    TAG=$(echo "$REPO_AND_TAG" | cut -d':' -f2-)
                  else
                    REPO="$REPO_AND_TAG"
                    TAG="latest"
                  fi
                  
                  # Normalisiere Registry
                  if [[ "$REGISTRY" == "docker.io" ]] || [[ "$REGISTRY" == "quay.io" ]] || [[ "$REGISTRY" == "registry.k8s.io" ]]; then
                    SOURCE_REPO="${REGISTRY}/${REPO}"
                    TARGET_REPO="$REPO"
                  elif [ -z "$REGISTRY" ]; then
                    SOURCE_REPO="docker.io/${REPO}"
                    TARGET_REPO="$REPO"
                  else
                    # Bereits ACR oder andere Registry
                    continue
                  fi
                  
                  # Pr√ºfe ob Image bereits im image-map.txt ist (flexibler Match)
                  if ! grep -qE "^${SOURCE_REPO}\|.*\|${TAG}$|^.*\|${TARGET_REPO}\|${TAG}$" image-map.txt 2>/dev/null; then
                    echo "${SOURCE_REPO}|${TARGET_REPO}|${TAG}" >> image-map.txt
                    echo "  ‚ûï Added missing image: ${SOURCE_REPO}:${TAG} -> ${ACR_REGISTRY}/${TARGET_REPO}:${TAG}"
                    MISSING_COUNT=$((MISSING_COUNT + 1))
                  fi
                done <<< "$HELM_IMAGES"
                
                if [ $MISSING_COUNT -gt 0 ]; then
                  echo "‚úÖ Added $MISSING_COUNT missing images from Helm Chart defaults to image-map.txt"
                  echo "üìã Updated image-map.txt now contains:"
                  wc -l image-map.txt || true
                else
                  echo "‚úÖ All images from Helm Chart are already in image-map.txt"
                fi
              else
                echo "‚ö†Ô∏è No images found in Helm template output"
              fi
            else
              echo "‚ö†Ô∏è Helm template failed or returned empty output"
            fi
          else
            echo "‚ö†Ô∏è Values file not found, skipping Helm template extraction"
          fi

      - name: Mirror container images
        run: |
          set -euo pipefail
          while IFS='|' read -r SOURCE_REPO TARGET_REPO TAG; do
            [[ -z "${SOURCE_REPO:-}" ]] && continue
            SOURCE_IMAGE="${SOURCE_REPO}:${TAG}"
            TARGET_IMAGE="${ACR_REGISTRY}/${TARGET_REPO}:${TAG}"
            echo "‚Üí Mirroring $SOURCE_IMAGE to $TARGET_IMAGE"
            docker pull "$SOURCE_IMAGE"
            docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
            docker push "$TARGET_IMAGE"
          done < image-map.txt

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: addon-mirror-metadata
          path: |
            image-map.txt
            infra/addons/values/observability.acr.yaml

  local-path:
    name: Deploy Local Path Provisioner
    needs: mirror
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Prepare namespace and pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns local-path-storage >/dev/null 2>&1 || kubectl create namespace local-path-storage
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n local-path-storage \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Check if Local Path Provisioner already exists
        id: check_provisioner
        shell: bash
        run: |
          set -euo pipefail
          # Check for deployment in local-path-storage namespace
          if kubectl get deployment local-path-provisioner -n local-path-storage >/dev/null 2>&1; then
            echo "exists=true" >> "$GITHUB_OUTPUT"
            echo "namespace=local-path-storage" >> "$GITHUB_OUTPUT"
            echo "‚úÖ Local Path Provisioner already exists in local-path-storage namespace"
            exit 0
          fi
          # Check for deployment in kube-system namespace (k3s default)
          if kubectl get deployment local-path-provisioner -n kube-system >/dev/null 2>&1; then
            echo "exists=true" >> "$GITHUB_OUTPUT"
            echo "namespace=kube-system" >> "$GITHUB_OUTPUT"
            echo "‚úÖ Local Path Provisioner already exists in kube-system namespace (k3s default)"
            exit 0
          fi
          # Check for StorageClass
          if kubectl get storageclass local-path >/dev/null 2>&1; then
            echo "storageclass_exists=true" >> "$GITHUB_OUTPUT"
            echo "‚úÖ StorageClass 'local-path' already exists"
          else
            echo "storageclass_exists=false" >> "$GITHUB_OUTPUT"
          fi
          echo "exists=false" >> "$GITHUB_OUTPUT"

      - name: Deploy Local Path Provisioner
        if: steps.check_provisioner.outputs.exists != 'true'
        shell: bash
        run: |
          set -euo pipefail
          curl -sL https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml | \
            sed "s|rancher/local-path-provisioner:v0.0.26|${ACR_REGISTRY}/rancher/local-path-provisioner:v0.0.26|g" | \
            kubectl apply -f -
          sleep 3
          kubectl patch serviceaccount local-path-provisioner-service-account -n local-path-storage \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          kubectl -n local-path-storage rollout status deployment/local-path-provisioner --timeout=5m

      - name: Fix k3s Local Path Provisioner RBAC and StorageClass
        shell: bash
        run: |
          set -euo pipefail
          # Check if k3s provisioner exists in kube-system namespace
          if kubectl get deployment local-path-provisioner -n kube-system >/dev/null 2>&1; then
            echo "üîß Fixing RBAC for k3s Local Path Provisioner in kube-system..."
            
            # Apply RBAC for local-path-provisioner-service-account
            cat <<EOF | kubectl apply -f -
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          metadata:
            name: local-path-provisioner-role
          rules:
          - apiGroups: [""]
            resources: ["nodes"]
            verbs: ["get", "list"]
          - apiGroups: [""]
            resources: ["persistentvolumes"]
            verbs: ["get", "list", "watch", "create", "delete"]
          - apiGroups: [""]
            resources: ["persistentvolumeclaims"]
            verbs: ["get", "list", "watch", "update"]
          - apiGroups: [""]
            resources: ["pods"]
            verbs: ["create", "delete", "get", "list", "watch"]
          - apiGroups: ["storage.k8s.io"]
            resources: ["storageclasses"]
            verbs: ["get", "list", "watch"]
          - apiGroups: [""]
            resources: ["events"]
            verbs: ["create", "patch"]
          - apiGroups: [""]
            resources: ["endpoints"]
            verbs: ["get", "list", "watch", "create", "update", "patch"]
          ---
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: local-path-provisioner-binding
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: local-path-provisioner-role
          subjects:
          - kind: ServiceAccount
            name: local-path-provisioner-service-account
            namespace: kube-system
          EOF
            echo "‚úÖ RBAC for k3s Local Path Provisioner applied"
          fi
          
          # Ensure StorageClass exists (for both k3s and manual deployments)
          if ! kubectl get storageclass local-path >/dev/null 2>&1; then
            echo "üì¶ Creating StorageClass local-path..."
            cat <<EOF | kubectl apply -f -
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: local-path
            annotations:
              storageclass.kubernetes.io/is-default-class: "true"
          provisioner: rancher.io/local-path
          volumeBindingMode: WaitForFirstConsumer
          reclaimPolicy: Delete
          EOF
            echo "‚úÖ StorageClass local-path created"
          else
            echo "‚úÖ StorageClass local-path already exists"
          fi

      - name: Ensure StorageClass is default
        shell: bash
        run: |
          set -euo pipefail
          if kubectl get storageclass local-path >/dev/null 2>&1; then
            kubectl patch storageclass local-path \
              -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' \
              || echo "StorageClass already default"
          else
            echo "‚ö†Ô∏è StorageClass 'local-path' not found, skipping default annotation"
          fi

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl get pods -n local-path-storage
          kubectl get storageclass

  metrics:
    name: Deploy Metrics Server
    needs: local-path
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Ensure pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n kube-system \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl patch serviceaccount default -n kube-system \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true

      - name: Deploy Metrics Server
        shell: bash
        run: |
          set -euo pipefail
          echo "üì¶ Deploying Metrics Server from latest GitHub release..."
          curl -sL https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml | \
            sed "s|registry.k8s.io/metrics-server/metrics-server|${ACR_REGISTRY}/metrics-server/metrics-server|g" | \
            kubectl apply -f -
          
          # Patch metrics-server ServiceAccount with pull secret (must be done after deployment creates the SA)
          echo "üîê Patching metrics-server ServiceAccount with ACR pull secret..."
          sleep 2
          kubectl patch serviceaccount metrics-server -n kube-system \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          
          # Restart deployment to pick up the new pull secret
          kubectl rollout restart deployment/metrics-server -n kube-system || true
          
          echo "‚è≥ Waiting for Metrics Server to be ready..."
          kubectl -n kube-system rollout status deployment/metrics-server --timeout=5m

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n kube-system get deploy metrics-server
          kubectl top nodes || echo "Metrics may take a few moments to appear."

  ingress:
    name: Deploy Ingress Controller
    needs: metrics
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Remove Traefik (k3s default) to avoid conflicts
        shell: bash
        run: |
          set -euo pipefail
          echo "üóëÔ∏è Removing Traefik components to avoid port conflicts with NGINX..."
          
          # Delete Traefik Service (LoadBalancer)
          kubectl delete -n kube-system svc traefik --ignore-not-found=true || true
          
          # Delete Traefik Deployment
          kubectl delete -n kube-system deployment traefik --ignore-not-found=true || true
          
          # Delete Traefik DaemonSet (svclb)
          kubectl delete -n kube-system daemonset svclb-traefik --ignore-not-found=true || true
          
          # Delete Traefik Helm Install Jobs (CRD and main install) - delete all matching jobs
          kubectl delete -n kube-system jobs -l app=helm,name=traefik --ignore-not-found=true || true
          kubectl delete -n kube-system job helm-install-traefik-crd --ignore-not-found=true || true
          kubectl delete -n kube-system job helm-install-traefik --ignore-not-found=true || true
          
          # Wait a bit for resources to be cleaned up
          sleep 5
          
          echo "‚úÖ Traefik components and Helm install jobs removed (if they existed)"
          echo "‚ÑπÔ∏è  Note: Traefik jobs will be deleted on each workflow run. To permanently disable Traefik,"
          echo "   manually add '--disable traefik' to k3s startup flags or configure /etc/rancher/k3s/config.yaml"

      - name: Ensure namespace and pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns ingress-nginx >/dev/null 2>&1 || kubectl create namespace ingress-nginx
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n ingress-nginx \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl patch serviceaccount default -n ingress-nginx \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true

      - name: Install helm dependencies
        shell: bash
        run: |
          set -euo pipefail
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo update

      - name: Deploy ingress controller
        shell: bash
        run: |
          set -euo pipefail
          ACR_IMAGE="${ACR_REGISTRY}/ingress-nginx/controller:v1.9.4"
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            -n ingress-nginx \
            -f infra/addons/values/ingress-nginx.yaml \
            --set controller.image.image="$ACR_IMAGE" \
            --timeout 5m
          kubectl patch serviceaccount ingress-nginx -n ingress-nginx \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          kubectl rollout status deployment/ingress-nginx-controller -n ingress-nginx --timeout=5m

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl get pods -n ingress-nginx

  monitoring:
    name: Deploy Monitoring Stack
    needs: ingress
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download mirror artifacts
        uses: actions/download-artifact@v4
        with:
          name: addon-mirror-metadata

      - name: Restore generated values
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p infra/addons/values
          # Artifacts preserve directory structure, so check both locations
          if [ -f "infra/addons/values/observability.acr.yaml" ]; then
            echo "‚úÖ observability.acr.yaml found in infra/addons/values/"
          elif [ -f "observability.acr.yaml" ]; then
            cp observability.acr.yaml infra/addons/values/observability.acr.yaml
            echo "‚úÖ observability.acr.yaml copied to infra/addons/values/"
          else
            echo "‚ö†Ô∏è observability.acr.yaml not found, will be generated if needed"
          fi
          
          # Validierung: Pr√ºfe ob observability.acr.yaml alle Images auf ACR umschreibt
          if [ -f "infra/addons/values/observability.acr.yaml" ]; then
            echo "üîç Validating observability.acr.yaml..."
            # Pr√ºfe ob noch quay.io oder docker.io Referenzen vorhanden sind (sollten nicht sein)
            PUBLIC_REFS=$(grep -E "(quay\.io|docker\.io)" infra/addons/values/observability.acr.yaml | grep -v "^#" | grep -v "registry:" || echo "")
            if [ -n "$PUBLIC_REFS" ]; then
              echo "‚ö†Ô∏è Warning: Found public registry references in observability.acr.yaml:"
              echo "$PUBLIC_REFS"
              echo "‚ö†Ô∏è These should be rewritten to use ACR registry"
            else
              echo "‚úÖ All image references appear to use ACR registry"
            fi
            
            # Zeige ein paar Image-Referenzen zur Verifikation
            echo "üìã Sample image references from observability.acr.yaml:"
            grep -A 2 "repository:" infra/addons/values/observability.acr.yaml | head -10 || true
          fi

      - name: Setup CICD access
        uses: ./.github/actions/setup-cicd
        with:
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Prepare namespace and pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns monitoring >/dev/null 2>&1 || kubectl create namespace monitoring
          kubectl create secret docker-registry acr-pull \
            --docker-server="${ACR_REGISTRY}" \
            --docker-username="${ACR_USERNAME}" \
            --docker-password="${ACR_PASSWORD}" \
            -n monitoring \
            --dry-run=client -o yaml | kubectl apply -f -
          
          # Patch default ServiceAccount
          kubectl patch serviceaccount default -n monitoring \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          
          # Patch kube-prometheus-stack-admission ServiceAccount (wird vom Chart erstellt)
          kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          
          # Patch alle ServiceAccounts im monitoring Namespace (f√ºr zuk√ºnftige)
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
            echo "Patching ServiceAccount: $sa"
            kubectl patch serviceaccount "$sa" -n monitoring \
              -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || true
          done

      - name: Install helm dependencies
        shell: bash
        run: |
          set -euo pipefail
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add grafana https://grafana.github.io/helm-charts
          helm repo update

      - name: Deploy monitoring stack
        shell: bash
        run: |
          set -euo pipefail
          
          # Debug: Zeige die verwendete Values-Datei
          echo "üìã Using values file:"
          cat infra/addons/values/observability.acr.yaml | head -50
          
          # Debug: Pr√ºfe aktuelle Pods vor Deployment
          echo "üìä Current pods in monitoring namespace:"
          kubectl get pods -n monitoring || true
          
          # Bereinige alte fehlgeschlagene Admission-Jobs
          echo "üßπ Cleaning up old failed admission jobs..."
          # L√∂sche Jobs (auch Terminating) - force delete f√ºr h√§ngende Jobs
          kubectl delete jobs -n monitoring -l app.kubernetes.io/name=kube-prometheus-stack,app.kubernetes.io/component=admission-webhook --ignore-not-found=true --grace-period=0 --force || true
          # L√∂sche Pods direkt (auch Terminating) - wichtig, da Pods manchmal h√§ngen bleiben
          kubectl get pods -n monitoring -l app.kubernetes.io/name=kube-prometheus-stack,app.kubernetes.io/component=admission-webhook -o name 2>/dev/null | xargs -r kubectl delete -n monitoring --grace-period=0 --force --ignore-not-found=true || true
          
          # Warte l√§nger, damit Cleanup abgeschlossen ist
          sleep 5
          
          # Deploy asynchron (ohne --wait, damit Workflow nicht h√§ngt)
          echo "üöÄ Deploying monitoring stack (asynchron)..."
          
          # Aggressives Cleanup f√ºr stuck Helm-Releases (IMMER ausf√ºhren)
          echo "üîß Force cleaning Helm locks and stuck releases..."
          
          # Versuche normale Uninstall (mit --no-hooks um Hooks zu √ºberspringen)
          helm uninstall kube-prometheus-stack -n monitoring --no-hooks --ignore-not-found=true 2>/dev/null || true
          
          # Debug: Zeige alle Secrets im Namespace
          echo "üìã Current secrets in monitoring namespace:"
          kubectl get secrets -n monitoring -o name 2>/dev/null || true
          
          # L√∂sche ALLE Helm-Secrets die mit dem Release zu tun haben
          echo "üóëÔ∏è Deleting Helm release secrets..."
          # Helm 3 Format: sh.helm.release.v1.<release-name>.<revision>
          kubectl get secrets -n monitoring -o name 2>/dev/null | grep -E "sh\.helm\.release\.v1\.kube-prometheus-stack\." | xargs -r kubectl delete -n monitoring --ignore-not-found=true || true
          # Alternative Formate
          kubectl get secrets -n monitoring -o name 2>/dev/null | grep -i "helm.*kube-prometheus-stack" | xargs -r kubectl delete -n monitoring --ignore-not-found=true || true
          # Mit Labels
          kubectl delete secret -n monitoring -l owner=helm,name=kube-prometheus-stack --ignore-not-found=true || true
          kubectl delete secret -n monitoring -l name=kube-prometheus-stack --ignore-not-found=true || true
          
          # L√∂sche ConfigMaps
          echo "üóëÔ∏è Deleting Helm release configmaps..."
          kubectl delete configmap -n monitoring -l owner=helm,name=kube-prometheus-stack --ignore-not-found=true || true
          kubectl get configmaps -n monitoring -o name 2>/dev/null | grep "kube-prometheus-stack" | xargs -r kubectl delete -n monitoring --ignore-not-found=true || true
          
          # Warte damit Cleanup abgeschlossen ist
          sleep 10
          
          # Debug: Zeige ob noch Secrets vorhanden sind
          echo "üìã Remaining secrets after cleanup:"
          kubectl get secrets -n monitoring -o name 2>/dev/null | grep -E "(helm|kube-prometheus)" || echo "No Helm secrets found"
          
          # KRITISCH: Erstelle und patche admission ServiceAccount VOR Helm-Deployment
          # Helm Hooks (post-install) erstellen Jobs, die sofort laufen und ImagePullSecrets ben√∂tigen
          echo "üîê Creating and patching admission ServiceAccount BEFORE Helm deployment..."
          if ! kubectl get serviceaccount kube-prometheus-stack-admission -n monitoring 2>/dev/null; then
            echo "  Creating kube-prometheus-stack-admission ServiceAccount..."
            kubectl create serviceaccount kube-prometheus-stack-admission -n monitoring || true
            kubectl label serviceaccount kube-prometheus-stack-admission -n monitoring \
              app.kubernetes.io/name=kube-prometheus-stack \
              app.kubernetes.io/component=prometheus-operator-webhook \
              app.kubernetes.io/managed-by=Helm --overwrite || true
            kubectl annotate serviceaccount kube-prometheus-stack-admission -n monitoring \
              helm.sh/hook=pre-install,pre-upgrade,post-install,post-upgrade --overwrite || true
          fi
          # Patch den ServiceAccount mit imagePullSecrets
          echo "  Patching kube-prometheus-stack-admission ServiceAccount with acr-pull..."
          kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null || true
          
          # Verifiziere dass der ServiceAccount gepatcht ist
          ADMISSION_IPS=$(kubectl get serviceaccount kube-prometheus-stack-admission -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
          if [[ "$ADMISSION_IPS" == *"acr-pull"* ]]; then
            echo "  ‚úÖ Admission ServiceAccount is patched: $ADMISSION_IPS"
          else
            echo "  ‚ö†Ô∏è Warning: Admission ServiceAccount may not be patched correctly"
            # Versuche nochmal mit JSON Patch (falls bereits ImagePullSecrets vorhanden sind)
            kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
              --type='json' \
              -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null || \
            kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
              -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null || true
          fi
          
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            -n monitoring \
            -f infra/addons/values/observability.acr.yaml \
            --timeout 10m
          
          echo "‚úÖ Helm deployment command completed. Resources are being created in the background."
          
          # KRITISCH: Patch ALLE ServiceAccounts SOFORT nach Helm-Deployment
          # Helm kann ServiceAccounts √ºberschreiben, daher m√ºssen wir sie erneut patchen
          echo "üîê CRITICAL: Patching all ServiceAccounts immediately after Helm deployment..."
          sleep 2  # Kurz warten, damit Helm ServiceAccounts erstellt hat
          PATCHED_COUNT=0
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
            HAS_SECRET=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[?(@.name=="acr-pull")].name}' 2>/dev/null || echo "")
            if [ -z "$HAS_SECRET" ]; then
              echo "  üîê Patching ServiceAccount: $sa"
              CURRENT_IPS=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
              if [ -n "$CURRENT_IPS" ]; then
                kubectl patch serviceaccount "$sa" -n monitoring \
                  --type='json' \
                  -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null && PATCHED_COUNT=$((PATCHED_COUNT + 1)) || true
              else
                kubectl patch serviceaccount "$sa" -n monitoring \
                  -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null && PATCHED_COUNT=$((PATCHED_COUNT + 1)) || true
              fi
            fi
          done
          echo "‚úÖ Immediately patched $PATCHED_COUNT ServiceAccounts after Helm deployment"
          
          # DEBUG: Pr√ºfe welche Images die Pods verwenden
          echo "üîç DEBUG: Checking which images pods are trying to pull..."
          sleep 5  # Warte kurz, damit Pods erstellt werden
          for pod in $(kubectl get pods -n monitoring -o name 2>/dev/null | head -5); do
            echo "üì¶ Pod: $pod"
            IMAGE=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.containers[0].image}' 2>/dev/null || echo "N/A")
            echo "  Image: $IMAGE"
            SA=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.serviceAccountName}' 2>/dev/null || echo "default")
            echo "  ServiceAccount: $SA"
            IPS=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.imagePullSecrets[*].name}' 2>/dev/null || echo "(none)")
            echo "  ImagePullSecrets: $IPS"
            echo "---"
          done
          
          # DEBUG: Pr√ºfe ob ServiceAccounts ImagePullSecrets haben
          echo "üîç DEBUG: Checking ServiceAccount ImagePullSecrets..."
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null); do
            echo "üìã $sa:"
            IPS=$(kubectl get $sa -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "(none)")
            echo "  ImagePullSecrets: $IPS"
          done
          
          # DEBUG: Zeige erwartete ACR Image-Pfade
          echo "üîç DEBUG: Expected ACR image paths (should match pod images above):"
          if [ -n "${ACR_REGISTRY:-}" ]; then
            echo "ACR Registry: $ACR_REGISTRY"
            echo "Expected images:"
            echo "  - ${ACR_REGISTRY}/grafana/grafana:10.2.0"
            echo "  - ${ACR_REGISTRY}/prometheus/prometheus:v2.48.0"
            echo "  - ${ACR_REGISTRY}/prometheus-operator/prometheus-operator:v0.86.2"
            echo "  - ${ACR_REGISTRY}/jettech/kube-webhook-certgen:v1.5.1"
            echo "  - ${ACR_REGISTRY}/prometheus/node-exporter:v1.10.2"
            echo "  - ${ACR_REGISTRY}/kube-state-metrics/kube-state-metrics:v2.10.1"
          fi
          
          # KRITISCH: Pr√ºfe ob Helm Release existiert, falls nicht erstelle ServiceAccounts manuell
          echo "üîç Checking if Helm release exists..."
          HELM_RELEASE_EXISTS=$(helm list -n monitoring -q 2>/dev/null | grep -q "kube-prometheus-stack" && echo "true" || echo "false")
          
          if [ "$HELM_RELEASE_EXISTS" = "false" ]; then
            echo "‚ö†Ô∏è Helm release does not exist! Creating ServiceAccounts manually before Helm deployment..."
            
            # Erstelle kritische ServiceAccounts manuell (falls sie fehlen), damit Jobs nicht fehlschlagen
            echo "üîê Creating admission ServiceAccount if missing..."
            if ! kubectl get serviceaccount kube-prometheus-stack-admission -n monitoring 2>/dev/null; then
              kubectl create serviceaccount kube-prometheus-stack-admission -n monitoring || true
              kubectl label serviceaccount kube-prometheus-stack-admission -n monitoring \
                app.kubernetes.io/name=kube-prometheus-stack \
                app.kubernetes.io/component=prometheus-operator-webhook \
                app.kubernetes.io/managed-by=Helm --overwrite || true
              kubectl annotate serviceaccount kube-prometheus-stack-admission -n monitoring \
                helm.sh/hook=pre-install,pre-upgrade,post-install,post-upgrade --overwrite || true
            fi
            # Patch den ServiceAccount mit imagePullSecrets
            kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
              -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' --ignore-not-found=true || true
            
            echo "‚úÖ ServiceAccount created/verified"
          fi
          
          # KRITISCH: Kontinuierliches Patchen von ServiceAccounts nach Helm-Deployment
          echo "üîê CRITICAL: Continuously patching ServiceAccounts after Helm deployment..."
          echo "‚ö†Ô∏è This ensures ALL ServiceAccounts get patched, even if created later"
          
          # Kontinuierliche Loop: Patch ServiceAccounts und l√∂sche fehlgeschlagene Pods
          MAX_ITERATIONS=20  # 20 * 5s = 100 Sekunden max
          ITERATION=0
          LAST_PATCHED_COUNT=0
          
          while [ $ITERATION -lt $MAX_ITERATIONS ]; do
            ITERATION=$((ITERATION + 1))
            echo "üîÑ Iteration $ITERATION/$MAX_ITERATIONS: Checking and patching ServiceAccounts..."
            
            # Erstelle admission ServiceAccount falls er fehlt (wichtig f√ºr Pre-Install Hooks)
            if ! kubectl get serviceaccount kube-prometheus-stack-admission -n monitoring 2>/dev/null; then
              kubectl create serviceaccount kube-prometheus-stack-admission -n monitoring || true
              kubectl label serviceaccount kube-prometheus-stack-admission -n monitoring \
                app.kubernetes.io/name=kube-prometheus-stack \
                app.kubernetes.io/component=prometheus-operator-webhook \
                app.kubernetes.io/managed-by=Helm --overwrite || true
              kubectl annotate serviceaccount kube-prometheus-stack-admission -n monitoring \
                helm.sh/hook=pre-install,pre-upgrade,post-install,post-upgrade --overwrite || true
            fi
            # Patch den ServiceAccount mit imagePullSecrets
            kubectl patch serviceaccount kube-prometheus-stack-admission -n monitoring \
              -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' --ignore-not-found=true || true
            
            # Patch ALLE ServiceAccounts (auch neu erstellte) - INKLUSIVE default
            PATCHED_COUNT=0
            for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
              # Pr√ºfe ob ServiceAccount bereits acr-pull als ImagePullSecret hat
              HAS_SECRET=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[?(@.name=="acr-pull")].name}' 2>/dev/null || echo "")
              if [ -z "$HAS_SECRET" ]; then
                echo "  üîê Patching ServiceAccount: $sa"
                # Hole aktuelle ImagePullSecrets (falls vorhanden)
                CURRENT_IPS=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
                if [ -n "$CURRENT_IPS" ]; then
                  # F√ºge acr-pull zu bestehenden Secrets hinzu
                  kubectl patch serviceaccount "$sa" -n monitoring \
                    --type='json' \
                    -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null && PATCHED_COUNT=$((PATCHED_COUNT + 1)) && echo "    ‚úÖ Patched $sa (added to existing)" || true
                else
                  # Erstelle neue ImagePullSecrets Liste
                  kubectl patch serviceaccount "$sa" -n monitoring \
                    -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null && PATCHED_COUNT=$((PATCHED_COUNT + 1)) && echo "    ‚úÖ Patched $sa (created new)" || true
                fi
              else
                echo "  ‚úÖ ServiceAccount $sa already has acr-pull"
              fi
            done
            
            if [ $PATCHED_COUNT -gt 0 ]; then
              echo "‚úÖ Patched $PATCHED_COUNT new ServiceAccounts"
              LAST_PATCHED_COUNT=$PATCHED_COUNT
            fi
            
            # L√∂sche Pods mit ImagePullBackOff oder ErrImagePull, damit sie mit gepatchten ServiceAccounts neu erstellt werden
            IMAGEPULL_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff" || @.status.containerStatuses[0].state.waiting.reason=="ErrImagePull")].metadata.name}' 2>/dev/null || echo "")
            if [ -n "$IMAGEPULL_PODS" ]; then
              echo "üóëÔ∏è Deleting pods with ImagePullBackOff/ErrImagePull to trigger restart..."
              for pod in $IMAGEPULL_PODS; do
                echo "  üóëÔ∏è Deleting pod: $pod"
                kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
              done
            fi
            
            # L√∂sche fehlgeschlagene Admission-Jobs NUR wenn ServiceAccount gepatcht ist
            ADMISSION_SA_EXISTS=$(kubectl get serviceaccount kube-prometheus-stack-admission -n monitoring -o name 2>/dev/null || echo "")
            if [ -n "$ADMISSION_SA_EXISTS" ]; then
              ADMISSION_SA_PATCHED=$(kubectl get serviceaccount kube-prometheus-stack-admission -n monitoring -o jsonpath='{.imagePullSecrets[?(@.name=="acr-pull")].name}' 2>/dev/null || echo "")
              if [ -n "$ADMISSION_SA_PATCHED" ]; then
                echo "üîÑ Admission ServiceAccount is patched, deleting failed jobs..."
                kubectl delete jobs -n monitoring -l app.kubernetes.io/component=admission-webhook --ignore-not-found=true || true
                kubectl delete jobs -n monitoring -l app.kubernetes.io/name=kube-prometheus-stack,app.kubernetes.io/component=admission-webhook --ignore-not-found=true || true
              else
                echo "‚è≥ Waiting for admission ServiceAccount to be patched before deleting jobs..."
              fi
            fi
            
            # Wenn keine neuen ServiceAccounts mehr gepatcht wurden und keine ImagePullBackOff/ErrImagePull Pods mehr existieren, k√∂nnen wir aufh√∂ren
            if [ $PATCHED_COUNT -eq 0 ] && [ -z "$IMAGEPULL_PODS" ] && [ $ITERATION -gt 5 ]; then
              echo "‚úÖ No more ServiceAccounts to patch and no ImagePullBackOff/ErrImagePull pods - continuing..."
              break
            fi
            
            sleep 5  # Warte zwischen Iterationen
          done
          
          echo "‚úÖ Finished continuous ServiceAccount patching. Patched $LAST_PATCHED_COUNT ServiceAccounts in total."
          sleep 3  # Kurz warten, damit alles angewendet ist
          
          # Pr√ºfe ob Ressourcen erstellt wurden (falls nicht, ist Helm stuck)
          echo "üîç Checking if resources were created..."
          sleep 2  # Kurz warten, damit Helm Ressourcen erstellt hat
          
          OPERATOR_EXISTS=$(kubectl get deployment kube-prometheus-stack-operator -n monitoring -o name 2>/dev/null || echo "")
          if [ -z "$OPERATOR_EXISTS" ]; then
            echo "‚ö†Ô∏è Operator deployment not found! Helm may be stuck. Checking status..."
            echo "üìä Current resources in monitoring namespace:"
            kubectl get all -n monitoring || true
            echo "üìä Current jobs:"
            kubectl get jobs -n monitoring || true
            
            # Pr√ºfe ob fehlgeschlagene Admission-Jobs das Deployment blockieren
            FAILED_JOBS=$(kubectl get jobs -n monitoring -o jsonpath='{.items[?(@.status.failed>0)].metadata.name}' 2>/dev/null || echo "")
            if [ -n "$FAILED_JOBS" ]; then
              echo "üîß Found failed jobs blocking deployment. Cleaning up..."
              kubectl delete jobs -n monitoring --all --ignore-not-found=true || true
              sleep 5
            fi
            
            # Versuche nochmal zu warten (manchmal dauert es etwas l√§nger)
            echo "‚è≥ Waiting additional 30s for resources to be created..."
            sleep 30
            
            OPERATOR_EXISTS=$(kubectl get deployment kube-prometheus-stack-operator -n monitoring -o name 2>/dev/null || echo "")
            if [ -z "$OPERATOR_EXISTS" ]; then
              echo "‚ùå Operator still not found after cleanup. Helm deployment may have failed."
              echo "üìä Current state:"
              kubectl get all -n monitoring || true
              echo "‚ö†Ô∏è Continuing anyway - resources may be created later..."
            fi
          fi
          
          # Warte gezielt auf den Operator (wichtigster Teil) - mit Retry-Logik
          echo "‚è≥ Waiting for operator to be ready..."
          MAX_RETRIES=12  # 12 * 10s = 2 Minuten
          RETRY_COUNT=0
          OPERATOR_READY=false
          
          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            if kubectl rollout status deployment/kube-prometheus-stack-operator -n monitoring --timeout=10s 2>/dev/null; then
              OPERATOR_READY=true
              echo "‚úÖ Operator is ready!"
              break
            fi
            RETRY_COUNT=$((RETRY_COUNT + 1))
            echo "‚è≥ Operator not ready yet (attempt $RETRY_COUNT/$MAX_RETRIES)..."
            sleep 10
          done
          
          if [ "$OPERATOR_READY" = false ]; then
            echo "‚ö†Ô∏è Operator rollout status check failed or timed out"
            echo "üìä Checking operator pod status:"
            kubectl get pods -n monitoring -l app.kubernetes.io/name=kube-prometheus-stack,app.kubernetes.io/component=operator || true
            echo "‚ö†Ô∏è Continuing anyway - ServiceAccounts are already patched in continuous loop..."
          fi
          
          # Zeige Status aller Pods
          echo "üìä Current pod status:"
          kubectl get pods -n monitoring -o wide || true

      - name: Patch ServiceAccounts after deployment and restart failed jobs
        shell: bash
        run: |
          set -euo pipefail
          echo "üîê Final patching of all ServiceAccounts and cleanup of failed pods..."
          
          # Patch ALLE ServiceAccounts, die noch nicht gepatcht sind (INKLUSIVE default)
          PATCHED_COUNT=0
          for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
            # Pr√ºfe ob ServiceAccount bereits acr-pull als ImagePullSecret hat
            HAS_SECRET=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[?(@.name=="acr-pull")].name}' 2>/dev/null || echo "")
            if [ -z "$HAS_SECRET" ]; then
              echo "üîê Patching ServiceAccount: $sa"
              # Hole aktuelle ImagePullSecrets (falls vorhanden)
              CURRENT_IPS=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
              if [ -n "$CURRENT_IPS" ]; then
                # F√ºge acr-pull zu bestehenden Secrets hinzu
                if kubectl patch serviceaccount "$sa" -n monitoring \
                  --type='json' \
                  -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null; then
                  PATCHED_COUNT=$((PATCHED_COUNT + 1))
                  echo "  ‚úÖ Patched $sa (added to existing)"
                fi
              else
                # Erstelle neue ImagePullSecrets Liste
                if kubectl patch serviceaccount "$sa" -n monitoring \
                  -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null; then
                  PATCHED_COUNT=$((PATCHED_COUNT + 1))
                  echo "  ‚úÖ Patched $sa (created new)"
                fi
              fi
            else
              echo "  ‚úì ServiceAccount $sa already patched"
            fi
          done
          echo "‚úÖ Patched $PATCHED_COUNT ServiceAccounts"
          
          # Warte kurz, damit Patch angewendet ist
          sleep 3
          
          # Pr√ºfe ob fehlgeschlagene Jobs oder Pods existieren
          echo "üîç Checking for failed jobs and pods..."
          FAILED_JOBS=$(kubectl get jobs -n monitoring -o jsonpath='{.items[?(@.status.failed>0)].metadata.name}' 2>/dev/null || echo "")
          IMAGEPULL_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff" || @.status.containerStatuses[0].state.waiting.reason=="ErrImagePull")].metadata.name}' 2>/dev/null || echo "")
          
          # Restart Jobs/Pods, die ImagePullBackOff oder ErrImagePull haben oder fehlgeschlagen sind
          if [ -n "$FAILED_JOBS" ] || [ -n "$IMAGEPULL_PODS" ]; then
            echo "üîÑ Restarting failed jobs and pods..."
            
            # L√∂sche alle Admission-Jobs (werden automatisch neu erstellt)
            for job in $(kubectl get jobs -n monitoring -o name 2>/dev/null | grep -E "(admission|webhook)"); do
              echo "  üóëÔ∏è Deleting job: $job"
              kubectl delete "$job" -n monitoring --ignore-not-found=true --grace-period=0 --force || true
            done
            
            # L√∂sche auch alle Pods die ImagePullBackOff oder ErrImagePull haben (nicht nur Jobs)
            for pod in $IMAGEPULL_PODS; do
              echo "  üóëÔ∏è Deleting pod with ImagePullBackOff/ErrImagePull: $pod"
              kubectl delete pod "$pod" -n monitoring --ignore-not-found=true --grace-period=0 --force || true
            done
            
            # KRITISCH: L√∂sche ALLE Pods, die keine ImagePullSecrets haben (damit sie mit gepatchten ServiceAccounts neu erstellt werden)
            echo "üîÑ Checking for pods without ImagePullSecrets..."
            for pod in $(kubectl get pods -n monitoring -o name 2>/dev/null); do
              POD_IPS=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.imagePullSecrets[*].name}' 2>/dev/null || echo "")
              if [ -z "$POD_IPS" ]; then
                echo "  üóëÔ∏è Deleting pod without ImagePullSecrets: $pod"
                kubectl delete $pod -n monitoring --ignore-not-found=true --grace-period=0 --force || true
              fi
            done
            
            # Warte kurz, damit neue Jobs/Pods gestartet werden
            sleep 5
            
            # Zeige Status der Jobs
            echo "üìä Job status after restart:"
            kubectl get jobs -n monitoring | grep -E "(admission|webhook)" || echo "No admission jobs found"
          else
            echo "‚úÖ No failed jobs or ImagePullBackOff pods found - all resources are healthy"
          fi
          
          # KRITISCH: Pr√ºfe ob Pods noch von √∂ffentlichen Registries ziehen und patche ServiceAccounts erneut
          echo "üîç Checking if pods are pulling from ACR or public registries..."
          sleep 5  # Warte kurz, damit Pods erstellt sind
          
          # Finde Pods die noch von quay.io, docker.io oder anderen √∂ffentlichen Registries ziehen
          PUBLIC_REGISTRY_PODS=""
          for pod in $(kubectl get pods -n monitoring -o name 2>/dev/null); do
            # Pr√ºfe alle Container im Pod (auch Init-Container)
            CONTAINER_IMAGES=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.containers[*].image}' 2>/dev/null || echo "")
            INIT_IMAGES=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.initContainers[*].image}' 2>/dev/null || echo "")
            ALL_IMAGES="$CONTAINER_IMAGES $INIT_IMAGES"
            
            for image in $ALL_IMAGES; do
              if [[ "$image" == quay.io/* ]] || [[ "$image" == docker.io/* ]] || [[ "$image" == registry.k8s.io/* ]]; then
                if [[ "$PUBLIC_REGISTRY_PODS" != *"$pod"* ]]; then
                  if [ -z "$PUBLIC_REGISTRY_PODS" ]; then
                    PUBLIC_REGISTRY_PODS="$pod"
                  else
                    PUBLIC_REGISTRY_PODS="$PUBLIC_REGISTRY_PODS $pod"
                  fi
                  echo "  ‚ö†Ô∏è Pod $pod is pulling from public registry: $image"
                fi
              fi
            done
          done
          
          # Wenn Pods noch von √∂ffentlichen Registries ziehen, patche alle ServiceAccounts erneut
          if [ -n "$PUBLIC_REGISTRY_PODS" ]; then
            echo "‚ö†Ô∏è Found pods pulling from public registries. Patching all ServiceAccounts again..."
            PATCHED_AGAIN=0
            for sa in $(kubectl get serviceaccounts -n monitoring -o name 2>/dev/null | cut -d/ -f2); do
              HAS_SECRET=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[?(@.name=="acr-pull")].name}' 2>/dev/null || echo "")
              if [ -z "$HAS_SECRET" ]; then
                echo "  üîê Patching ServiceAccount: $sa"
                CURRENT_IPS=$(kubectl get serviceaccount "$sa" -n monitoring -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
                if [ -n "$CURRENT_IPS" ]; then
                  kubectl patch serviceaccount "$sa" -n monitoring \
                    --type='json' \
                    -p='[{"op": "add", "path": "/imagePullSecrets/-", "value": {"name": "acr-pull"}}]' 2>/dev/null && PATCHED_AGAIN=$((PATCHED_AGAIN + 1)) || true
                else
                  kubectl patch serviceaccount "$sa" -n monitoring \
                    -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' 2>/dev/null && PATCHED_AGAIN=$((PATCHED_AGAIN + 1)) || true
                fi
              fi
            done
            echo "‚úÖ Repatched $PATCHED_AGAIN ServiceAccounts"
            
            # L√∂sche Pods die von √∂ffentlichen Registries ziehen, damit sie neu erstellt werden
            echo "üóëÔ∏è Deleting pods pulling from public registries..."
            for pod in $PUBLIC_REGISTRY_PODS; do
              echo "  üóëÔ∏è Deleting pod: $pod"
              kubectl delete $pod -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
            
            echo "‚è≥ Waiting for pods to be recreated..."
            sleep 10
          else
            echo "‚úÖ All pods are pulling from ACR"
          fi
          
          # Finaler Status-Check
          echo "üìä Final pod status:"
          kubectl get pods -n monitoring -o wide || true
          
          # Zeige welche Images die Pods verwenden
          echo "üì¶ Pod images (should all be from ACR):"
          for pod in $(kubectl get pods -n monitoring -o name 2>/dev/null | head -10); do
            IMAGE=$(kubectl get $pod -n monitoring -o jsonpath='{.spec.containers[0].image}' 2>/dev/null || echo "N/A")
            echo "  $pod: $IMAGE"
          done
          
          # Zeige ServiceAccounts Status
          echo "üìä ServiceAccounts with imagePullSecrets:"
          kubectl get serviceaccounts -n monitoring -o jsonpath='{range .items[*]}{.metadata.name}{": "}{.imagePullSecrets[*].name}{"\n"}{end}' || true

      - name: Cleanup failed resources
        shell: bash
        run: |
          set -euo pipefail
          echo "üßπ Cleaning up failed and terminated resources..."
          
          # 1. L√∂sche alle Terminating Pods
          echo "üóëÔ∏è Cleaning up Terminating pods..."
          TERMINATING_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.metadata.deletionTimestamp!=null)].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$TERMINATING_PODS" ]; then
            for pod in $TERMINATING_PODS; do
              echo "  üóëÔ∏è Force deleting terminating pod: $pod"
              kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          fi
          
          # 2. L√∂sche alle Pods mit Error/ImagePullBackOff/ErrImagePull Status
          echo "üóëÔ∏è Cleaning up pods with Error/ImagePullBackOff/ErrImagePull status..."
          ERROR_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.phase=="Failed" || @.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff" || @.status.containerStatuses[0].state.waiting.reason=="ErrImagePull" || @.status.containerStatuses[0].state.terminated.reason=="Error")].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$ERROR_PODS" ]; then
            for pod in $ERROR_PODS; do
              echo "  üóëÔ∏è Deleting error pod: $pod"
              kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          fi
          
          # 3. L√∂sche alle fehlgeschlagenen Jobs
          echo "üóëÔ∏è Cleaning up failed jobs..."
          FAILED_JOBS=$(kubectl get jobs -n monitoring -o jsonpath='{.items[?(@.status.failed>0)].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$FAILED_JOBS" ]; then
            for job in $FAILED_JOBS; do
              echo "  üóëÔ∏è Deleting failed job: $job"
              kubectl delete job "$job" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          fi
          
          # 4. L√∂sche alle Completed Jobs (die nicht mehr ben√∂tigt werden, besonders admission/webhook)
          echo "üóëÔ∏è Cleaning up completed admission/webhook jobs..."
          for job in $(kubectl get jobs -n monitoring -o name 2>/dev/null | grep -E "(admission|webhook|patch)"); do
            JOB_STATUS=$(kubectl get $job -n monitoring -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null || echo "")
            if [ "$JOB_STATUS" = "True" ]; then
              echo "  üóëÔ∏è Deleting completed job: $job"
              kubectl delete $job -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            fi
          done
          
          # 5. L√∂sche alle Pods im CrashLoopBackOff Status
          echo "üóëÔ∏è Cleaning up CrashLoopBackOff pods..."
          CRASHLOOP_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.containerStatuses[0].state.waiting.reason=="CrashLoopBackOff")].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$CRASHLOOP_PODS" ]; then
            for pod in $CRASHLOOP_PODS; do
              echo "  üóëÔ∏è Deleting CrashLoopBackOff pod: $pod"
              kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
            done
          fi
          
          # 6. L√∂sche alle Pending Pods die l√§nger als 5 Minuten warten (wahrscheinlich h√§ngend)
          echo "üóëÔ∏è Cleaning up stuck pending pods..."
          PENDING_PODS=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.phase=="Pending")].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$PENDING_PODS" ]; then
            for pod in $PENDING_PODS; do
              POD_AGE=$(kubectl get pod "$pod" -n monitoring -o jsonpath='{.metadata.creationTimestamp}' 2>/dev/null || echo "")
              if [ -n "$POD_AGE" ]; then
                # Pr√ºfe ob Pod √§lter als 5 Minuten ist (vereinfacht - pr√ºft nur ob er existiert)
                echo "  üóëÔ∏è Deleting pending pod (may be stuck): $pod"
                kubectl delete pod "$pod" -n monitoring --grace-period=0 --force --ignore-not-found=true || true
              fi
            done
          fi
          
          # 7. Warte kurz, damit Cleanup abgeschlossen ist
          sleep 5
          
          # 8. Finale Status-Anzeige
          echo "üìä Final cleanup status:"
          TERMINATING_COUNT=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.metadata.deletionTimestamp!=null)].metadata.name}' 2>/dev/null | wc -w || echo 0)
          ERROR_COUNT=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.phase=="Failed")].metadata.name}' 2>/dev/null | wc -w || echo 0)
          IMAGEPULL_COUNT=$(kubectl get pods -n monitoring -o jsonpath='{.items[?(@.status.containerStatuses[0].state.waiting.reason=="ImagePullBackOff")].metadata.name}' 2>/dev/null | wc -w || echo 0)
          FAILED_JOBS_COUNT=$(kubectl get jobs -n monitoring -o jsonpath='{.items[?(@.status.failed>0)].metadata.name}' 2>/dev/null | wc -w || echo 0)
          
          echo "  Terminating pods: $TERMINATING_COUNT"
          echo "  Error pods: $ERROR_COUNT"
          echo "  ImagePullBackOff pods: $IMAGEPULL_COUNT"
          echo "  Failed jobs: $FAILED_JOBS_COUNT"
          
          if [ "$TERMINATING_COUNT" -eq 0 ] && [ "$ERROR_COUNT" -eq 0 ] && [ "$IMAGEPULL_COUNT" -eq 0 ] && [ "$FAILED_JOBS_COUNT" -eq 0 ]; then
            echo "‚úÖ All cleanup completed - workspace is clean!"
          else
            echo "‚ö†Ô∏è Some resources may still need cleanup"
          fi

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          kubectl get pods -n monitoring