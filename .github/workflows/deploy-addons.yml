name: Deploy Cluster Infrastructure

on:
  workflow_dispatch:
  push:
    branches: [ main ]
    paths:
      - 'infra/addons/**'
      - '.github/workflows/deploy-addons.yml'

jobs:
  # mirror-images:
  #   name: Mirror Images to ACR
  #   runs-on: ubuntu-latest
  #   env:
  #     ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
  #     ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
  #     ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
  #   steps:
  #     - name: Login to ACR
  #       uses: docker/login-action@v3
  #       with:
  #         registry: ${{ secrets.ACR_REGISTRY }}
  #         username: ${{ secrets.ACR_USERNAME }}
  #         password: ${{ secrets.ACR_PASSWORD }}
  #
  #     - name: Mirror NGINX Ingress Controller image
  #       shell: bash
  #       run: |
  #         SOURCE_IMAGE="registry.k8s.io/ingress-nginx/controller:v1.9.4"
  #         TARGET_IMAGE="${{ secrets.ACR_REGISTRY }}/ingress-nginx/controller:v1.9.4"
  #         docker pull "$SOURCE_IMAGE"
  #         docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
  #         docker push "$TARGET_IMAGE"
  #         echo "‚úÖ Mirrored: $SOURCE_IMAGE ‚Üí $TARGET_IMAGE"
  #
  #     - name: Mirror Prometheus image
  #       shell: bash
  #       run: |
  #         SOURCE_IMAGE="quay.io/prometheus/prometheus:v2.48.0"
  #         TARGET_IMAGE="${{ secrets.ACR_REGISTRY }}/prometheus/prometheus:v2.48.0"
  #         docker pull "$SOURCE_IMAGE"
  #         docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
  #         docker push "$TARGET_IMAGE"
  #         echo "‚úÖ Mirrored: $SOURCE_IMAGE ‚Üí $TARGET_IMAGE"
  #
  #     - name: Mirror Grafana image
  #       shell: bash
  #       run: |
  #         SOURCE_IMAGE="docker.io/grafana/grafana:10.2.0"
  #         TARGET_IMAGE="${{ secrets.ACR_REGISTRY }}/grafana/grafana:10.2.0"
  #         docker pull "$SOURCE_IMAGE"
  #         docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
  #         docker push "$TARGET_IMAGE"
  #         echo "‚úÖ Mirrored: $SOURCE_IMAGE ‚Üí $TARGET_IMAGE"
  #
  #     - name: Mirror Alertmanager image
  #       shell: bash
  #       run: |
  #         SOURCE_IMAGE="quay.io/prometheus/alertmanager:v0.26.0"
  #         TARGET_IMAGE="${{ secrets.ACR_REGISTRY }}/prometheus/alertmanager:v0.26.0"
  #         docker pull "$SOURCE_IMAGE"
  #         docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
  #         docker push "$TARGET_IMAGE"
  #         echo "‚úÖ Mirrored: $SOURCE_IMAGE ‚Üí $TARGET_IMAGE"
  #
  #     - name: Mirror Metrics Server image
  #       shell: bash
  #       run: |
  #         SOURCE_IMAGE="registry.k8s.io/metrics-server/metrics-server:v0.6.4"
  #         TARGET_IMAGE="${{ secrets.ACR_REGISTRY }}/metrics-server/metrics-server:v0.6.4"
  #         docker pull "$SOURCE_IMAGE"
  #         docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
  #         docker push "$TARGET_IMAGE"
  #         echo "‚úÖ Mirrored: $SOURCE_IMAGE ‚Üí $TARGET_IMAGE"
  #
  #     - name: Mirror Local Path Provisioner image
  #       shell: bash
  #       run: |
  #         SOURCE_IMAGE="rancher/local-path-provisioner:v0.0.26"
  #         TARGET_IMAGE="${{ secrets.ACR_REGISTRY }}/rancher/local-path-provisioner:v0.0.26"
  #         docker pull "$SOURCE_IMAGE"
  #         docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
  #         docker push "$TARGET_IMAGE"
  #         echo "‚úÖ Mirrored: $SOURCE_IMAGE ‚Üí $TARGET_IMAGE"

  deploy-infrastructure:
    name: Deploy Base Infrastructure
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: üîê Setup Admin kubeconfig
        shell: bash
        run: |
          echo "Using GitHub Secret kubeconfig..."
          mkdir -p ~/.kube
          
          SECRET_VALUE="${{ secrets.KUBECONFIG }}"
          
          # Try base64 decode (might be double-encoded)
          DECODED=$(printf '%s' "$SECRET_VALUE" | base64 -d 2>/dev/null)
          DECODE_EXIT=$?
          
          if [ $DECODE_EXIT -eq 0 ] && [ -n "$DECODED" ]; then
            # Check if decoded content is still base64 (starts with base64-like pattern)
            # Base64 often starts with letters/numbers, but kubeconfig starts with "apiVersion:"
            if printf '%s' "$DECODED" | head -c 20 | grep -qE "^[A-Za-z0-9+/=]+$" 2>/dev/null && \
               ! printf '%s' "$DECODED" | head -n 1 | grep -qE "^(apiVersion|kind):" 2>/dev/null; then
              echo "‚ö†Ô∏è Decoded content looks like base64, trying second decode..."
              DOUBLE_DECODED=$(printf '%s' "$DECODED" | base64 -d 2>/dev/null)
              if [ $? -eq 0 ] && [ -n "$DOUBLE_DECODED" ] && \
                 printf '%s' "$DOUBLE_DECODED" | head -n 1 | grep -qE "^(apiVersion|kind):" 2>/dev/null; then
                echo "‚úÖ Detected double base64-encoded kubeconfig"
                KUBECONFIG_CONTENT="$DOUBLE_DECODED"
              else
                echo "‚ö†Ô∏è Second decode failed or invalid, using first decode..."
                KUBECONFIG_CONTENT="$DECODED"
              fi
            elif printf '%s' "$DECODED" | head -n 1 | grep -qE "^(apiVersion|kind):" 2>/dev/null; then
              echo "‚úÖ Detected base64-encoded kubeconfig"
              KUBECONFIG_CONTENT="$DECODED"
            else
              echo "‚ö†Ô∏è Base64 decode succeeded but doesn't look like kubeconfig, trying plain text..."
              KUBECONFIG_CONTENT="$SECRET_VALUE"
            fi
          else
            echo "‚úÖ Using plain text kubeconfig (base64 decode failed)"
            KUBECONFIG_CONTENT="$SECRET_VALUE"
          fi
          
          # Write kubeconfig using printf to preserve content exactly
          printf '%s' "$KUBECONFIG_CONTENT" > /tmp/kubeconfig-admin
          
          # Validate by trying to read it with kubectl
          export KUBECONFIG=/tmp/kubeconfig-admin
          if kubectl config view --raw >/dev/null 2>&1; then
            echo "‚úÖ Kubeconfig is valid"
            mv /tmp/kubeconfig-admin ~/.kube/config-admin
            chmod 600 ~/.kube/config-admin
            export KUBECONFIG=~/.kube/config-admin
            echo "KUBECONFIG set to: $KUBECONFIG"
            kubectl config current-context
            echo "‚úÖ Admin kubeconfig ready"
          else
            echo "‚ùå Invalid kubeconfig - kubectl cannot read it"
            echo "File size: $(wc -c < /tmp/kubeconfig-admin) bytes"
            echo "First 10 characters (hex): $(head -c 10 /tmp/kubeconfig-admin | od -An -tx1)"
            echo "First 10 characters (ASCII): $(head -c 10 /tmp/kubeconfig-admin | tr -d '\0' | cat -A)"
            echo "Checking for common patterns:"
            grep -c "apiVersion" /tmp/kubeconfig-admin 2>/dev/null || echo "No 'apiVersion' found"
            grep -c "kind" /tmp/kubeconfig-admin 2>/dev/null || echo "No 'kind' found"
            exit 1
          fi

      - name: üîß Setup CI/CD ServiceAccount and kubeconfig
        shell: bash
        run: |
          export KUBECONFIG=~/.kube/config-admin
          
          # Apply RBAC manifests
          echo "Creating CI/CD ServiceAccount and RBAC..."
          kubectl apply -f - <<EOF
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: cicd-deploy
            namespace: kube-system
          ---
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          metadata:
            name: cicd-deploy-role
          rules:
            # Namespaces verwalten
            - apiGroups: [""]
              resources: ["namespaces"]
              verbs: ["create", "get", "list", "watch"]
            # Deployments, Services, ConfigMaps, Secrets
            - apiGroups: ["apps"]
              resources: ["deployments", "daemonsets", "statefulsets"]
              verbs: ["create", "update", "patch", "delete", "get", "list", "watch"]
            - apiGroups: [""]
              resources: ["services", "configmaps", "secrets", "pods", "endpoints"]
              verbs: ["create", "update", "patch", "delete", "get", "list", "watch"]
            # ServiceAccounts
            - apiGroups: [""]
              resources: ["serviceaccounts"]
              verbs: ["create", "update", "patch", "get", "list", "watch"]
            # F√ºr Helm (falls verwendet)
            - apiGroups: ["helm.cattle.io"]
              resources: ["helmchartconfigs", "helmcharts"]
              verbs: ["*"]
            # F√ºr Verifikation
            - apiGroups: [""]
              resources: ["nodes"]
              verbs: ["get", "list"]
            # F√ºr Metrics Server
            - apiGroups: ["metrics.k8s.io"]
              resources: ["*"]
              verbs: ["get", "list"]
            # StorageClasses
            - apiGroups: ["storage.k8s.io"]
              resources: ["storageclasses"]
              verbs: ["get", "list", "watch", "patch"]
          ---
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: cicd-deploy-binding
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: cicd-deploy-role
          subjects:
            - kind: ServiceAccount
              name: cicd-deploy
              namespace: kube-system
          EOF
          
          # Wait for token secret to be created
          echo "Waiting for ServiceAccount token..."
          sleep 5
          
          # Get token secret name
          SECRET_NAME=$(kubectl get serviceaccount cicd-deploy -n kube-system -o jsonpath='{.secrets[0].name}' || echo "")
          if [ -z "$SECRET_NAME" ]; then
            echo "‚ö†Ô∏è Token secret not found, creating manually..."
            kubectl apply -f - <<EOF
          apiVersion: v1
          kind: Secret
          metadata:
            name: cicd-deploy-token
            namespace: kube-system
            annotations:
              kubernetes.io/service-account.name: cicd-deploy
          type: kubernetes.io/service-account-token
          EOF
            sleep 3
            SECRET_NAME="cicd-deploy-token"
          fi
          
          # Get token and CA cert
          TOKEN=$(kubectl get secret $SECRET_NAME -n kube-system -o jsonpath='{.data.token}' | base64 -d)
          CA_CERT=$(kubectl get secret $SECRET_NAME -n kube-system -o jsonpath='{.data.ca\.crt}')
          CLUSTER_ENDPOINT=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')
          
          # Generate CI/CD kubeconfig
          echo "Generating CI/CD kubeconfig..."
          cat > ~/.kube/config <<EOF
          apiVersion: v1
          kind: Config
          clusters:
          - name: cluster
            cluster:
              server: $CLUSTER_ENDPOINT
              certificate-authority-data: $CA_CERT
          contexts:
          - name: cicd-deploy
            context:
              cluster: cluster
              user: cicd-deploy
              namespace: kube-system
          current-context: cicd-deploy
          users:
          - name: cicd-deploy
            user:
              token: $TOKEN
          EOF
          
          chmod 600 ~/.kube/config
          
          # Verify new kubeconfig
          export KUBECONFIG=~/.kube/config
          kubectl config current-context
          kubectl get nodes
          echo "‚úÖ CI/CD kubeconfig created and verified"
          
          # Set KUBECONFIG for all subsequent steps
          echo "KUBECONFIG=$HOME/.kube/config" >> $GITHUB_ENV

      - name: Verify cluster access
        shell: bash
        run: |
          # KUBECONFIG is set via GITHUB_ENV from previous step
          export KUBECONFIG="${KUBECONFIG:-$HOME/.kube/config}"
          # Test cluster access with CI/CD ServiceAccount
          kubectl cluster-info
          kubectl get nodes
          kubectl get namespaces
          echo "‚úÖ Cluster access verified with CI/CD ServiceAccount"

      - name: Check helm installation
        shell: bash
        run: |
          if ! command -v helm >/dev/null 2>&1; then
            echo "‚ùå Helm is not installed. Please install it on the runner server:"
            echo "   curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash"
            exit 1
          fi
          helm version

      - name: Create ACR imagePullSecret for ingress-nginx namespace
        shell: bash
        run: |
          kubectl get ns ingress-nginx >/dev/null 2>&1 || kubectl create namespace ingress-nginx
          kubectl create secret docker-registry acr-pull \
            --docker-server="${{ secrets.ACR_REGISTRY }}" \
            --docker-username="${{ secrets.ACR_USERNAME }}" \
            --docker-password="${{ secrets.ACR_PASSWORD }}" \
            -n ingress-nginx \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl patch serviceaccount default -n ingress-nginx \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}'

      - name: Create ACR imagePullSecret for monitoring namespace
        shell: bash
        run: |
          kubectl get ns monitoring >/dev/null 2>&1 || kubectl create namespace monitoring
          kubectl create secret docker-registry acr-pull \
            --docker-server="${{ secrets.ACR_REGISTRY }}" \
            --docker-username="${{ secrets.ACR_USERNAME }}" \
            --docker-password="${{ secrets.ACR_PASSWORD }}" \
            -n monitoring \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl patch serviceaccount default -n monitoring \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}'

      - name: Create ACR imagePullSecret for kube-system namespace
        shell: bash
        run: |
          kubectl create secret docker-registry acr-pull \
            --docker-server="${{ secrets.ACR_REGISTRY }}" \
            --docker-username="${{ secrets.ACR_USERNAME }}" \
            --docker-password="${{ secrets.ACR_PASSWORD }}" \
            -n kube-system \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl patch serviceaccount default -n kube-system \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}'

      - name: Create ACR imagePullSecret for local-path-storage namespace
        shell: bash
        run: |
          kubectl get ns local-path-storage >/dev/null 2>&1 || kubectl create namespace local-path-storage
          kubectl create secret docker-registry acr-pull \
            --docker-server="${{ secrets.ACR_REGISTRY }}" \
            --docker-username="${{ secrets.ACR_USERNAME }}" \
            --docker-password="${{ secrets.ACR_PASSWORD }}" \
            -n local-path-storage \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Add Helm repositories
        shell: bash
        run: |
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add grafana https://grafana.github.io/helm-charts
          helm repo update

      - name: Deploy NGINX Ingress Controller
        shell: bash
        run: |
          # KUBECONFIG is set via GITHUB_ENV from Prepare kubeconfig step
          export KUBECONFIG="${KUBECONFIG:-$HOME/.kube/config}"
          kubectl get ns ingress-nginx >/dev/null 2>&1 || kubectl create namespace ingress-nginx
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            -n ingress-nginx \
            -f infra/addons/values/ingress-nginx.yaml \
            --set controller.image.registry="${{ secrets.ACR_REGISTRY }}" \
            --wait --timeout 5m

      - name: Deploy Monitoring Stack (Prometheus + Grafana)
        shell: bash
        run: |
          kubectl get ns monitoring >/dev/null 2>&1 || kubectl create namespace monitoring
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            -n monitoring \
            -f infra/addons/values/observability.yaml \
            --set grafana.image.registry="${{ secrets.ACR_REGISTRY }}" \
            --set prometheus.prometheusSpec.image.registry="${{ secrets.ACR_REGISTRY }}" \
            --set alertmanager.image.registry="${{ secrets.ACR_REGISTRY }}" \
            --wait --timeout 10m

      - name: Deploy Metrics Server
        shell: bash
        run: |
          if ! kubectl -n kube-system get deploy metrics-server >/dev/null 2>&1; then
            # Download manifest and replace image URL with ACR
            curl -sL https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml | \
              sed "s|registry.k8s.io/metrics-server/metrics-server|${{ secrets.ACR_REGISTRY }}/metrics-server/metrics-server|g" | \
              kubectl apply -f -
            kubectl wait --for=condition=available --timeout=5m deployment/metrics-server -n kube-system
          else
            echo "Metrics Server already deployed"
          fi

      - name: Deploy Local Path Provisioner
        shell: bash
        run: |
          if ! kubectl -n local-path-storage get deploy local-path-provisioner >/dev/null 2>&1; then
            # Download manifest and replace image URL with ACR
            curl -sL https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml | \
              sed "s|rancher/local-path-provisioner:v0.0.26|${{ secrets.ACR_REGISTRY }}/rancher/local-path-provisioner:v0.0.26|g" | \
              kubectl apply -f -
            # Patch ServiceAccount to use imagePullSecret (after it's created)
            sleep 2
            kubectl patch serviceaccount local-path-provisioner-service-account -n local-path-storage \
              -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || echo "ServiceAccount patched or not found yet"
            kubectl wait --for=condition=available --timeout=5m deployment/local-path-provisioner -n local-path-storage
            # Set as default storage class if not already set
            kubectl patch storageclass local-path -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' || echo "StorageClass already configured"
          else
            echo "Local Path Provisioner already deployed"
            # Ensure imagePullSecret is set even if already deployed
            kubectl patch serviceaccount local-path-provisioner-service-account -n local-path-storage \
              -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || echo "ServiceAccount already patched"
          fi

      - name: Verify deployments
        shell: bash
        run: |
          echo "=== Ingress Controller ==="
          kubectl get pods -n ingress-nginx
          echo ""
          echo "=== Monitoring Stack ==="
          kubectl get pods -n monitoring
          echo ""
          echo "=== Metrics Server ==="
          kubectl get pods -n kube-system | grep metrics-server
          echo ""
          echo "=== Local Path Provisioner ==="
          kubectl get pods -n local-path-storage
          echo ""
          echo "=== Storage Classes ==="
          kubectl get storageclass
          echo ""
          echo "=== Cluster Status ==="
          kubectl top nodes || echo "Metrics not yet available (may take a few minutes)"

      - name: Summary
        shell: bash
        run: |
          echo "‚úÖ Cluster Infrastructure deployed successfully!"
          echo "  - NGINX Ingress Controller: ingress-nginx namespace"
          echo "  - Prometheus + Grafana: monitoring namespace"
          echo "  - Metrics Server: kube-system namespace"
          echo "  - Local Path Provisioner: local-path-storage namespace"
          echo "  - All images pulled from ACR: ${{ secrets.ACR_REGISTRY }}"


