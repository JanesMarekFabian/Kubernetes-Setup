name: Deploy Cluster Infrastructure

on:
  workflow_dispatch:
  push:
    branches: [ main ]
    paths:
      - 'infra/addons/**'
      - '.github/workflows/deploy-addons.yml'

jobs:
  mirror-images:
    name: Mirror Images to ACR
    runs-on: ubuntu-latest
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Login to ACR
        uses: docker/login-action@v3
        with:
          registry: ${{ secrets.ACR_REGISTRY }}
          username: ${{ secrets.ACR_USERNAME }}
          password: ${{ secrets.ACR_PASSWORD }}

      - name: Mirror NGINX Ingress Controller image
        shell: bash
        run: |
          SOURCE_IMAGE="registry.k8s.io/ingress-nginx/controller:v1.9.4"
          TARGET_IMAGE="${{ secrets.ACR_REGISTRY }}/ingress-nginx/controller:v1.9.4"
          docker pull "$SOURCE_IMAGE"
          docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
          docker push "$TARGET_IMAGE"
          echo "‚úÖ Mirrored: $SOURCE_IMAGE ‚Üí $TARGET_IMAGE"

      - name: Mirror Prometheus image
        shell: bash
        run: |
          SOURCE_IMAGE="quay.io/prometheus/prometheus:v2.48.0"
          TARGET_IMAGE="${{ secrets.ACR_REGISTRY }}/prometheus/prometheus:v2.48.0"
          docker pull "$SOURCE_IMAGE"
          docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
          docker push "$TARGET_IMAGE"
          echo "‚úÖ Mirrored: $SOURCE_IMAGE ‚Üí $TARGET_IMAGE"

      - name: Mirror Grafana image
        shell: bash
        run: |
          SOURCE_IMAGE="docker.io/grafana/grafana:10.2.0"
          TARGET_IMAGE="${{ secrets.ACR_REGISTRY }}/grafana/grafana:10.2.0"
          docker pull "$SOURCE_IMAGE"
          docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
          docker push "$TARGET_IMAGE"
          echo "‚úÖ Mirrored: $SOURCE_IMAGE ‚Üí $TARGET_IMAGE"

      - name: Mirror Alertmanager image
        shell: bash
        run: |
          SOURCE_IMAGE="quay.io/prometheus/alertmanager:v0.26.0"
          TARGET_IMAGE="${{ secrets.ACR_REGISTRY }}/prometheus/alertmanager:v0.26.0"
          docker pull "$SOURCE_IMAGE"
          docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
          docker push "$TARGET_IMAGE"
          echo "‚úÖ Mirrored: $SOURCE_IMAGE ‚Üí $TARGET_IMAGE"

      - name: Mirror Metrics Server image
        shell: bash
        run: |
          SOURCE_IMAGE="registry.k8s.io/metrics-server/metrics-server:v0.6.4"
          TARGET_IMAGE="${{ secrets.ACR_REGISTRY }}/metrics-server/metrics-server:v0.6.4"
          docker pull "$SOURCE_IMAGE"
          docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
          docker push "$TARGET_IMAGE"
          echo "‚úÖ Mirrored: $SOURCE_IMAGE ‚Üí $TARGET_IMAGE"

      - name: Mirror Local Path Provisioner image
        shell: bash
        run: |
          SOURCE_IMAGE="rancher/local-path-provisioner:v0.0.26"
          TARGET_IMAGE="${{ secrets.ACR_REGISTRY }}/rancher/local-path-provisioner:v0.0.26"
          docker pull "$SOURCE_IMAGE"
          docker tag "$SOURCE_IMAGE" "$TARGET_IMAGE"
          docker push "$TARGET_IMAGE"
          echo "‚úÖ Mirrored: $SOURCE_IMAGE ‚Üí $TARGET_IMAGE"

  deploy-infrastructure:
    name: Deploy Base Infrastructure
    needs: mirror-images
    runs-on: self-hosted
    env:
      ACR_REGISTRY: ${{ secrets.ACR_REGISTRY }}
      ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
      ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: üîê Setup Admin kubeconfig
        shell: bash
        run: |
          echo "Using GitHub Secret kubeconfig..."
          mkdir -p ~/.kube
          
          SECRET_VALUE="${{ secrets.KUBECONFIG }}"
          
          # First check if secret itself is already a valid kubeconfig (plain text)
          if printf '%s' "$SECRET_VALUE" | head -n 1 | grep -qE "^(apiVersion|kind):" 2>/dev/null; then
            echo "‚úÖ Secret is already a plain text kubeconfig"
            KUBECONFIG_CONTENT="$SECRET_VALUE"
          else
            # Try base64 decode (might be double-encoded)
            DECODED=$(printf '%s' "$SECRET_VALUE" | base64 -d 2>/dev/null)
            DECODE_EXIT=$?
            
            if [ $DECODE_EXIT -eq 0 ] && [ -n "$DECODED" ]; then
              # Check if decoded content is a valid kubeconfig
              if printf '%s' "$DECODED" | head -n 1 | grep -qE "^(apiVersion|kind):" 2>/dev/null; then
                echo "‚úÖ Detected base64-encoded kubeconfig"
                KUBECONFIG_CONTENT="$DECODED"
              else
                # First decode succeeded but doesn't look like kubeconfig
                echo "‚ö†Ô∏è First decode succeeded but doesn't look like kubeconfig"
                echo "   First 20 chars of decoded: $(printf '%s' "$DECODED" | head -c 20)"
                
                # Check if it looks like a certificate (might be part of kubeconfig)
                if printf '%s' "$DECODED" | head -c 15 | grep -qE "^-----BEGIN"; then
                  echo "   Decoded content looks like a certificate, not a kubeconfig"
                  echo "   This suggests the secret might not be the full kubeconfig"
                  echo "   Trying plain text (secret might be plain text kubeconfig)..."
                  KUBECONFIG_CONTENT="$SECRET_VALUE"
                else
                  # Try second decode
                  echo "   Trying second decode..."
                  DOUBLE_DECODED=$(printf '%s' "$DECODED" | base64 -d 2>/dev/null)
                  DOUBLE_DECODE_EXIT=$?
                  
                  if [ $DOUBLE_DECODE_EXIT -eq 0 ] && [ -n "$DOUBLE_DECODED" ]; then
                    echo "   Second decode successful, checking if it's a kubeconfig..."
                    if printf '%s' "$DOUBLE_DECODED" | head -n 1 | grep -qE "^(apiVersion|kind):" 2>/dev/null; then
                      echo "‚úÖ Detected double base64-encoded kubeconfig"
                      KUBECONFIG_CONTENT="$DOUBLE_DECODED"
                    else
                      echo "‚ö†Ô∏è Second decode succeeded but doesn't look like kubeconfig"
                      echo "   First 20 chars: $(printf '%s' "$DOUBLE_DECODED" | head -c 20)"
                      echo "   Trying plain text..."
                      KUBECONFIG_CONTENT="$SECRET_VALUE"
                    fi
                  else
                    echo "‚ö†Ô∏è Second decode failed (exit code: $DOUBLE_DECODE_EXIT), trying plain text..."
                    KUBECONFIG_CONTENT="$SECRET_VALUE"
                  fi
                fi
              fi
            else
              echo "‚úÖ Using plain text kubeconfig (base64 decode failed)"
              KUBECONFIG_CONTENT="$SECRET_VALUE"
            fi
          fi
          
          # Write kubeconfig using printf to preserve content exactly
          printf '%s' "$KUBECONFIG_CONTENT" > /tmp/kubeconfig-admin
          
          # Validate by trying to read it with kubectl
          export KUBECONFIG=/tmp/kubeconfig-admin
          if kubectl config view --raw >/dev/null 2>&1; then
            echo "‚úÖ Kubeconfig is valid"
            mv /tmp/kubeconfig-admin ~/.kube/config-admin
            chmod 600 ~/.kube/config-admin
            export KUBECONFIG=~/.kube/config-admin
            echo "KUBECONFIG set to: $KUBECONFIG"
            kubectl config current-context
            echo "‚úÖ Admin kubeconfig ready"
          else
            echo "‚ùå Invalid kubeconfig - kubectl cannot read it"
            echo "File size: $(wc -c < /tmp/kubeconfig-admin) bytes"
            echo "First 10 characters (hex): $(head -c 10 /tmp/kubeconfig-admin | od -An -tx1)"
            echo "First 10 characters (ASCII): $(head -c 10 /tmp/kubeconfig-admin | tr -d '\0' | cat -A)"
            echo "First 50 characters: $(head -c 50 /tmp/kubeconfig-admin)"
            echo "Checking for common patterns:"
            grep -c "apiVersion" /tmp/kubeconfig-admin 2>/dev/null || echo "No 'apiVersion' found"
            grep -c "kind" /tmp/kubeconfig-admin 2>/dev/null || echo "No 'kind' found"
            exit 1
          fi

      - name: üîß Setup CI/CD ServiceAccount and kubeconfig
        shell: bash
        run: |
          export KUBECONFIG=~/.kube/config-admin
          
          # Apply RBAC manifests
          echo "Creating CI/CD ServiceAccount and RBAC..."
          kubectl apply -f - <<EOF
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: cicd-deploy
            namespace: kube-system
          ---
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          metadata:
            name: cicd-deploy-role
          rules:
            # Namespaces verwalten
            - apiGroups: [""]
              resources: ["namespaces"]
              verbs: ["create", "get", "list", "watch"]
            # Deployments, Services, ConfigMaps, Secrets
            - apiGroups: ["apps"]
              resources: ["deployments", "daemonsets", "statefulsets"]
              verbs: ["create", "update", "patch", "delete", "get", "list", "watch"]
            # Jobs (f√ºr Helm Hooks)
            - apiGroups: ["batch"]
              resources: ["jobs"]
              verbs: ["create", "update", "patch", "delete", "get", "list", "watch"]
            - apiGroups: [""]
              resources: ["services", "configmaps", "secrets", "pods", "endpoints"]
              verbs: ["create", "update", "patch", "delete", "get", "list", "watch"]
            # ServiceAccounts
            - apiGroups: [""]
              resources: ["serviceaccounts"]
              verbs: ["create", "update", "patch", "delete", "get", "list", "watch"]
            # RBAC Ressourcen (f√ºr Helm und allgemeine Pr√ºfungen)
            - apiGroups: ["rbac.authorization.k8s.io"]
              resources: ["clusterroles", "clusterrolebindings", "roles", "rolebindings"]
              verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
            # F√ºr Helm (falls verwendet)
            - apiGroups: ["helm.cattle.io"]
              resources: ["helmchartconfigs", "helmcharts"]
              verbs: ["*"]
            # F√ºr Verifikation
            - apiGroups: [""]
              resources: ["nodes"]
              verbs: ["get", "list", "watch"]
            # F√ºr Metrics Server
            - apiGroups: ["metrics.k8s.io"]
              resources: ["*"]
              verbs: ["get", "list"]
            # Coordination (f√ºr Leases, z.B. Leader Election)
            - apiGroups: ["coordination.k8s.io"]
              resources: ["leases"]
              verbs: ["get", "list", "watch", "create", "update", "patch"]
            # Discovery (f√ºr EndpointSlices)
            - apiGroups: ["discovery.k8s.io"]
              resources: ["endpointslices"]
              verbs: ["get", "list", "watch"]
            # StorageClasses
            - apiGroups: ["storage.k8s.io"]
              resources: ["storageclasses"]
              verbs: ["get", "list", "watch", "patch"]
            # Helm Releases und Secrets (f√ºr Helm 3)
            - apiGroups: [""]
              resources: ["secrets"]
              verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
            # CustomResourceDefinitions (f√ºr Helm Charts mit CRDs)
            - apiGroups: ["apiextensions.k8s.io"]
              resources: ["customresourcedefinitions"]
              verbs: ["get", "list", "watch", "create", "update", "patch"]
            # Netzwerk-Ressourcen (f√ºr Ingress Controller, etc.)
            - apiGroups: ["networking.k8s.io"]
              resources: ["ingresses", "ingressclasses", "networkpolicies"]
              verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
            # Ingress Status (Subresource f√ºr Status-Updates)
            - apiGroups: ["networking.k8s.io"]
              resources: ["ingresses/status"]
              verbs: ["update"]
            # PersistentVolumes und PersistentVolumeClaims (f√ºr Storage)
            - apiGroups: [""]
              resources: ["persistentvolumes", "persistentvolumeclaims"]
              verbs: ["get", "list", "watch", "create", "update", "patch"]
            # Events (f√ºr Debugging und Logging)
            - apiGroups: [""]
              resources: ["events"]
              verbs: ["get", "list", "watch", "create", "patch"]
            # Admission Registration (f√ºr Webhooks, z.B. NGINX Ingress)
            - apiGroups: ["admissionregistration.k8s.io"]
              resources: ["validatingwebhookconfigurations", "mutatingwebhookconfigurations"]
              verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
          ---
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: cicd-deploy-binding
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: cicd-deploy-role
          subjects:
            - kind: ServiceAccount
              name: cicd-deploy
              namespace: kube-system
          EOF
          
          # Wait for token secret to be created
          echo "Waiting for ServiceAccount token..."
          sleep 5
          
          # Get token secret name
          SECRET_NAME=$(kubectl get serviceaccount cicd-deploy -n kube-system -o jsonpath='{.secrets[0].name}' || echo "")
          if [ -z "$SECRET_NAME" ]; then
            echo "‚ö†Ô∏è Token secret not found, creating manually..."
            kubectl apply -f - <<EOF
          apiVersion: v1
          kind: Secret
          metadata:
            name: cicd-deploy-token
            namespace: kube-system
            annotations:
              kubernetes.io/service-account.name: cicd-deploy
          type: kubernetes.io/service-account-token
          EOF
            sleep 3
            SECRET_NAME="cicd-deploy-token"
          fi
          
          # Get token and CA cert
          TOKEN=$(kubectl get secret $SECRET_NAME -n kube-system -o jsonpath='{.data.token}' | base64 -d)
          CA_CERT=$(kubectl get secret $SECRET_NAME -n kube-system -o jsonpath='{.data.ca\.crt}')
          CLUSTER_ENDPOINT=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')
          
          # Generate CI/CD kubeconfig
          echo "Generating CI/CD kubeconfig..."
          
          # Ensure .kube directory exists with correct permissions
          mkdir -p ~/.kube
          chmod 700 ~/.kube
          
          # Remove old config if it exists and has wrong permissions
          if [ -f ~/.kube/config ] && [ ! -w ~/.kube/config ]; then
            echo "‚ö†Ô∏è Removing old config file with wrong permissions..."
            rm -f ~/.kube/config
          fi
          
          # Write kubeconfig to temp file first, then move it
          cat > /tmp/kubeconfig-cicd <<EOF
          apiVersion: v1
          kind: Config
          clusters:
          - name: cluster
            cluster:
              server: $CLUSTER_ENDPOINT
              certificate-authority-data: $CA_CERT
          contexts:
          - name: cicd-deploy
            context:
              cluster: cluster
              user: cicd-deploy
              namespace: kube-system
          current-context: cicd-deploy
          users:
          - name: cicd-deploy
            user:
              token: $TOKEN
          EOF
          
          # Move to final location and set permissions
          mv /tmp/kubeconfig-cicd ~/.kube/config
          chmod 600 ~/.kube/config
          
          # Verify new kubeconfig
          export KUBECONFIG=~/.kube/config
          kubectl config current-context
          kubectl get nodes
          echo "‚úÖ CI/CD kubeconfig created and verified"
          
          # Set KUBECONFIG for all subsequent steps
          echo "KUBECONFIG=$HOME/.kube/config" >> $GITHUB_ENV

      - name: Verify cluster access
        shell: bash
        run: |
          # KUBECONFIG is set via GITHUB_ENV from previous step
          export KUBECONFIG="${KUBECONFIG:-$HOME/.kube/config}"
          # Test cluster access with CI/CD ServiceAccount
          kubectl cluster-info
          kubectl get nodes
          kubectl get namespaces
          echo "‚úÖ Cluster access verified with CI/CD ServiceAccount"

      - name: Check helm installation
        shell: bash
        run: |
          if ! command -v helm >/dev/null 2>&1; then
            echo "‚ùå Helm is not installed. Please install it on the runner server:"
            echo "   curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash"
            exit 1
          fi
          helm version

      - name: Create ACR imagePullSecret for ingress-nginx namespace
        shell: bash
        run: |
          kubectl get ns ingress-nginx >/dev/null 2>&1 || kubectl create namespace ingress-nginx
          kubectl create secret docker-registry acr-pull \
            --docker-server="${{ secrets.ACR_REGISTRY }}" \
            --docker-username="${{ secrets.ACR_USERNAME }}" \
            --docker-password="${{ secrets.ACR_PASSWORD }}" \
            -n ingress-nginx \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl patch serviceaccount default -n ingress-nginx \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}'

      - name: Create ACR imagePullSecret for monitoring namespace
        shell: bash
        run: |
          kubectl get ns monitoring >/dev/null 2>&1 || kubectl create namespace monitoring
          kubectl create secret docker-registry acr-pull \
            --docker-server="${{ secrets.ACR_REGISTRY }}" \
            --docker-username="${{ secrets.ACR_USERNAME }}" \
            --docker-password="${{ secrets.ACR_PASSWORD }}" \
            -n monitoring \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl patch serviceaccount default -n monitoring \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}'

      - name: Create ACR imagePullSecret for kube-system namespace
        shell: bash
        run: |
          kubectl create secret docker-registry acr-pull \
            --docker-server="${{ secrets.ACR_REGISTRY }}" \
            --docker-username="${{ secrets.ACR_USERNAME }}" \
            --docker-password="${{ secrets.ACR_PASSWORD }}" \
            -n kube-system \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl patch serviceaccount default -n kube-system \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}'

      - name: Create ACR imagePullSecret for local-path-storage namespace
        shell: bash
        run: |
          kubectl get ns local-path-storage >/dev/null 2>&1 || kubectl create namespace local-path-storage
          kubectl create secret docker-registry acr-pull \
            --docker-server="${{ secrets.ACR_REGISTRY }}" \
            --docker-username="${{ secrets.ACR_USERNAME }}" \
            --docker-password="${{ secrets.ACR_PASSWORD }}" \
            -n local-path-storage \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Add Helm repositories
        shell: bash
        run: |
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add grafana https://grafana.github.io/helm-charts
          helm repo update

      - name: Clean up stuck Helm releases
        shell: bash
        run: |
          # Function to check and clean up stuck releases
          cleanup_stuck_release() {
            local namespace=$1
            local release_name=$2
            
            echo "Checking release $release_name in namespace $namespace..."
            
            # Check if release exists
            if ! helm list -n "$namespace" 2>/dev/null | grep -q "$release_name"; then
              echo "‚ÑπÔ∏è Release $release_name not found in namespace $namespace"
              return 0
            fi
            
            # Try to get status - if this fails, release might be stuck
            if ! helm status "$release_name" -n "$namespace" >/dev/null 2>&1; then
              echo "‚ö†Ô∏è Cannot get status of $release_name, might be stuck. Attempting cleanup..."
              helm uninstall "$release_name" -n "$namespace" --ignore-not-found || true
              sleep 3
            fi
            
            # Get release status
            RELEASE_STATUS=$(helm status "$release_name" -n "$namespace" -o json 2>/dev/null | grep -o '"status":"[^"]*"' | cut -d'"' -f4 || echo "unknown")
            echo "Release $release_name status: $RELEASE_STATUS"
            
            # Check for stuck states or if status check failed
            if [ "$RELEASE_STATUS" = "pending-install" ] || [ "$RELEASE_STATUS" = "pending-upgrade" ] || [ "$RELEASE_STATUS" = "pending-rollback" ] || [ "$RELEASE_STATUS" = "unknown" ]; then
              echo "‚ö†Ô∏è Release $release_name is stuck or in problematic state: $RELEASE_STATUS"
              
              # Try rollback first
              echo "Attempting rollback..."
              if helm rollback "$release_name" -n "$namespace" 2>/dev/null; then
                echo "‚úÖ Rollback successful, waiting..."
                sleep 5
              else
                echo "‚ö†Ô∏è Rollback failed, attempting force cleanup..."
                
                # Force uninstall
                helm uninstall "$release_name" -n "$namespace" --ignore-not-found || true
                sleep 3
                
                # Delete Helm release secrets (this removes the lock)
                echo "Deleting Helm release secrets to remove lock..."
                kubectl delete secret -n "$namespace" -l "owner=helm,name=$release_name" --ignore-not-found || true
                kubectl get secret -n "$namespace" 2>/dev/null | grep "sh.helm.release.v1.$release_name" | awk '{print $1}' | xargs -r kubectl delete secret -n "$namespace" --ignore-not-found || true
                sleep 2
              fi
            else
              echo "‚úÖ Release $release_name status: $RELEASE_STATUS (OK)"
            fi
          }
          
          # Clean up NGINX Ingress Controller
          cleanup_stuck_release "ingress-nginx" "ingress-nginx"
          
          # Clean up Monitoring Stack (if exists)
          cleanup_stuck_release "monitoring" "kube-prometheus-stack"
          
          # Additional check: try to force unlock if Helm lock exists
          echo "Checking for Helm locks..."
          if kubectl get secret -n ingress-nginx 2>/dev/null | grep -q "sh.helm.release"; then
            echo "‚ÑπÔ∏è Helm release secrets found (normal)"
          fi

      - name: Prepare NGINX namespace and ServiceAccount
        shell: bash
        run: |
          # KUBECONFIG is set via GITHUB_ENV from Prepare kubeconfig step
          export KUBECONFIG="${KUBECONFIG:-$HOME/.kube/config}"
          kubectl get ns ingress-nginx >/dev/null 2>&1 || kubectl create namespace ingress-nginx
          
          # Ensure imagePullSecret exists
          kubectl create secret docker-registry acr-pull \
            --docker-server="${{ secrets.ACR_REGISTRY }}" \
            --docker-username="${{ secrets.ACR_USERNAME }}" \
            --docker-password="${{ secrets.ACR_PASSWORD }}" \
            -n ingress-nginx \
            --dry-run=client -o yaml | kubectl apply -f -
          
          # Patch default ServiceAccount before deployment (for admission webhook jobs)
          kubectl patch serviceaccount default -n ingress-nginx \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || echo "Default ServiceAccount already patched"

      - name: Deploy NGINX Ingress Controller
        shell: bash
        run: |
          # KUBECONFIG is set via GITHUB_ENV from Prepare kubeconfig step
          export KUBECONFIG="${KUBECONFIG:-$HOME/.kube/config}"
          
          # Try to deploy with retry mechanism
          MAX_RETRIES=2
          RETRY_COUNT=0
          
          while [ $RETRY_COUNT -le $MAX_RETRIES ]; do
            if [ $RETRY_COUNT -gt 0 ]; then
              echo "‚ö†Ô∏è Previous attempt failed, retrying (attempt $RETRY_COUNT/$MAX_RETRIES)..."
              # Force clean up stuck release before retry
              echo "Force cleaning up stuck release before retry..."
              helm uninstall ingress-nginx -n ingress-nginx --ignore-not-found || true
              sleep 3
              
              # Delete Helm release secrets to remove lock
              echo "Deleting Helm release secrets to remove lock..."
              kubectl delete secret -n ingress-nginx -l "owner=helm,name=ingress-nginx" --ignore-not-found || true
              kubectl get secret -n ingress-nginx 2>/dev/null | grep "sh.helm.release.v1.ingress-nginx" | awk '{print $1}' | xargs -r kubectl delete secret -n ingress-nginx --ignore-not-found || true
              sleep 3
            fi
            
            # Deploy with explicit full image path (no Docker Hub fallback)
            ACR_IMAGE="${{ secrets.ACR_REGISTRY }}/ingress-nginx/controller:v1.9.4"
            echo "Deploying NGINX Ingress Controller with ACR image: $ACR_IMAGE"
            echo "Using controller.image.image to set full image path (no Docker Hub fallback)"
            
            if helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
              -n ingress-nginx \
              -f infra/addons/values/ingress-nginx.yaml \
              --set controller.image.image="$ACR_IMAGE" \
              --wait --timeout 5m; then
              echo "‚úÖ NGINX Ingress Controller deployed successfully"
              
              # Verify deployed image
              echo "Verifying deployed image..."
              DEPLOYED_IMAGE=$(kubectl get deployment ingress-nginx-controller -n ingress-nginx -o jsonpath='{.spec.template.spec.containers[0].image}' 2>/dev/null || echo "Could not get image")
              echo "Deployed image: $DEPLOYED_IMAGE"
              if echo "$DEPLOYED_IMAGE" | grep -q "${{ secrets.ACR_REGISTRY }}"; then
                echo "‚úÖ Image is correctly set to use ACR: $DEPLOYED_IMAGE"
              else
                echo "‚ùå ERROR: Image does not use ACR registry! Expected: $ACR_IMAGE, Got: $DEPLOYED_IMAGE"
                exit 1
              fi
              break
            else
              RETRY_COUNT=$((RETRY_COUNT + 1))
              if [ $RETRY_COUNT -gt $MAX_RETRIES ]; then
                echo "‚ùå Failed to deploy NGINX Ingress Controller after $MAX_RETRIES retries"
                exit 1
              fi
            fi
          done

      - name: Patch NGINX ServiceAccount with imagePullSecret
        shell: bash
        run: |
          # Wait for ServiceAccount to be created by Helm
          echo "Waiting for ingress-nginx ServiceAccount to be created..."
          for i in {1..30}; do
            if kubectl get serviceaccount ingress-nginx -n ingress-nginx >/dev/null 2>&1; then
              echo "‚úÖ ServiceAccount found"
              break
            fi
            echo "Waiting... ($i/30)"
            sleep 2
          done
          
          # Patch ingress-nginx ServiceAccount to use ACR imagePullSecret
          if kubectl get serviceaccount ingress-nginx -n ingress-nginx >/dev/null 2>&1; then
            kubectl patch serviceaccount ingress-nginx -n ingress-nginx \
              -p '{"imagePullSecrets":[{"name":"acr-pull"}]}'
            echo "‚úÖ Patched ingress-nginx ServiceAccount"
          else
            echo "‚ö†Ô∏è ServiceAccount ingress-nginx not found, skipping patch"
          fi
          
          # Ensure default ServiceAccount is also patched (for admission webhook jobs)
          kubectl patch serviceaccount default -n ingress-nginx \
            -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || echo "Default ServiceAccount already patched"
          
          # Restart pods to use new imagePullSecret
          echo "Restarting NGINX pods to use ACR imagePullSecret..."
          kubectl rollout restart deployment ingress-nginx-controller -n ingress-nginx || echo "Deployment might not exist yet"
          kubectl wait --for=condition=available --timeout=5m deployment/ingress-nginx-controller -n ingress-nginx || echo "Waiting for deployment..."

      - name: Deploy Monitoring Stack (Prometheus + Grafana)
        shell: bash
        run: |
          kubectl get ns monitoring >/dev/null 2>&1 || kubectl create namespace monitoring
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            -n monitoring \
            -f infra/addons/values/observability.yaml \
            --set grafana.image.registry="${{ secrets.ACR_REGISTRY }}" \
            --set prometheus.prometheusSpec.image.registry="${{ secrets.ACR_REGISTRY }}" \
            --set alertmanager.image.registry="${{ secrets.ACR_REGISTRY }}" \
            --wait --timeout 10m

      - name: Deploy Metrics Server
        shell: bash
        run: |
          if ! kubectl -n kube-system get deploy metrics-server >/dev/null 2>&1; then
            # Download manifest and replace image URL with ACR
            curl -sL https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml | \
              sed "s|registry.k8s.io/metrics-server/metrics-server|${{ secrets.ACR_REGISTRY }}/metrics-server/metrics-server|g" | \
              kubectl apply -f -
            kubectl wait --for=condition=available --timeout=5m deployment/metrics-server -n kube-system
          else
            echo "Metrics Server already deployed"
          fi

      - name: Deploy Local Path Provisioner
        shell: bash
        run: |
          if ! kubectl -n local-path-storage get deploy local-path-provisioner >/dev/null 2>&1; then
            # Download manifest and replace image URL with ACR
            curl -sL https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml | \
              sed "s|rancher/local-path-provisioner:v0.0.26|${{ secrets.ACR_REGISTRY }}/rancher/local-path-provisioner:v0.0.26|g" | \
              kubectl apply -f -
            # Patch ServiceAccount to use imagePullSecret (after it's created)
            sleep 2
            kubectl patch serviceaccount local-path-provisioner-service-account -n local-path-storage \
              -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || echo "ServiceAccount patched or not found yet"
            kubectl wait --for=condition=available --timeout=5m deployment/local-path-provisioner -n local-path-storage
            # Set as default storage class if not already set
            kubectl patch storageclass local-path -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' || echo "StorageClass already configured"
          else
            echo "Local Path Provisioner already deployed"
            # Ensure imagePullSecret is set even if already deployed
            kubectl patch serviceaccount local-path-provisioner-service-account -n local-path-storage \
              -p '{"imagePullSecrets":[{"name":"acr-pull"}]}' || echo "ServiceAccount already patched"
          fi

      - name: Verify deployments
        shell: bash
        run: |
          echo "=== Ingress Controller ==="
          kubectl get pods -n ingress-nginx
          echo ""
          echo "=== Monitoring Stack ==="
          kubectl get pods -n monitoring
          echo ""
          echo "=== Metrics Server ==="
          kubectl get pods -n kube-system | grep metrics-server
          echo ""
          echo "=== Local Path Provisioner ==="
          kubectl get pods -n local-path-storage
          echo ""
          echo "=== Storage Classes ==="
          kubectl get storageclass
          echo ""
          echo "=== Cluster Status ==="
          kubectl top nodes || echo "Metrics not yet available (may take a few minutes)"

      - name: Summary
        shell: bash
        run: |
          echo "‚úÖ Cluster Infrastructure deployed successfully!"
          echo "  - NGINX Ingress Controller: ingress-nginx namespace"
          echo "  - Prometheus + Grafana: monitoring namespace"
          echo "  - Metrics Server: kube-system namespace"
          echo "  - Local Path Provisioner: local-path-storage namespace"
          echo "  - All images pulled from ACR: ${{ secrets.ACR_REGISTRY }}"


